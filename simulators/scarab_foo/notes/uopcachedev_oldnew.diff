diff --git a/source.sh b/source.sh
new file mode 100644
index 0000000..033d390
--- /dev/null
+++ b/source.sh
@@ -0,0 +1,4 @@
+export DRIO_ROOT=/mnt/storage/takh/git-repos/dynamorio
+export PIN_ROOT=/mnt/storage/takh/tools/pinplay/pinplay
+export LD_LIBRARY_PATH=/mnt/storage/takh/tools/pinplay/pinplay/extras/xed-intel64/lib/:$LD_LIBRARY_PATH
+export LD_LIBRARY_PATH=/mnt/storage/takh/tools/pinplay/pinplay/intel64/runtime/pincrt/:$LD_LIBRARY_PATH
diff --git a/src/Makefile b/src/Makefile
index 407abc4..69330a7 100644
--- a/src/Makefile
+++ b/src/Makefile
@@ -100,7 +100,6 @@ DR_LINK := -L$(DR_REL) -L$(DR_BIN) -L$(DR_EXT) -L$(DR_REL2) -L$(DR_LIB64)
 DR_LIBS := -ldrmemtrace_raw2trace -ldirectory_iterator -ldrfrontendlib -ldrutil_static -ldrmgr_static -ldrmemfuncs
 DR_LIBS := $(DR_LIBS) -ldynamorio_static -ldrlibc -ldrcovlib_static -ldrx_static -ldrreg_static -ldrcontainers
 DR_LIBS := $(DR_LIBS) -ldrmemtrace_analyzer -lsnappy -ldl -lconfig++ -lz -lrt
-
 DR_ENV := -DLINUX -DX86_64
 #-DVERSION_NUMBER_INTEGER 1
 XED := -L$(PIN_ROOT)/extras/xed-intel64/lib/ -L$(PIN_ROOT)/intel64/runtime/pincrt/
diff --git a/src/bp/bp.c b/src/bp/bp.c
index b3fa97e..51530be 100644
--- a/src/bp/bp.c
+++ b/src/bp/bp.c
@@ -37,14 +37,16 @@
 
 #include "bp//bp_conf.h"
 #include "bp/bp.h"
-//#include "bp/bp_dir_mech.h"
 #include "bp/bp_targ_mech.h"
+#include "bp/cbp_to_scarab.h"
 #include "bp/gshare.h"
 #include "bp/hybridgp.h"
 #include "bp/tagescl.h"
 #include "libs/cache_lib.h"
 #include "model.h"
 #include "thread.h"
+#include "uop_cache.h"
+#include "icache_stage.h"
 
 #include "bp/bp.param.h"
 #include "core.param.h"
@@ -80,7 +82,7 @@ extern void tc_do_stat(Op*, Flag);
 Bp_Recovery_Info* bp_recovery_info = NULL;
 Bp_Data*          g_bp_data        = NULL;
 Flag              USE_LATE_BP      = FALSE;
-
+extern List       op_buf;
 
 /******************************************************************************/
 // Local prototypes
@@ -113,6 +115,9 @@ void init_bp_recovery_info(uns8              proc_id,
   new_bp_recovery_info->redirect_cycle = MAX_CTR;
 
   bp_recovery_info = new_bp_recovery_info;
+
+  init_hash_table(&per_branch_stat, "Per Branch Hit/Miss and Recovery/Redirect Stall cycles",
+                  15000000, sizeof(Per_Branch_Stat));
 }
 
 
@@ -129,8 +134,19 @@ void bp_sched_recovery(Bp_Recovery_Info* bp_recovery_info, Op* op,
   if(bp_recovery_info->recovery_cycle == MAX_CTR ||
      op->op_num <= bp_recovery_info->recovery_op_num) {
     const Addr next_fetch_addr = op->oracle_info.npc;
-    const uns  latency         = late_bp_recovery ? LATE_BP_LATENCY :
-                                           1 + EXTRA_RECOVERY_CYCLES;
+
+    uns fetch_latency;
+    Flag uc_hit = in_uop_cache(next_fetch_addr, NULL, FALSE);
+    inc_bstat_miss(op, uc_hit);
+    if (uc_hit) {
+      fetch_latency = UOP_CACHE_LATENCY;
+      INC_STAT_EVENT(bp_recovery_info->proc_id, BP_RECOVERY_FETCH_CYCLES_UC, UOP_CACHE_LATENCY);
+    } else {
+      fetch_latency = ICACHE_LATENCY;
+      INC_STAT_EVENT(bp_recovery_info->proc_id, BP_RECOVERY_FETCH_CYCLES_IC, ICACHE_LATENCY);
+    }
+    const uns latency = late_bp_recovery ? fetch_latency + 1 + LATE_BP_LATENCY :
+                                           fetch_latency + 1;
     DEBUG(
       bp_recovery_info->proc_id,
       "Recovery signaled for op_num:%s @ 0x%s  next_fetch:0x%s offpath:%d\n",
@@ -147,6 +163,8 @@ void bp_sched_recovery(Bp_Recovery_Info* bp_recovery_info, Op* op,
     bp_recovery_info->recovery_cf_type       = op->table_info->cf_type;
     bp_recovery_info->recovery_info          = op->recovery_info;
     bp_recovery_info->recovery_info.op_num   = op->op_num;
+    if (FDIP_ENABLE)
+      bp_recovery_info->recovery_info.npc    = next_fetch_addr;
     bp_recovery_info->recovery_inst_info     = op->inst_info;
     bp_recovery_info->recovery_force_offpath = op->off_path;
     bp_recovery_info->recovery_op            = op;
@@ -168,6 +186,62 @@ void bp_sched_recovery(Bp_Recovery_Info* bp_recovery_info, Op* op,
   }
 }
 
+void inc_bstat_fetched(Op* op) {
+  Flag new_entry;
+  Per_Branch_Stat* bstat = (Per_Branch_Stat*) hash_table_access_create(&per_branch_stat, op->inst_info->addr, &new_entry);
+  if (new_entry) {
+    memset(bstat, 0, sizeof(*bstat));
+    bstat->addr = op->inst_info->addr;
+    bstat->cf_type = op->table_info->cf_type;
+  }
+
+  const uns8 mispred =  op->oracle_info.mispred;
+  const uns8 misfetch = op->oracle_info.misfetch;
+  const uns8 btb_miss = op->oracle_info.btb_miss;
+
+  if (!mispred && !misfetch && !btb_miss) {
+    if (in_uop_cache(op->oracle_info.npc, NULL, FALSE)) {
+      bstat->bpu_hit_uc_hit += 1;
+    }else {
+      bstat->bpu_hit_uc_miss += 1;
+      if (!in_icache(op->inst_info->addr)) bstat->bpu_hit_uc_ic_miss += 1;
+    }
+  }
+
+  // target if taken
+  if (op->oracle_info.pred && !op->oracle_info.mispred)
+    bstat->target = op->oracle_info.npc;
+}
+
+void inc_bstat_miss(Op* op, Flag uc_hit) {
+  Per_Branch_Stat* bstat = (Per_Branch_Stat*) hash_table_access(&per_branch_stat, op->inst_info->addr);
+  ASSERT(bp_recovery_info->proc_id, bstat);
+
+  const uns8 mispred =  op->oracle_info.mispred;
+  const uns8 misfetch = op->oracle_info.misfetch;
+  const uns8 btb_miss = op->oracle_info.btb_miss;
+  ASSERT(bp_recovery_info->proc_id, mispred || misfetch || btb_miss);
+
+  const Flag in_ic = in_icache(op->inst_info->addr);
+
+  if (uc_hit) {
+    if (mispred)        bstat->mispred_uc_hit += 1;
+    else if (misfetch)  bstat->misfetch_uc_hit += 1;
+    else                bstat->btb_miss_uc_hit += 1;
+  } else {
+    if (mispred) {
+      bstat->mispred_uc_miss += 1;
+      if (!in_ic) bstat->mispred_uc_ic_miss += 1;
+    } else if (misfetch) {
+      bstat->misfetch_uc_miss += 1;
+      if (!in_ic) bstat->misfetch_uc_ic_miss += 1;
+    } else {
+      bstat->btb_miss_uc_miss += 1;
+      if (!in_ic) bstat->btb_miss_uc_ic_miss += 1;
+    }
+    bstat->recover_redirect_extra_fetch_latency += ICACHE_LATENCY - UOP_CACHE_LATENCY;
+  }
+}
 
 /******************************************************************************/
 /* bp_sched_redirect: called on an op that caused the fetch stage to suspend
@@ -179,12 +253,26 @@ void bp_sched_redirect(Bp_Recovery_Info* bp_recovery_info, Op* op,
      op->op_num < bp_recovery_info->redirect_op_num) {
     DEBUG(bp_recovery_info->proc_id, "Redirect signaled for op_num:%s @ 0x%s\n",
           unsstr64(op->op_num), hexstr64s(op->inst_info->addr));
-    bp_recovery_info->redirect_cycle = cycle + 1 + EXTRA_REDIRECT_CYCLES +
+
+    uns fetch_latency;
+    Flag uc_hit = in_uop_cache(op->oracle_info.npc, NULL, FALSE);  // should this be pred_npc? We may not have predicted correctly.
+    inc_bstat_miss(op, uc_hit);
+    if (uc_hit) {
+      fetch_latency = UOP_CACHE_LATENCY;
+      INC_STAT_EVENT(bp_recovery_info->proc_id, BP_REDIRECT_FETCH_CYCLES_UC, UOP_CACHE_LATENCY);
+    } else {
+      fetch_latency = ICACHE_LATENCY;
+      INC_STAT_EVENT(bp_recovery_info->proc_id, BP_REDIRECT_FETCH_CYCLES_IC, ICACHE_LATENCY);
+    }
+    bp_recovery_info->redirect_cycle = cycle + 1 + fetch_latency +
                                        (op->table_info->cf_type == CF_SYS ?
                                           EXTRA_CALLSYS_CYCLES :
                                           0);
     bp_recovery_info->redirect_op     = op;
     bp_recovery_info->redirect_op_num = op->op_num;
+    if (FDIP_ENABLE && !op->oracle_info.mispred && !op->oracle_info.misfetch) {
+      fdip_redirect(op->oracle_info.pred_npc);
+    }
   }
 }
 
@@ -243,6 +331,10 @@ void init_bp_data(uns8 proc_id, Bp_Data* bp_data) {
   }
 }
 
+Flag bp_is_predictable(Bp_Data* bp_data, Op* op) {
+  return !bp_data->bp->full_func(op);
+}
+
 
 /******************************************************************************/
 /* bp_predict_op:  predicts the target of a control flow instruction */
@@ -347,8 +439,13 @@ Addr bp_predict_op(Bp_Data* bp_data, Op* op, uns br_num, Addr fetch_addr) {
   // {{{ handle predictions for individual cf types
   switch(op->table_info->cf_type) {
     case CF_BR:
-      op->oracle_info.pred      = TAKEN;
-      op->oracle_info.late_pred = TAKEN;
+      if(PERFECT_BP) {
+        op->oracle_info.pred      = op->oracle_info.dir;
+        op->oracle_info.late_pred = op->oracle_info.dir;
+      } else {
+        op->oracle_info.pred      = TAKEN;
+        op->oracle_info.late_pred = TAKEN;
+      }
       if(!op->off_path)
         STAT_EVENT(op->proc_id, CF_BR_USED_TARGET_CORRECT +
                                   (pred_target != op->oracle_info.npc));
@@ -395,8 +492,13 @@ Addr bp_predict_op(Bp_Data* bp_data, Op* op, uns br_num, Addr fetch_addr) {
       break;
 
     case CF_IBR:
-      op->oracle_info.pred      = TAKEN;
-      op->oracle_info.late_pred = TAKEN;
+      if(PERFECT_BP) {
+        op->oracle_info.pred      = op->oracle_info.dir;
+        op->oracle_info.late_pred = op->oracle_info.dir;
+      } else {
+        op->oracle_info.pred      = TAKEN;
+        op->oracle_info.late_pred = TAKEN;
+      }
       if(ENABLE_IBP) {
         Addr ibp_target = bp_data->bp_ibtb->pred_func(bp_data, op);
         if(ibp_target) {
@@ -405,16 +507,20 @@ Addr bp_predict_op(Bp_Data* bp_data, Op* op, uns br_num, Addr fetch_addr) {
           op->oracle_info.ibp_miss  = FALSE;
         } else
           op->oracle_info.ibp_miss = TRUE;
-
-        if(!op->off_path)
-          STAT_EVENT(op->proc_id, CF_IBR_USED_TARGET_CORRECT +
-                                    (pred_target != op->oracle_info.npc));
+      if(!op->off_path)
+        STAT_EVENT(op->proc_id, CF_IBR_USED_TARGET_CORRECT +
+            (pred_target != op->oracle_info.npc));
       }
       break;
 
     case CF_ICALL:
-      op->oracle_info.pred      = TAKEN;
-      op->oracle_info.late_pred = TAKEN;
+      if(PERFECT_BP) {
+        op->oracle_info.pred      = op->oracle_info.dir;
+        op->oracle_info.late_pred = op->oracle_info.dir;
+      } else {
+        op->oracle_info.pred      = TAKEN;
+        op->oracle_info.late_pred = TAKEN;
+      }
       if(ENABLE_IBP) {
         Addr ibp_target = bp_data->bp_ibtb->pred_func(bp_data, op);
         if(ibp_target) {
@@ -447,8 +553,13 @@ Addr bp_predict_op(Bp_Data* bp_data, Op* op, uns br_num, Addr fetch_addr) {
       break;
 
     case CF_RET:
-      op->oracle_info.pred      = TAKEN;
-      op->oracle_info.late_pred = TAKEN;
+      if(PERFECT_BP) {
+        op->oracle_info.pred      = op->oracle_info.dir;
+        op->oracle_info.late_pred = op->oracle_info.dir;
+      } else {
+        op->oracle_info.pred      = TAKEN;
+        op->oracle_info.late_pred = TAKEN;
+      }
       if(ENABLE_CRS)
         pred_target = CRS_REALISTIC ? bp_crs_realistic_pop(bp_data, op) :
                                       bp_crs_pop(bp_data, op);
@@ -475,6 +586,9 @@ Addr bp_predict_op(Bp_Data* bp_data, Op* op, uns br_num, Addr fetch_addr) {
   const Addr pc_plus_offset = ADDR_PLUS_OFFSET(
     op->inst_info->addr, op->inst_info->trace_info.inst_size);
 
+  op->pred_target = pred_target;
+  op->pc_plus_offset = pc_plus_offset;
+
   const Addr prediction = op->oracle_info.pred ? pred_target : pc_plus_offset;
   op->oracle_info.pred_npc = prediction;
   if(USE_LATE_BP) {
@@ -483,6 +597,7 @@ Addr bp_predict_op(Bp_Data* bp_data, Op* op, uns br_num, Addr fetch_addr) {
     op->oracle_info.late_pred_npc = late_prediction;
   }
 
+  bp_predict_op_evaluate(bp_data, op, prediction);
   return prediction;
 }
 
@@ -556,7 +671,7 @@ Addr bp_predict_op_evaluate(Bp_Data* bp_data, Op *op, Addr prediction) {
     bp_data->proc_id,
     "BTB:  op_num:%s  off_path:%d  cf_type:%s  addr:0x%s  btb_miss:%d\n",
     unsstr64(op->op_num), op->off_path, cf_type_names[op->table_info->cf_type],
-    hexstr64s(addr), op->oracle_info.btb_miss);
+    hexstr64s(op->inst_info->addr), op->oracle_info.btb_miss);
 
   DEBUG(bp_data->proc_id,
         "BP:  op_num:%s  off_path:%d  cf_type:%s  addr:%s  p_npc:%s  "
diff --git a/src/bp/bp.h b/src/bp/bp.h
index e03e98a..71ab80d 100644
--- a/src/bp/bp.h
+++ b/src/bp/bp.h
@@ -177,6 +177,9 @@ typedef enum Bp_Id_enum {
   GSHARE_BP,
   HYBRIDGP_BP,
   TAGESCL_BP,
+  #define DEF_CBP(CBP_NAME, CBP_CLASS) CBP_CLASS##_BP,
+  #include "cbp_table.def"
+  #undef DEF_CBP
   NUM_BP,
 } Bp_Id;
 
@@ -213,6 +216,7 @@ typedef struct Bp_struct {
                                the bp that has to be updated after retirement*/
   void (*recover_func)(Recovery_Info*); /* called to recover the bp when a
                                            misprediction is realized */
+  uns8 (*full_func)(Op*);
 } Bp;
 
 typedef struct Bp_Btb_struct {
@@ -273,6 +277,7 @@ void bp_sched_recovery(Bp_Recovery_Info* bp_recovery_info, Op* op,
 void bp_sched_redirect(Bp_Recovery_Info*, Op*, Counter);
 
 void init_bp_data(uns8, Bp_Data*);
+Flag bp_is_predictable(Bp_Data*, Op*);
 Addr bp_predict_op(Bp_Data*, Op*, uns, Addr);
 Addr bp_predict_op_evaluate(Bp_Data* bp_data, Op *op, Addr prediction);
 void bp_target_known_op(Bp_Data*, Op*);
@@ -280,6 +285,8 @@ void bp_resolve_op(Bp_Data*, Op*);
 void bp_retire_op(Bp_Data*, Op*);
 void bp_recover_op(Bp_Data*, Cf_Type, Recovery_Info*);
 
+void inc_bstat_fetched(Op* op);
+void inc_bstat_miss(Op* op, Flag uc_hit);
 
 /**************************************************************************************/
 
diff --git a/src/bp/bp_table.def b/src/bp/bp_table.def
index 02d517a..24d7175 100644
--- a/src/bp/bp_table.def
+++ b/src/bp/bp_table.def
@@ -30,10 +30,14 @@
 Bp bp_table [] = {
     /* Enum         Name        init                timestamp               pred              spec_update               update               retire               recover            */
     /* ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- */
-    { GSHARE_BP,    "gshare",   bp_gshare_init,     bp_gshare_timestamp,    bp_gshare_pred,   bp_gshare_spec_update,    bp_gshare_update,    bp_gshare_retire,    bp_gshare_recover   },
-    { HYBRIDGP_BP,  "hybridgp", bp_hybridgp_init,   bp_hybridgp_timestamp,  bp_hybridgp_pred, bp_hybridgp_spec_update,  bp_hybridgp_update,  bp_hybridgp_retire,  bp_hybridgp_recover },
-    { TAGESCL_BP,   "tagescl",  bp_tagescl_init,    bp_tagescl_timestamp,   bp_tagescl_pred,  bp_tagescl_spec_update,   bp_tagescl_update,   bp_tagescl_retire,   bp_tagescl_recover },    
-    { NUM_BP,       0,          NULL,               NULL,                   NULL,             NULL,                     NULL,                NULL,                NULL }
+    { GSHARE_BP,    "gshare",   bp_gshare_init,     bp_gshare_timestamp,    bp_gshare_pred,   bp_gshare_spec_update,    bp_gshare_update,    bp_gshare_retire,    bp_gshare_recover,    bp_gshare_full},
+    { HYBRIDGP_BP,  "hybridgp", bp_hybridgp_init,   bp_hybridgp_timestamp,  bp_hybridgp_pred, bp_hybridgp_spec_update,  bp_hybridgp_update,  bp_hybridgp_retire,  bp_hybridgp_recover,  bp_hybridgp_full},
+    { TAGESCL_BP,   "tagescl",  bp_tagescl_init,    bp_tagescl_timestamp,   bp_tagescl_pred,  bp_tagescl_spec_update,   bp_tagescl_update,   bp_tagescl_retire,   bp_tagescl_recover,   bp_tagescl_full},
+    #define DEF_CBP(CBP_NAME, CBP_CLASS) \
+     { CBP_CLASS ## _BP,    CBP_NAME,   SCARAB_BP_INTF_FUNC(CBP_CLASS, init), SCARAB_BP_INTF_FUNC(CBP_CLASS, timestamp), SCARAB_BP_INTF_FUNC(CBP_CLASS, pred), SCARAB_BP_INTF_FUNC(CBP_CLASS, spec_update), SCARAB_BP_INTF_FUNC(CBP_CLASS, update), SCARAB_BP_INTF_FUNC(CBP_CLASS, retire), SCARAB_BP_INTF_FUNC(CBP_CLASS, recover)}, 
+     #include "cbp_table.def"
+     #undef DEF_CBP    
+    { NUM_BP,       0,          NULL,               NULL,                   NULL,             NULL,                     NULL,                NULL,                NULL,                 NULL}
     
 };
 
diff --git a/src/bp/cbp_table.def b/src/bp/cbp_table.def
new file mode 100644
index 0000000..1c8afe2
--- /dev/null
+++ b/src/bp/cbp_table.def
@@ -0,0 +1,15 @@
+/**
+ * @file cbp_table.def
+ * @author Stephen Pruett (stephen.pruett@utexas.edu)
+ * @brief
+ * @version 0.1
+ * @date 2021-01-12
+ *
+ * Usage:
+ *   Add your CBP predictor below, as indicated by the example.
+ *
+ * Example:
+ *   DEF_CBP("predictor_name", PREDICTOR_CLASS)
+ */
+
+DEF_CBP("mtage", MTAGE)
\ No newline at end of file
diff --git a/src/bp/cbp_to_scarab.cc b/src/bp/cbp_to_scarab.cc
new file mode 100644
index 0000000..ef43297
--- /dev/null
+++ b/src/bp/cbp_to_scarab.cc
@@ -0,0 +1,121 @@
+/**
+ * @file cbp_to_scarab.c
+ * @author Stephen Pruett (stephen.pruett@utexas.edu)
+ * @brief Implements the interface between CBP and Scarab
+ * @version 0.1
+ * @date 2021-01-12
+ *
+ * USAGE:
+ *  Add your CBP header file to the list below.
+ */
+
+/**Add CBP Header Below**/
+#include "mtage_unlimited.h"
+/************************/
+
+/******DO NOT MODIFY BELOW THIS POINT*****/
+
+/**
+ * @brief The template class below defines how all CBP predictors
+ * interact with scarab.
+ */
+
+#include "cbp_to_scarab.h"
+#include "bp/bp.param.h"
+
+template <typename CBP_CLASS>
+class CBP_To_Scarab_Intf {
+  std::vector<CBP_CLASS> cbp_predictors;
+
+ public:
+  void init() {
+    if(cbp_predictors.size() == 0) {
+      cbp_predictors.reserve(NUM_CORES);
+      for(uns i = 0; i < NUM_CORES; ++i) {
+        cbp_predictors.emplace_back();
+      }
+    }
+    ASSERTM(0, cbp_predictors.size() == NUM_CORES,
+            "cbp_predictors not initialized correctly");
+  }
+
+  void timestamp(Op* op) {
+    /* CBP Interface does not support speculative updates */
+    op->recovery_info.branch_id = 0;
+  }
+
+  uns8 pred(Op* op) {
+    uns proc_id = op->proc_id;
+    if(op->off_path)
+      return op->oracle_info.dir;
+    return cbp_predictors.at(proc_id).GetPrediction(op->inst_info->addr);
+  }
+
+  void spec_update(Op* op) {
+    /* CBP Interface does not support speculative updates */
+    if(op->off_path)
+      return;
+
+    uns    proc_id = op->proc_id;
+    OpType optype  = scarab_to_cbp_optype(op);
+
+    if(is_conditional_branch(op)) {
+      cbp_predictors.at(proc_id).UpdatePredictor(
+        op->inst_info->addr, optype, op->oracle_info.dir, op->oracle_info.pred,
+        op->oracle_info.target);
+    } else {
+      cbp_predictors.at(proc_id).TrackOtherInst(op->inst_info->addr, optype,
+                                                op->oracle_info.dir,
+                                                op->oracle_info.target);
+    }
+  }
+
+  void update(Op* op) { /* CBP Interface does not support update at exec */
+  }
+
+  void retire(Op* op) {
+    /* CBP Interface updates predictor at speculative update time */
+  }
+
+  void recover(Recovery_Info*) {
+    /* CBP Interface does not support speculative updates */
+  }
+
+  void full(Op* op) {
+    uns proc_id = op->proc_id;
+    return cbp_predictors.at(proc_id).IsFull();
+  }
+};
+
+/******DO NOT MODIFY BELOW THIS POINT*****/
+
+/**
+ * @brief Macros below define c-style functions to interface with the
+ * template class above. This way these functions can be called from C code.
+ */
+
+#define CBP_PREDICTOR(CBP_CLASS) cbp_predictor_##CBP_CLASS
+
+#define DEF_CBP(CBP_NAME, CBP_CLASS) \
+  CBP_To_Scarab_Intf<CBP_CLASS> CBP_PREDICTOR(CBP_CLASS);
+#include "cbp_table.def"
+#undef DEF_CBP
+
+#define SCARAB_BP_INTF_FUNC_IMPL(CBP_CLASS, FCN_NAME, Ret, RetType, Type, Arg) \
+  RetType SCARAB_BP_INTF_FUNC(CBP_CLASS, FCN_NAME)(Type Arg) {                 \
+    Ret CBP_PREDICTOR(CBP_CLASS).FCN_NAME(Arg);                                \
+  }
+
+#define DEF_CBP(CBP_NAME, CBP_CLASS)                                \
+  SCARAB_BP_INTF_FUNC_IMPL(CBP_CLASS, init, , void, , )             \
+  SCARAB_BP_INTF_FUNC_IMPL(CBP_CLASS, timestamp, , void, Op*, op)   \
+  SCARAB_BP_INTF_FUNC_IMPL(CBP_CLASS, pred, return, uns8, Op*, op)  \
+  SCARAB_BP_INTF_FUNC_IMPL(CBP_CLASS, spec_update, , void, Op*, op) \
+  SCARAB_BP_INTF_FUNC_IMPL(CBP_CLASS, update, , void, Op*, op)      \
+  SCARAB_BP_INTF_FUNC_IMPL(CBP_CLASS, retire, , void, Op*, op)      \
+  SCARAB_BP_INTF_FUNC_IMPL(CBP_CLASS, recover, , void, Recovery_Info*, info)
+
+#include "cbp_table.def"
+#undef DEF_CBP
+#undef SCARAB_BP_INF_FUNC_IMPL
+#undef CBP_PREDICTOR
\ No newline at end of file
diff --git a/src/bp/cbp_to_scarab.h b/src/bp/cbp_to_scarab.h
new file mode 100644
index 0000000..de0b36c
--- /dev/null
+++ b/src/bp/cbp_to_scarab.h
@@ -0,0 +1,165 @@
+/**
+ * @file cbp_to_scarab.h
+ * @author Stephen Pruett (stephen.pruett@utexas.edu)
+ * @brief Convert CBP interface to Scarab Interface
+ * @version 0.1
+ * @date 2021-01-12
+ *
+ * The goal of this file is to make transitioning between CBP to Scarab as
+ * painless as possible. This file was written with CBP 2016 in mind.
+ *
+ * USAGE:
+ *   1. Add CBP files to scarab
+ *      a. Download files from CBP (predictor.h and predictor.cc) and place them
+ * in src/bp/ directory. b. Rename files to more descriptive name (e.g., mtage.h
+ * and mtage.cc) c. Search and replace all occurances of PREDICTOR with new
+ * name (e.g., s/PREDICTOR/MTAGE/g) d. Rename any conflicting names (e.g.,
+ * ASSERT)
+ *   2. Add the CBP header file to cbp_to_scarab.cpp
+ *   3. Add entry for CBP predictor in cbp_table.def (make sure names do not
+ * conflict)
+ */
+
+#ifndef __CBP_TO_SCARAB_H__
+#define __CBP_TO_SCARAB_H__
+
+// Random libraries that may be useful
+#include <assert.h>
+#include <inttypes.h>
+#include <math.h>
+#include <stdlib.h>
+#include "stdint.h"
+#include "stdio.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include "core.param.h"
+#include "globals/assert.h"
+#include "globals/utils.h"
+#include "table_info.h"
+
+#ifdef __cplusplus
+}
+
+#include <cmath>
+#include <cstdio>
+#include <cstdlib>
+#include <iostream>
+#include <string>
+#endif
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+// Library needed for Scarab Interface
+#include "bp/bp.h"
+
+#define SCARAB_BP_INTF_FUNC(CBP_CLASS, FCN_NAME) bp_##CBP_CLASS##_##FCN_NAME
+
+/*************Interface to Scarab***************/
+#define DEF_CBP(CBP_NAME, CBP_CLASS)                         \
+  void SCARAB_BP_INTF_FUNC(CBP_CLASS, init)();               \
+  void SCARAB_BP_INTF_FUNC(CBP_CLASS, timestamp)(Op * op);   \
+  uns8 SCARAB_BP_INTF_FUNC(CBP_CLASS, pred)(Op*);            \
+  void SCARAB_BP_INTF_FUNC(CBP_CLASS, spec_update)(Op * op); \
+  void SCARAB_BP_INTF_FUNC(CBP_CLASS, update)(Op * op);      \
+  void SCARAB_BP_INTF_FUNC(CBP_CLASS, retire)(Op * op);      \
+  void SCARAB_BP_INTF_FUNC(CBP_CLASS, recover)(Recovery_Info*);
+#include "cbp_table.def"
+#undef DEF_CBP
+
+#ifdef __cplusplus
+}
+#endif
+
+/*************CPB 2016 UTILS**********************/
+
+#define UINT32 unsigned int
+#define INT32 int
+#define UINT64 unsigned long long
+#define COUNTER unsigned long long
+
+#define NOT_TAKEN 0
+#define TAKEN 1
+
+#define FAILURE 0
+#define SUCCESS 1
+
+typedef enum {
+  OPTYPE_OP = 2,
+
+  OPTYPE_RET_UNCOND,
+  OPTYPE_JMP_DIRECT_UNCOND,
+  OPTYPE_JMP_INDIRECT_UNCOND,
+  OPTYPE_CALL_DIRECT_UNCOND,
+  OPTYPE_CALL_INDIRECT_UNCOND,
+
+  OPTYPE_RET_COND,
+  OPTYPE_JMP_DIRECT_COND,
+  OPTYPE_JMP_INDIRECT_COND,
+  OPTYPE_CALL_DIRECT_COND,
+  OPTYPE_CALL_INDIRECT_COND,
+
+  OPTYPE_ERROR,
+
+  OPTYPE_MAX
+} OpType;
+
+static inline UINT32 SatIncrement(UINT32 x, UINT32 max) {
+  if(x < max)
+    return x + 1;
+  return x;
+}
+
+static inline UINT32 SatDecrement(UINT32 x) {
+  if(x > 0)
+    return x - 1;
+  return x;
+}
+
+static inline Flag is_conditional_branch(Op* op) {
+  return op->table_info->cf_type == CF_CBR;
+}
+
+static inline OpType scarab_to_cbp_optype(Op* op) {
+  Cf_Type cf_type = op->table_info->cf_type;
+  OpType  optype  = OPTYPE_OP;
+
+  switch(cf_type) {
+    case CF_BR:
+      optype = OPTYPE_JMP_DIRECT_UNCOND;
+      break;
+    case CF_CALL:
+      optype = OPTYPE_CALL_DIRECT_UNCOND;
+      break;
+    case CF_CBR:
+      optype = OPTYPE_JMP_DIRECT_COND;
+      break;
+    case CF_IBR:
+      optype = OPTYPE_JMP_INDIRECT_UNCOND;
+      break;
+    case CF_ICALL:
+      optype = OPTYPE_CALL_INDIRECT_UNCOND;
+      break;
+    case CF_ICO:
+      optype = OPTYPE_CALL_INDIRECT_UNCOND;
+      break;
+    case CF_RET:
+      optype = OPTYPE_RET_UNCOND;
+      break;
+    case CF_SYS:
+      optype = OPTYPE_CALL_INDIRECT_UNCOND;
+      break;
+    default:
+      // Should never see non-control flow instructions or invalid CF
+      // types in the branch predictor.
+      ASSERT(op->proc_id, 0);
+      break;
+  }
+  return optype;
+}
+
+#endif
\ No newline at end of file
diff --git a/src/bp/gshare.cc b/src/bp/gshare.cc
index 38d6d62..9d28aeb 100644
--- a/src/bp/gshare.cc
+++ b/src/bp/gshare.cc
@@ -54,6 +54,7 @@ void bp_gshare_timestamp(Op* op) {}
 void bp_gshare_recover(Recovery_Info* info) {}
 void bp_gshare_spec_update(Op* op) {}
 void bp_gshare_retire(Op* op) {}
+uns8 bp_gshare_full(Op* op) { return 0; }
 
 
 void bp_gshare_init() {
diff --git a/src/bp/gshare.h b/src/bp/gshare.h
index da56ade..1111e72 100644
--- a/src/bp/gshare.h
+++ b/src/bp/gshare.h
@@ -36,6 +36,7 @@ void bp_gshare_spec_update(Op*);
 void bp_gshare_update(Op*);
 void bp_gshare_retire(Op*);
 void bp_gshare_recover(Recovery_Info*);
+uns8 bp_gshare_full(Op*);
 
 #ifdef __cplusplus
 }
diff --git a/src/bp/hybridgp.cc b/src/bp/hybridgp.cc
index 1a6cfef..dbe7072 100644
--- a/src/bp/hybridgp.cc
+++ b/src/bp/hybridgp.cc
@@ -454,3 +454,10 @@ void bp_hybridgp_retire(Op* op) {
 
   hybridgp_state.in_flight.deallocate_front(op->recovery_info.branch_id);
 }
+
+uns8 bp_hybridgp_full(Op* op) {
+  const uns proc_id        = op->proc_id;
+  auto&     hybridgp_state = hybridgp_state_all_cores.at(proc_id);
+
+  return hybridgp_state.in_flight.is_full();
+}
diff --git a/src/bp/hybridgp.h b/src/bp/hybridgp.h
index 6e0ac8e..52a9fd4 100644
--- a/src/bp/hybridgp.h
+++ b/src/bp/hybridgp.h
@@ -36,6 +36,7 @@ void bp_hybridgp_spec_update(Op*);
 void bp_hybridgp_update(Op*);
 void bp_hybridgp_retire(Op*);
 void bp_hybridgp_recover(Recovery_Info*);
+uns8 bp_hybridgp_full(Op*);
 
 #ifdef __cplusplus
 }
diff --git a/src/bp/mtage_unlimited.cc b/src/bp/mtage_unlimited.cc
new file mode 100644
index 0000000..820cb36
--- /dev/null
+++ b/src/bp/mtage_unlimited.cc
@@ -0,0 +1,1762 @@
+/**
+ * @file mtage_unlimited.cc
+ * @brief P. Michaud and A. Seznec
+ * @version 0.1
+ * @date 2021-01-12
+ *
+ * Adapted to Scarab by Stephen Pruett
+ *
+ * Code is derived from P.Michaud and A. Seznec code for the CBP4 winner Sorry
+ * two very different code writing styles
+ *
+ * ////////////// STORAGE BUDGET JUSTIFICATION ////////////////
+ *   unlimited size
+ */
+
+#include "mtage_unlimited.h"
+#include "bp/bp.param.h"
+
+// for my personal statistics
+int  XX, YY, ZZ, TT;
+void PrintStat(double NUMINST) {
+  printf("  \nTAGE_MPKI   \t : %10.4f", 1000.0 * (double)(XX) / NUMINST);
+  printf("  \nCOLT_MPKI    \t : %10.4f", 1000.0 * (double)(ZZ) / NUMINST);
+  printf("  \nNEURAL_MPKI    \t : %10.4f", 1000.0 * (double)(YY) / NUMINST);
+  printf("  \nSC_MPKI    \t : %10.4f", 1000.0 * (double)(TT) / NUMINST);
+
+  //    printf("  XX %d YY %d ZZ %d TT %d",XX,YY,ZZ,TT);
+}
+
+bool COPRED;
+// VARIABLES FOR THE Statistical correctors
+int  LSUM;
+bool predSC;
+bool predfinal;
+bool pred_inter;
+
+
+#define IMLI  // just to be able to isolate IMLI impact: marginal on CBP5 traces
+
+#define PERCWIDTH 8
+#define GPSTEP 6
+#define LPSTEP 6
+#define BPSTEP 6
+#define PPSTEP 6
+#define SPSTEP 6
+#define YPSTEP 6
+#define TPSTEP 6
+
+#define GWIDTH 60
+#define LWIDTH 60
+#define BWIDTH 42
+#define PWIDTH 60
+#define SWIDTH 60
+#define YWIDTH 60
+#define TWIDTH 60
+
+#define LOGTAB 19
+#define TABSIZE (1 << LOGTAB)
+#define LOGB LOGTAB
+#define LOGG LOGTAB
+#define LOGSIZE (10)
+#define LOGSIZEG (LOGSIZE)
+#define LOGSIZEL (LOGSIZE)
+#define LOGSIZEB (LOGSIZE)
+#define LOGSIZES (LOGSIZE)
+#define LOGSIZEP (LOGSIZE)
+#define LOGSIZEY (LOGSIZE)
+#define LOGSIZET (LOGSIZE)
+
+// The perceptron-inspired components
+int8_t PERC[(1 << LOGSIZEP)][10 * (1 << GPSTEP)];
+int8_t PERCLOC[(1 << LOGSIZEL)][10 * (1 << LPSTEP)];
+int8_t PERCBACK[(1 << LOGSIZEB)][10 * (1 << BPSTEP)];
+int8_t PERCYHA[(1 << LOGSIZEY)][10 * (1 << YPSTEP)];
+int8_t PERCPATH[(1 << LOGSIZEP)][10 * (1 << PPSTEP)];
+int8_t PERCSLOC[(1 << LOGSIZES)][10 * (1 << SPSTEP)];
+int8_t PERCTLOC[(1 << LOGSIZET)][10 * (1 << TPSTEP)];
+
+// four  local histories components
+
+#define LOGLOCAL 10
+#define NLOCAL (1 << LOGLOCAL)
+#define INDLOCAL (PC & (NLOCAL - 1))
+long long L_shist[NLOCAL] = {0};
+
+#define LNB 15
+int     Lm[LNB] = {2, 4, 6, 9, 12, 16, 20, 24, 29, 34, 39, 44, 50, 56, 63};
+int8_t  LGEHLA[LNB][TABSIZE];
+int8_t* LGEHL[LNB] = {0};
+
+// Local history  +IMLI
+#define LINB 10
+int LIm[LNB] = {18, 20, 24, 29, 34, 39, 44, 50, 56, 63};
+
+int8_t  LIGEHLA[LNB][TABSIZE];
+int8_t* LIGEHL[LNB] = {0};
+
+#define LOGSECLOCAL 4
+#define NSECLOCAL (1 << LOGSECLOCAL)
+#define NB 3
+#define INDSLOCAL (((PC ^ (PC >> 5)) >> NB) & (NSECLOCAL - 1))
+long long S_slhist[NSECLOCAL] = {0};
+
+#define SNB 15
+int     Sm[SNB] = {2, 4, 6, 9, 12, 16, 20, 24, 29, 34, 39, 44, 50, 56, 63};
+int8_t  SGEHLA[SNB][TABSIZE];
+int8_t* SGEHL[SNB] = {0};
+
+#define LOGTLOCAL 4
+#define NTLOCAL (1 << LOGTLOCAL)
+#define INDTLOCAL (((PC ^ (PC >> 3) ^ (PC >> 6))) & (NTLOCAL - 1))
+long long T_slhist[NTLOCAL] = {0};
+
+#define TNB 15
+int     Tm[TNB] = {2, 4, 6, 9, 12, 16, 20, 24, 29, 34, 39, 44, 50, 56, 63};
+int8_t  TGEHLA[TNB][TABSIZE];
+int8_t* TGEHL[TNB] = {0};
+
+#define QPSTEP 6
+#define QWIDTH 60
+#define LOGSIZEQ (LOGSIZE)
+int8_t PERCQLOC[(1 << LOGSIZEQ)][10 * (1 << QPSTEP)];
+
+#define LOGQLOCAL 15
+#define NQLOCAL (1 << LOGQLOCAL)
+#define INDQLOCAL (((PC ^ (PC >> 2) ^ (PC >> 4) ^ (PC >> 8))) & (NQLOCAL - 1))
+long long Q_slhist[NQLOCAL] = {0};
+
+#define QNB 15
+int     Qm[QNB] = {2, 4, 6, 9, 12, 16, 20, 24, 29, 34, 39, 44, 50, 56, 63};
+int8_t  QGEHLA[QNB][TABSIZE];
+int8_t* QGEHL[QNB] = {0};
+
+// correlation at constant local history ? (without PC)
+#define QQNB 10
+int     QQm[QQNB] = {16, 20, 24, 29, 34, 39, 44, 50, 56, 63};
+int8_t  QQGEHLA[QQNB][TABSIZE];
+int8_t* QQGEHL[QQNB] = {0};
+
+
+// history at IMLI constant
+
+#define LOGTIMLI 12
+#define NTIMLI (1 << LOGTIMLI)
+#define INDIMLI (IMLIcount & (NTIMLI - 1))
+long long IMLIhist[NTIMLI] = {0};
+
+#define IMLINB 15
+int    IMLIm[IMLINB] = {2, 4, 6, 9, 12, 16, 20, 24, 29, 34, 39, 44, 50, 56, 63};
+int8_t IMLIGEHLA[IMLINB][TABSIZE];
+int8_t* IMLIGEHL[IMLINB] = {0};
+
+
+// about the skeleton histories: see CBP4
+#define YNB 15
+int Ym[SNB] = {2, 4, 6, 9, 12, 16, 20, 24, 29, 34, 39, 44, 50, 56, 63};
+
+int8_t  YGEHLA[SNB][TABSIZE];
+int8_t* YGEHL[SNB] = {0};
+
+long long YHA       = 0;
+int       LastBR[8] = {0};
+
+// about the IMLI in Micro 2015
+#define INB 5
+int Im[SNB] = {16, 19, 23, 29, 35};
+
+int8_t  IGEHLA[SNB][TABSIZE];
+int8_t* IGEHL[SNB] = {0};
+
+long long IMLIcount = 0;
+
+// corresponds to IMLI-OH in Micro 2015
+long long futurelocal;
+
+int8_t PAST[64];
+#define HISTTABLESIZE 65536
+int8_t histtable[HISTTABLESIZE];
+
+
+#define FNB 5
+int Fm[FNB] = {2, 4, 7, 11, 17};
+
+int8_t fGEHLA[FNB][TABSIZE];
+
+int8_t* fGEHL[FNB];
+
+
+// inherited from CBP4: usefulness ?
+#define BNB 10
+int Bm[BNB] = {2, 4, 6, 9, 12, 16, 20, 24, 29, 34};
+
+int8_t  BGEHLA[BNB][TABSIZE];
+int8_t* BGEHL[BNB] = {0};
+
+
+// close targets
+#define CNB 5
+int Cm[SNB] = {4, 8, 12, 20, 32};
+
+int8_t  CGEHLA[CNB][TABSIZE];
+int8_t* CGEHL[CNB] = {0};
+
+long long CHIST = 0;
+
+// more distant targets
+#define RNB 5
+int Rm[RNB] = {4, 8, 12, 20, 32};
+
+int8_t  RGEHLA[RNB][TABSIZE];
+int8_t* RGEHL[RNB] = {0};
+
+long long RHIST = 0;
+
+
+int PERCSUM;
+int Pupdatethreshold[(1 << LOGSIZE)] = {0};
+
+#define INDUPD (PC & ((1 << LOGSIZE) - 1))
+int updatethreshold;
+
+
+// the GEHL predictor
+#define MAXNHISTGEHL 209  // inherited from CBP4
+#ifndef LOGGEHL
+// base 2 logarithm of number of entries  on each GEHL  component
+#define LOGGEHL (LOGTAB + 1)
+#endif
+#define MINSTEP 2
+#define MINHISTGEHL 1
+static int8_t GEHL[1 << LOGGEHL][MAXNHISTGEHL + 1];  // GEHL tables
+int           mgehl[MAXNHISTGEHL + 1]     = {0};     // GEHL history lengths
+int           GEHLINDEX[MAXNHISTGEHL + 1] = {0};
+
+
+// The MACRHSP inspired predictor
+#define MAXNRHSP 80  // inherited from CBP4
+#define LOGRHSP LOGGEHL
+static int8_t RHSP[1 << LOGRHSP][MAXNRHSP + 1];  // RHSP tables
+int           mrhsp[MAXNRHSP + 1]     = {0};     // RHSP history lengths
+int           RHSPINDEX[MAXNRHSP + 1] = {0};
+
+int SUMRHSP;
+
+
+#define PERCWIDTH 8
+
+
+// local history management
+
+long long BHIST    = 0;
+uint64_t  lastaddr = 0;
+long long P_phist  = 0;
+long long GHIST    = 0;
+
+
+int SUMGEHL;
+
+// Another table
+
+#define LOGBIASFULL LOGTAB
+int8_t BiasFull[(1 << LOGBIASFULL)] = {0};
+
+#define INDBIASFULL                                                 \
+  (((PC << 4) ^ ((TypeFirstSum) + ((COPRED + (PRED << 1)) << 2))) & \
+   ((1 << (LOGBIASFULL)) - 1))
+
+#define LOGBIAS LOGTAB
+int8_t Bias[(1 << LOGBIAS)] = {0};
+
+#define INDBIAS (((PC << 1) ^ PRED) & ((1 << (LOGBIAS)) - 1))
+
+#define LOGBIASCOLT (LOGTAB)
+int8_t BiasColt[TABSIZE] = {0};
+
+#define INDBIASCOLT                                                    \
+  (((PC << 7) ^ PRED ^                                                 \
+    ((predtaken[0] ^ (predtaken[1] << 1) ^ (predtaken[2] << 2) ^       \
+      (predtaken[3] << 3) ^ (predtaken[4] << 4) ^ (predtaken[5] << 5)) \
+     << 1)) &                                                          \
+   ((1 << (LOGBIASCOLT)) - 1))
+
+
+// variables and tables for computing intermediate predictions
+int CTR[NPRED];
+int VAL;
+
+
+// variables for the finalr
+int indexfinal;
+int LFINAL;
+int Cupdatethreshold;
+//#define LOGSIZEFINAL LOGTAB
+//#define MAXSIZEFINAL  TABSIZE
+//#define SIZEFINAL  (1<<(LOGSIZEFINAL))
+int8_t GFINAL[TABSIZE]     = {0};
+int8_t GFINALCOLT[TABSIZE] = {0};
+
+int indexfinalcolt;
+// end finalr
+
+int FirstSum;
+int FirstThreshold;
+int TypeFirstSum;
+#define INDFIRST                                                           \
+  (((PC << 6) ^                                                            \
+    ((predtaken[0] ^ (predtaken[1] << 1) ^ (predtaken[2] << 2) ^           \
+      (predtaken[3] << 3) ^ (predtaken[4] << 4) ^ (predtaken[5] << 5)))) & \
+   ((1 << (LOGTAB)) - 1))
+int8_t FirstBIAS[(1 << LOGTAB)];
+int8_t TBias0[((1 << LOGTAB))] = {0};
+
+#define INDBIAS0 (((PC << 7) ^ CTR[0]) & ((1 << LOGTAB) - 1))
+int8_t TBias1[((1 << LOGTAB))] = {0};
+
+#define INDBIAS1 (((PC << 7) ^ CTR[1]) & ((1 << LOGTAB) - 1))
+int8_t TBias2[((1 << LOGTAB))] = {0};
+
+#define INDBIAS2 (((PC << 7) ^ CTR[2]) & ((1 << LOGTAB) - 1))
+int8_t TBias3[((1 << LOGTAB))] = {0};
+
+#define INDBIAS3 (((PC << 7) ^ CTR[3]) & ((1 << LOGTAB) - 1))
+int8_t TBias4[((1 << LOGTAB))] = {0};
+
+#define INDBIAS4 (((PC << 7) ^ CTR[4]) & ((1 << LOGTAB) - 1))
+int8_t TBias5[((1 << LOGTAB))] = {0};
+
+#define INDBIAS5 (((PC << 3) ^ CTR[5]) & ((1 << LOGTAB) - 1))
+
+int8_t SB0[((1 << LOGTAB))] = {0};
+
+#define INDSB0                                                         \
+  (((PC << 13) ^                                                       \
+    ((predtaken[0] ^ (predtaken[1] << 1) ^ (predtaken[2] << 2) ^       \
+      (predtaken[3] << 3) ^ (predtaken[4] << 4) ^ (predtaken[5] << 5)) \
+     << 7) ^                                                           \
+    CTR[0]) &                                                          \
+   ((1 << LOGTAB) - 1))
+int8_t SB1[((1 << LOGTAB))] = {0};
+
+#define INDSB1                                                         \
+  (((PC << 13) ^                                                       \
+    ((predtaken[0] ^ (predtaken[1] << 1) ^ (predtaken[2] << 2) ^       \
+      (predtaken[3] << 3) ^ (predtaken[4] << 4) ^ (predtaken[5] << 5)) \
+     << 7) ^                                                           \
+    CTR[1]) &                                                          \
+   ((1 << LOGTAB) - 1))
+int8_t SB2[((1 << LOGTAB))] = {0};
+
+#define INDSB2                                                         \
+  (((PC << 13) ^                                                       \
+    ((predtaken[0] ^ (predtaken[1] << 1) ^ (predtaken[2] << 2) ^       \
+      (predtaken[3] << 3) ^ (predtaken[4] << 4) ^ (predtaken[5] << 5)) \
+     << 7) ^                                                           \
+    CTR[2]) &                                                          \
+   ((1 << LOGTAB) - 1))
+int8_t SB3[((1 << LOGTAB))] = {0};
+
+#define INDSB3                                                         \
+  (((PC << 13) ^                                                       \
+    ((predtaken[0] ^ (predtaken[1] << 1) ^ (predtaken[2] << 2) ^       \
+      (predtaken[3] << 3) ^ (predtaken[4] << 4) ^ (predtaken[5] << 5)) \
+     << 7) ^                                                           \
+    CTR[3]) &                                                          \
+   ((1 << LOGTAB) - 1))
+int8_t SB4[((1 << LOGTAB))] = {0};
+
+#define INDSB4                                                         \
+  (((PC << 13) ^                                                       \
+    ((predtaken[0] ^ (predtaken[1] << 1) ^ (predtaken[2] << 2) ^       \
+      (predtaken[3] << 3) ^ (predtaken[4] << 4) ^ (predtaken[5] << 5)) \
+     << 7) ^                                                           \
+    CTR[4]) &                                                          \
+   ((1 << LOGTAB) - 1))
+int8_t SB5[((1 << LOGTAB))] = {0};
+
+#define INDSB5                                                         \
+  (((PC << 13) ^                                                       \
+    ((predtaken[0] ^ (predtaken[1] << 1) ^ (predtaken[2] << 2) ^       \
+      (predtaken[3] << 3) ^ (predtaken[4] << 4) ^ (predtaken[5] << 5)) \
+     << 7) ^                                                           \
+    CTR[5]) &                                                          \
+   ((1 << LOGTAB) - 1))
+
+
+// global history
+#define HISTBUFFERLENGTH (1 << 18)
+uint8_t ghist[HISTBUFFERLENGTH];
+int     ptghist;
+
+// utilities for computing GEHL indices
+folded_history chgehl_i[MAXNHISTGEHL + 1];
+folded_history chrhsp_i[MAXNRHSP + 1];
+
+
+int NGEHL;
+int NRHSP;
+int MAXHISTGEHL;
+
+
+#define MTAGE_ASSERT(cond)                                           \
+  if(!(cond)) {                                                      \
+    fprintf(stderr, "file %s assert line %d\n", __FILE__, __LINE__); \
+    abort();                                                         \
+  }
+
+
+#define DECPTR(ptr, size) \
+  {                       \
+    ptr--;                \
+    if(ptr == (-1)) {     \
+      ptr = size - 1;     \
+    }                     \
+  }
+
+#define INCSAT(ctr, max) \
+  {                      \
+    if(ctr < (max))      \
+      ctr++;             \
+  }
+
+#define DECSAT(ctr, min) \
+  {                      \
+    if(ctr > (min))      \
+      ctr--;             \
+  }
+
+
+// for updating up-down saturating counters
+bool ctrupdate(int8_t& ctr, bool inc, int nbits) {
+  MTAGE_ASSERT(nbits <= 8);
+  int  ctrmin = -(1 << (nbits - 1));
+  int  ctrmax = -ctrmin - 1;
+  bool issat  = (ctr == ctrmax) || (ctr == ctrmin);
+  if(inc) {
+    INCSAT(ctr, ctrmax);
+  } else {
+    DECSAT(ctr, ctrmin);
+  }
+  return issat && ((ctr == ctrmax) || (ctr == ctrmin));
+}
+
+
+void path_history::init(int hlen) {
+  hlength = hlen;
+  h       = new unsigned[hlen];
+  for(int i = 0; i < hlength; i++) {
+    h[i] = 0;
+  }
+  ptr = 0;
+}
+
+
+void path_history::insert(unsigned val) {
+  DECPTR(ptr, hlength);
+  h[ptr] = val;
+}
+
+
+unsigned& path_history::operator[](int n) {
+  MTAGE_ASSERT((n >= 0) && (n < hlength));
+  int k = ptr + n;
+  if(k >= hlength) {
+    k -= hlength;
+  }
+  MTAGE_ASSERT((k >= 0) && (k < hlength));
+  return h[k];
+}
+
+
+compressed_history::compressed_history() {
+  reset();
+}
+
+
+void compressed_history::reset() {
+  comp = 0;  // must be consistent with path_history::reset()
+}
+
+
+void compressed_history::init(int original_length, int compressed_length,
+                              int injected_bits) {
+  olength  = original_length;
+  clength  = compressed_length;
+  nbits    = injected_bits;
+  outpoint = olength % clength;
+  MTAGE_ASSERT(clength < 32);
+  MTAGE_ASSERT(nbits <= clength);
+  mask1 = (1 << clength) - 1;
+  mask2 = (1 << nbits) - 1;
+  reset();
+}
+
+
+void compressed_history::rotateleft(unsigned& x, int m) {
+  MTAGE_ASSERT(m < clength);
+  MTAGE_ASSERT((x >> clength) == 0);
+  unsigned y = x >> (clength - m);
+  x          = (x << m) | y;
+  x &= mask1;
+}
+
+
+void compressed_history::update(path_history& ph) {
+  rotateleft(comp, 1);
+  unsigned inbits  = ph[0] & mask2;
+  unsigned outbits = ph[olength] & mask2;
+  rotateleft(outbits, outpoint);
+  comp ^= inbits ^ outbits;
+}
+
+
+coltentry::coltentry() {
+  for(int i = 0; i < (1 << NPRED); i++) {
+    c[i] = ((i >> (NPRED - 1)) & 1) ? 1 : -2;
+  }
+}
+
+
+int8_t& coltentry::ctr(bool predtaken[NPRED]) {
+  int v = 0;
+  for(int i = 0; i < NPRED; i++) {
+    v = (v << 1) | ((predtaken[i]) ? 1 : 0);
+  }
+  return c[v];
+}
+
+
+int8_t& colt::ctr(uint64_t pc, bool predtaken[NPRED]) {
+  int i = pc & ((1 << LOGCOLT) - 1);
+  return c[i].ctr(predtaken);
+}
+
+
+bool colt::predict(uint64_t pc, bool predtaken[NPRED]) {
+  return (ctr(pc, predtaken) >= 0);
+}
+
+
+void colt::update(uint64_t pc, bool predtaken[NPRED], bool taken) {
+  ctrupdate(ctr(pc, predtaken), taken, COLTBITS);
+}
+
+
+bftable::bftable() {
+  for(int i = 0; i < BFTSIZE; i++) {
+    freq[i] = 0;
+  }
+}
+
+
+int& bftable::getfreq(uint64_t pc) {
+  int i = pc % BFTSIZE;
+  MTAGE_ASSERT((i >= 0) && (i < BFTSIZE));
+  return freq[i];
+}
+
+
+void subpath::init(int ng, int hist[], int logg, int tagbits, int pathbits,
+                   int hp) {
+  MTAGE_ASSERT(ng > 0);
+  numg = ng;
+  ph.init(hist[numg - 1] + 1);
+  chg       = new compressed_history[numg];
+  chgg      = new compressed_history[numg];
+  cht       = new compressed_history[numg];
+  chtt      = new compressed_history[numg];
+  int ghlen = 0;
+  for(int i = numg - 1; i >= 0; i--) {
+    ghlen = (ghlen < hist[numg - 1 - i]) ? hist[numg - 1 - i] : ghlen + 1;
+    chg[i].init(ghlen, logg, pathbits);
+    chgg[i].init(ghlen, logg - hp, pathbits);
+    cht[i].init(ghlen, tagbits, pathbits);
+    chtt[i].init(ghlen, tagbits - 1, pathbits);
+  }
+}
+
+
+void subpath::init(int ng, int minhist, int maxhist, int logg, int tagbits,
+                   int pathbits, int hp) {
+  int* h = new int[ng];
+  for(int i = 0; i < ng; i++) {
+    h[i] = minhist * pow((double)maxhist / minhist, (double)i / (ng - 1));
+  }
+  init(ng, h, logg, tagbits, pathbits, hp);
+}
+
+
+void subpath::update(uint64_t targetpc, bool taken) {
+  ph.insert((targetpc << 1) | taken);
+  for(int i = 0; i < numg; i++) {
+    chg[i].update(ph);
+    chgg[i].update(ph);
+    cht[i].update(ph);
+    chtt[i].update(ph);
+  }
+}
+
+
+unsigned subpath::cg(int bank) {
+  MTAGE_ASSERT((bank >= 0) && (bank < numg));
+  return chg[bank].comp;
+}
+
+
+unsigned subpath::cgg(int bank) {
+  MTAGE_ASSERT((bank >= 0) && (bank < numg));
+  return chgg[bank].comp << (chg[bank].clength - chgg[bank].clength);
+}
+
+
+unsigned subpath::ct(int bank) {
+  MTAGE_ASSERT((bank >= 0) && (bank < numg));
+  return cht[bank].comp;
+}
+
+
+unsigned subpath::ctt(int bank) {
+  MTAGE_ASSERT((bank >= 0) && (bank < numg));
+  return chtt[bank].comp << (cht[bank].clength - chtt[bank].clength);
+}
+
+
+spectrum::spectrum() {
+  size = 0;
+  p    = NULL;
+}
+
+
+void spectrum::init(int sz, int ng, int minhist, int maxhist, int logg,
+                    int tagbits, int pathbits, int hp) {
+  size = sz;
+  p    = new subpath[size];
+  for(int i = 0; i < size; i++) {
+    p[i].init(ng, minhist, maxhist, logg, tagbits, pathbits, hp);
+  }
+}
+
+
+void freqbins::init(int nb) {
+  nbins   = nb;
+  maxfreq = 0;
+}
+
+
+int freqbins::find(int bfreq) {
+  // find in which frequency bin the input branch frequency falls
+  MTAGE_ASSERT(bfreq >= 0);
+  int b = -1;
+  int f = maxfreq;
+  for(int i = 0; i < nbins; i++) {
+    f = f >> FRATIOBITS;
+    if(bfreq >= f) {
+      b = i;
+      break;
+    }
+  }
+  if(b < 0) {
+    b = nbins - 1;
+  }
+  return b;
+}
+
+
+void freqbins::update(int bfreq) {
+  if(bfreq > maxfreq) {
+    MTAGE_ASSERT(bfreq == (maxfreq + 1));
+    maxfreq = bfreq;
+  }
+}
+
+
+gentry::gentry() {
+  ctr = 0;
+  tag = 0;
+  u   = 0;
+}
+
+
+tage::tage() {
+  b     = NULL;
+  g     = NULL;
+  gi    = NULL;
+  postp = NULL;
+  nmisp = 0;
+}
+
+
+void tage::init(const char* nm, int ng, int logb, int logg, int tagb, int ctrb,
+                int ppb, int ru, int caph) {
+  MTAGE_ASSERT(ng > 1);
+  MTAGE_ASSERT(logb < 30);
+  MTAGE_ASSERT(logg < 30);
+  name      = nm;
+  numg      = ng;
+  bsize     = 1 << logb;
+  gsize     = 1 << logg;
+  tagbits   = tagb;
+  ctrbits   = ctrb;
+  postpbits = ppb;
+  postpsize = 1 << (2 * ctrbits + 1);
+  b         = new int8_t[bsize];
+  for(int i = 0; i < bsize; i++) {
+    b[i] = 0;
+  }
+  g = new gentry*[numg];
+  for(int i = 0; i < numg; i++) {
+    g[i] = new gentry[gsize];
+  }
+  gi    = new int[numg];
+  postp = new int8_t[postpsize];
+  for(int i = 0; i < postpsize; i++) {
+    postp[i] = -(((i >> 1) >> (ctrbits - 1)) & 1);
+  }
+  allocfail = 0;
+  rampup    = ru;
+  caphist   = caph;
+}
+
+
+int tage::bindex(uint64_t pc) {
+  return pc & (bsize - 1);
+}
+
+
+int tage::gindex(uint64_t pc, subpath& p, int bank) {
+  return (pc ^ p.cg(bank) ^ p.cgg(bank)) & (gsize - 1);
+}
+
+
+int tage::gtag(uint64_t pc, subpath& p, int bank) {
+  return (pc ^ p.ct(bank) ^ p.ctt(bank)) & ((1 << tagbits) - 1);
+}
+
+
+int tage::postp_index() {
+  // post predictor index function
+  int ctr[2];
+  for(int i = 0; i < 2; i++) {
+    ctr[i] = (i < (int)hit.size()) ? getg(hit[i]).ctr : b[bi];
+  }
+  int v = 0;
+  for(int i = 2; i >= 0; i--) {
+    v = (v << ctrbits) | (ctr[i] & (((1 << ctrbits) - 1)));
+  }
+
+  int u0 = (hit.size() > 0) ? (getg(hit[0]).u > 0) : 1;
+  v      = (v << 1) | u0;
+  v &= postpsize - 1;
+  VAL = v;
+
+  return v;
+}
+
+
+gentry& tage::getg(int i) {
+  MTAGE_ASSERT((i >= 0) && (i < numg));
+  return g[i][gi[i]];
+}
+
+
+bool tage::condbr_predict(uint64_t pc, subpath& p) {
+  hit.clear();
+  bi = bindex(pc);
+  for(int i = 0; i < numg; i++) {
+    gi[i] = gindex(pc, p, i);
+    if(g[i][gi[i]].tag == gtag(pc, p, i)) {
+      hit.push_back(i);
+    }
+  }
+
+  predtaken    = (hit.size() > 0) ? (getg(hit[0]).ctr >= 0) : (b[bi] >= 0);
+  altpredtaken = (hit.size() > 1) ? (getg(hit[1]).ctr >= 0) : (b[bi] >= 0);
+  ppi          = postp_index();
+  MTAGE_ASSERT(ppi < postpsize);
+  postpredtaken = (postp[ppi] >= 0);
+  return postpredtaken;
+}
+
+
+void tage::uclear() {
+  for(int i = 0; i < numg; i++) {
+    for(int j = 0; j < gsize; j++) {
+      if(g[i][j].u)
+        g[i][j].u--;
+    }
+  }
+}
+
+
+void tage::galloc(int i, uint64_t pc, bool taken, subpath& p) {
+  getg(i).tag = gtag(pc, p, i);
+  getg(i).ctr = (taken) ? 0 : -1;
+  getg(i).u   = 0;
+}
+
+
+void tage::aggressive_update(uint64_t pc, bool taken, subpath& p) {
+  // update policy used during ramp up
+  bool allsat = true;
+
+  // AS: slightly improved from CBP4
+  if(hit.size() > 0) {
+    bool inter = (getg(hit[0]).ctr >= 0);
+    allsat &= ctrupdate(getg(hit[0]).ctr, taken, ctrbits);
+    int  start = 1;
+    bool Done  = false;
+    bool STOP  = false;
+
+    if(getg(hit[0]).u == 0) {
+      if(hit.size() > 1) {
+        if((getg(hit[1]).ctr >= 0) != inter)
+          STOP = true;
+
+        start = 2;
+        allsat &= ctrupdate(getg(hit[1]).ctr, taken, ctrbits);
+      } else {
+        Done = true;
+        allsat &= ctrupdate(b[bi], taken, ctrbits);
+      }
+    }
+
+    if(!STOP)
+      for(int i = start; i < (int)hit.size(); i++) {
+        if((getg(hit[i]).ctr >= 0) == inter)
+          allsat &= ctrupdate(getg(hit[i]).ctr, taken, ctrbits);
+        else {
+          Done = true;
+          break;
+        }
+      }
+    if(!Done)
+      if((b[bi] >= 0) == inter)
+        allsat &= ctrupdate(b[bi], taken, ctrbits);
+  } else {
+    ctrupdate(b[bi], taken, ctrbits);
+  }
+
+
+  int i = (hit.size() > 0) ? hit[0] : numg;
+  while(--i >= 0) {
+    if(getg(i).u != 0)
+      continue;
+    if(!allsat || (p.chg[i].olength <= caphist)) {
+      galloc(i, pc, taken, p);
+    }
+  }
+}
+
+
+void tage::careful_update(uint64_t pc, bool taken, subpath& p) {
+  // update policy devised by Andre Seznec for the ISL-TAGE predictor (MICRO
+  // 2011)
+
+  if(hit.size() > 0) {
+    ctrupdate(getg(hit[0]).ctr, taken, ctrbits);
+
+    if(getg(hit[0]).u == 0) {
+      if(hit.size() > 1) {
+        ctrupdate(getg(hit[1]).ctr, taken, ctrbits);
+      } else {
+        ctrupdate(b[bi], taken, ctrbits);
+      }
+    }
+  } else {
+    ctrupdate(b[bi], taken, ctrbits);
+  }
+
+
+  if(mispred) {
+    int nalloc = 0;
+    int i      = (hit.size() > 0) ? hit[0] : numg;
+    while(--i >= 0) {
+      if(getg(i).u == 0) {
+        galloc(i, pc, taken, p);
+        DECSAT(allocfail, 0);
+        i--;
+        nalloc++;
+        if(nalloc == MAXALLOC)
+          break;
+      } else {
+        INCSAT(allocfail, ALLOCFAILMAX);
+        if(allocfail == ALLOCFAILMAX) {
+          uclear();
+        }
+      }
+    }
+  }
+}
+
+
+bool tage::condbr_update(uint64_t pc, bool taken, subpath& p) {
+  mispred = (postpredtaken != taken);
+
+  if(mispred) {
+    nmisp++;
+  }
+
+  if(nmisp < rampup) {
+    aggressive_update(pc, taken, p);
+  } else {
+    careful_update(pc, taken, p);
+  }
+
+  // update u bit (see TAGE, JILP 2006)
+  if(predtaken != altpredtaken) {
+    if(altpredtaken != predtaken)
+      if(predtaken == taken)
+        ctrupdate(getg(hit[0]).u, true, 3);
+  }
+
+  // update post pred
+  ctrupdate(postp[ppi], taken, postpbits);
+
+  return mispred;
+}
+
+
+void tage::printconfig(subpath& p) {
+  printf("%s path lengths: ", name.c_str());
+  for(int i = numg - 1; i >= 0; i--) {
+    printf("%d ", p.chg[i].olength);
+  }
+  printf("\n");
+}
+
+
+/////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////
+
+
+void folded_history::init(int original_length, int compressed_length, int N) {
+  comp     = 0;
+  OLENGTH  = original_length;
+  CLENGTH  = compressed_length;
+  OUTPOINT = OLENGTH % CLENGTH;
+}
+
+void folded_history::update(uint8_t* h, int PT) {
+  comp = (comp << 1) ^ h[PT & (HISTBUFFERLENGTH - 1)];
+  comp ^= h[(PT + OLENGTH) & (HISTBUFFERLENGTH - 1)] << OUTPOINT;
+  comp ^= (comp >> CLENGTH);
+  comp = (comp) & ((1 << CLENGTH) - 1);
+}
+
+
+/////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////
+
+
+MTAGE::MTAGE(void) {
+  sp[0].init(P0_SPSIZE, P0_NUMG, P0_MINHIST, P0_MAXHIST, P0_LOGG, TAGBITS,
+             PATHBITS, P0_HASHPARAM);
+  sp[1].init(P1_SPSIZE, P1_NUMG, P1_MINHIST, P1_MAXHIST, P1_LOGG, TAGBITS,
+             PATHBITS, P1_HASHPARAM);
+  sp[2].init(P2_SPSIZE, P2_NUMG, P2_MINHIST, P2_MAXHIST, P2_LOGG, TAGBITS,
+             PATHBITS, P2_HASHPARAM);
+  sp[3].init(P3_SPSIZE, P3_NUMG, P3_MINHIST, P3_MAXHIST, P3_LOGG, TAGBITS,
+             PATHBITS, P3_HASHPARAM);
+  sp[4].init(P4_SPSIZE, P4_NUMG, P4_MINHIST, P4_MAXHIST, P4_LOGG, TAGBITS,
+             PATHBITS, P4_HASHPARAM);
+  sp[5].init(P5_SPSIZE, P5_NUMG, P5_MINHIST, P5_MAXHIST, P5_LOGG, TAGBITS,
+             PATHBITS, P5_HASHPARAM);
+
+  pred[0].init("G", P0_NUMG, P0_LOGB, P0_LOGG, TAGBITS, CTRBITS, POSTPBITS,
+               P0_RAMPUP, CAPHIST);
+  pred[1].init("A", P1_NUMG, P1_LOGB, P1_LOGG, TAGBITS, CTRBITS, POSTPBITS,
+               P1_RAMPUP, CAPHIST);
+  pred[2].init("S", P2_NUMG, P2_LOGB, P2_LOGG, TAGBITS, CTRBITS, POSTPBITS,
+               P2_RAMPUP, CAPHIST);
+  pred[3].init("s", P3_NUMG, P3_LOGB, P3_LOGG, TAGBITS, CTRBITS, POSTPBITS,
+               P3_RAMPUP, CAPHIST);
+  pred[4].init("F", P4_NUMG, P4_LOGB, P4_LOGG, TAGBITS, CTRBITS, POSTPBITS,
+               P4_RAMPUP, CAPHIST);
+
+  pred[5].init("g", P5_NUMG, P5_LOGB, P5_LOGG, TAGBITS, CTRBITS, POSTPBITS,
+               P5_RAMPUP, CAPHIST);
+
+  bfreq.init(P4_SPSIZE);  // number of frequency bins = P4 spectrum size
+
+  initSC();
+}
+
+
+bool MTAGE::GetPrediction(uint64_t PC) {
+  subp[0] = &sp[0].p[0];                             // global path
+  subp[1] = &sp[1].p[PC % P1_SPSIZE];                // per-address subpath
+  subp[2] = &sp[2].p[(PC >> P2_PARAM) % P2_SPSIZE];  // per-set subpath
+  subp[3] = &sp[3].p[(PC >> P3_PARAM) % P3_SPSIZE];  // another per-set subpath
+  int f   = bfreq.find(bft.getfreq(PC));
+  MTAGE_ASSERT((f >= 0) && (f < P4_SPSIZE));
+  subp[4] = &sp[4].p[f];  // frequency subpath
+  subp[5] = &sp[5].p[0];  // global backward path
+
+  for(int i = 0; i < NPRED; i++) {
+    predtaken[i] = pred[i].condbr_predict(PC, *subp[i]);
+    CTR[i] = VAL;  // 7 bits of information: the two longest hitting counters +
+                   // the u bit
+  }
+
+  // The TAGE combiner
+  // the neural combination
+  int ctr  = FirstBIAS[INDFIRST];
+  FirstSum = (2 * ctr + 1);
+  ctr      = TBias0[INDBIAS0];
+  FirstSum += (2 * ctr + 1);
+  ctr = TBias1[INDBIAS1];
+  FirstSum += (2 * ctr + 1);
+  ctr = TBias2[INDBIAS2];
+  FirstSum += (2 * ctr + 1);
+  ctr = TBias3[INDBIAS3];
+  FirstSum += (2 * ctr + 1);
+  ctr = TBias4[INDBIAS4];
+  FirstSum += (2 * ctr + 1);
+  ctr = TBias5[INDBIAS5];
+  FirstSum += (2 * ctr + 1);
+  ctr = SB0[INDSB0];
+  FirstSum += (2 * ctr + 1);
+  ctr = SB1[INDSB1];
+  FirstSum += (2 * ctr + 1);
+  ctr = SB2[INDSB2];
+  FirstSum += (2 * ctr + 1);
+  ctr = SB3[INDSB3];
+  FirstSum += (2 * ctr + 1);
+  ctr = SB4[INDSB4];
+  FirstSum += (2 * ctr + 1);
+  ctr = SB5[INDSB5];
+  FirstSum += (2 * ctr + 1);
+
+  pred_inter = (FirstSum >= 0);
+  // Extracting the confidence level
+  if(abs(FirstSum) < FirstThreshold / 4)
+    TypeFirstSum = 0;
+  else if(abs(FirstSum) < FirstThreshold / 2)
+    TypeFirstSum = 1;
+  else if(abs(FirstSum) < FirstThreshold)
+    TypeFirstSum = 2;
+  else
+    TypeFirstSum = 3;
+  // the COLT predicition
+  COPRED = co.predict(PC, predtaken);
+
+  // the statistical correlator
+  predSC         = SCpredict(PC, pred_inter);
+  bool finalpred = predSC;
+
+
+  finalpred = FinalSCpredict(PC, pred_inter);
+  return finalpred;
+}
+
+
+/////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////
+
+void MTAGE::UpdatePredictor(uint64_t PC, OpType OPTYPE, bool resolveDir,
+                            bool predDir, uint64_t branchTarget) {
+  if (PERFECT_BP && !subp[0])
+    GetPrediction(PC);
+
+  XX += (predtaken[0] != resolveDir);
+  YY += (pred_inter != resolveDir);
+  ZZ += (COPRED != resolveDir);
+  TT += (predSC != resolveDir);
+
+  // the TAGE stage
+  uint64_t ForUpdate = (resolveDir) ? (branchTarget << 1) ^ PC : PC;
+  for(int i = 0; i < NPRED - 1; i++) {
+    pred[i].condbr_update(PC, resolveDir, *subp[i]);
+    subp[i]->update(ForUpdate, resolveDir);
+  }
+  pred[NPRED - 1].condbr_update(PC, resolveDir, *subp[NPRED - 1]);
+  if(branchTarget < PC)
+    subp[NPRED - 1]->update(ForUpdate, ((branchTarget < PC) & resolveDir));
+  bfreq.update(bft.getfreq(PC));
+  bft.getfreq(PC)++;
+
+  // update of the TAGE combiner
+
+  if((abs(FirstSum) < FirstThreshold) || (pred_inter != resolveDir)) {
+    if(pred_inter != resolveDir) {
+      FirstThreshold += 1;
+    } else {
+      FirstThreshold -= 1;
+    }
+    ctrupdate(FirstBIAS[INDFIRST], resolveDir, PERCWIDTH);
+    ctrupdate(TBias0[INDBIAS0], resolveDir, PERCWIDTH);
+    ctrupdate(TBias1[INDBIAS1], resolveDir, PERCWIDTH);
+    ctrupdate(TBias2[INDBIAS1], resolveDir, PERCWIDTH);
+    ctrupdate(TBias3[INDBIAS3], resolveDir, PERCWIDTH);
+    ctrupdate(TBias4[INDBIAS4], resolveDir, PERCWIDTH);
+    ctrupdate(TBias5[INDBIAS5], resolveDir, PERCWIDTH);
+    ctrupdate(SB0[INDSB0], resolveDir, PERCWIDTH);
+    ctrupdate(SB1[INDSB1], resolveDir, PERCWIDTH);
+    ctrupdate(SB2[INDSB1], resolveDir, PERCWIDTH);
+    ctrupdate(SB3[INDSB3], resolveDir, PERCWIDTH);
+    ctrupdate(SB4[INDSB4], resolveDir, PERCWIDTH);
+    ctrupdate(SB5[INDSB5], resolveDir, PERCWIDTH);
+  }
+  co.update(PC, predtaken, resolveDir);
+  // end of the TAGE combiner
+
+  // the statistical corrector
+
+  UpdateSC(PC, resolveDir, pred_inter);
+
+  // The final stage
+  UpdateFinalSC(PC, resolveDir);
+
+
+  HistoryUpdate(PC, 1, resolveDir, branchTarget, ptghist, chgehl_i, chrhsp_i);
+}
+
+
+/////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////
+void MTAGE::TrackOtherInst(uint64_t PC, OpType opType, bool taken,
+                           uint64_t branchTarget) {
+  // also update the global path with unconditional branches
+  uint64_t PC0           = (PC ^ (PC >> 2));
+  uint64_t branchTarget0 = (branchTarget ^ (branchTarget >> 2));
+  uint64_t ForUpdate     = (branchTarget0 << 1) ^ PC0;
+  sp[0].p[0].update(ForUpdate, true);
+  sp[5].p[0].update(ForUpdate, true);
+  HistoryUpdate(PC, 0, true, branchTarget, ptghist, chgehl_i, chrhsp_i);
+}
+
+
+void MTAGE::initSC() {
+  NRHSP       = 80;
+  NGEHL       = 209;
+  MAXHISTGEHL = 1393;
+
+  for(int i = 0; i < HISTBUFFERLENGTH; i++)
+    ghist[0] = 0;
+
+  ptghist = 0;
+
+  // GEHL initialization
+  mgehl[0]     = 0;
+  mgehl[1]     = MINHISTGEHL;
+  mgehl[NGEHL] = MAXHISTGEHL;
+
+  for(int i = 2; i <= NGEHL; i++)
+    mgehl[i] = (int)(((double)MINHISTGEHL *
+                      pow((double)MAXHISTGEHL / (double)MINHISTGEHL,
+                          (double)(i - 1) / (double)(NGEHL - 1))) +
+                     0.5);
+
+  // just guarantee that all history lengths are distinct
+
+  for(int i = 1; i <= NGEHL; i++)
+    if(mgehl[i] <= mgehl[i - 1] + MINSTEP)
+      mgehl[i] = mgehl[i - 1] + MINSTEP;
+
+  for(int i = 1; i <= NGEHL; i++)
+    chgehl_i[i].init(mgehl[i], LOGGEHL, ((i & 1)) ? i : 1);
+
+  // initialization of GEHL tables
+
+  for(int j = 0; j < (1 << LOGGEHL); j++)
+    for(int i = 0; i <= NGEHL; i++)
+      GEHL[j][i] = (i & 1) ? -4 : 3;
+
+  // RHSP initialization
+
+  for(int i = 1; i <= NRHSP; i++)
+    mrhsp[i] = 6 * i;
+
+  for(int i = 1; i <= NRHSP; i++)
+    chrhsp_i[i].init(mrhsp[i], LOGRHSP, ((i & 1)) ? i : 1);
+
+  // initialization of RHSP tables
+
+  for(int j = 0; j < (1 << LOGRHSP); j++)
+    for(int i = 0; i <= NRHSP; i++)
+      RHSP[j][i] = (i & 1) ? -4 : 3;
+
+  updatethreshold  = 100;
+  Cupdatethreshold = 11;
+
+  for(int i = 0; i < (1 << LOGSIZE); i++)
+    Pupdatethreshold[i] = 0;
+
+  for(int i = 0; i < LNB; i++)
+    LGEHL[i] = &LGEHLA[i][0];
+  for(int i = 0; i < LINB; i++)
+    LIGEHL[i] = &LIGEHLA[i][0];
+  for(int i = 0; i < SNB; i++)
+    SGEHL[i] = &SGEHLA[i][0];
+
+  for(int i = 0; i < QNB; i++)
+    QGEHL[i] = &QGEHLA[i][0];
+
+  for(int i = 0; i < TNB; i++)
+    TGEHL[i] = &TGEHLA[i][0];
+  for(int i = 0; i < IMLINB; i++)
+    IMLIGEHL[i] = &IMLIGEHLA[i][0];
+
+
+  for(int i = 0; i < BNB; i++)
+    BGEHL[i] = &BGEHLA[i][0];
+
+  for(int i = 0; i < YNB; i++)
+    YGEHL[i] = &YGEHLA[i][0];
+  for(int i = 0; i < INB; i++)
+    IGEHL[i] = &IGEHLA[i][0];
+
+  for(int i = 0; i < FNB; i++)
+    fGEHL[i] = &fGEHLA[i][0];
+
+#ifdef LOOPMTAGE
+  ltable = new lentry[1 << (LOGL)];
+#endif
+#ifdef LOOPMTAGE
+  LVALID   = false;
+  WITHLOOP = -1;
+#endif
+  for(int i = 0; i < LNB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        LGEHL[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < SNB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        SGEHL[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < QNB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        QGEHL[i][j] = -1;
+      }
+    }
+  for(int i = 0; i < LINB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        LIGEHL[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < TNB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        TGEHL[i][j] = -1;
+      }
+    }
+  for(int i = 0; i < IMLINB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        IMLIGEHL[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < BNB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        BGEHL[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < YNB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        YGEHL[i][j] = -1;
+      }
+    }
+  for(int i = 0; i < INB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        IGEHL[i][j] = -1;
+      }
+    }
+  for(int i = 0; i < FNB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        fGEHL[i][j] = -1;
+      }
+    }
+  for(int i = 0; i < CNB; i++)
+    CGEHL[i] = &CGEHLA[i][0];
+  for(int i = 0; i < CNB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        CGEHL[i][j] = -1;
+      }
+    }
+  for(int i = 0; i < RNB; i++)
+    RGEHL[i] = &RGEHLA[i][0];
+  for(int i = 0; i < RNB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        RGEHL[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < QQNB; i++)
+    QQGEHL[i] = &QQGEHLA[i][0];
+  for(int i = 0; i < QQNB; i++)
+    for(int j = 0; j < TABSIZE; j++) {
+      if(j & 1) {
+        QQGEHL[i][j] = -1;
+      }
+    }
+
+
+  for(int j = 0; j < (1 << LOGBIAS); j++)
+    Bias[j] = (j & 1) ? 15 : -16;
+
+  for(int j = 0; j < (1 << LOGBIASCOLT); j++)
+    BiasColt[j] = (j & 1) ? 0 : -1;
+
+  for(int i = 0; i < (1 << LOGSIZES); i++)
+    for(int j = 0; j < (SWIDTH / SPSTEP) * (1 << SPSTEP); j++) {
+      if(j & 1) {
+        PERCSLOC[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < (1 << LOGSIZEQ); i++)
+    for(int j = 0; j < (QWIDTH / QPSTEP) * (1 << QPSTEP); j++) {
+      if(j & 1) {
+        PERCQLOC[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < (1 << LOGSIZEP); i++)
+    for(int j = 0; j < (GWIDTH / GPSTEP) * (1 << GPSTEP); j++) {
+      if(j & 1) {
+        PERC[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < (1 << LOGSIZEL); i++)
+    for(int j = 0; j < (LWIDTH / LPSTEP) * (1 << LPSTEP); j++) {
+      if(j & 1) {
+        PERCLOC[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < (1 << LOGSIZEB); i++)
+    for(int j = 0; j < ((BWIDTH / BPSTEP)) * (1 << BPSTEP); j++) {
+      if(j & 1) {
+        PERCBACK[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < (1 << LOGSIZEY); i++)
+    for(int j = 0; j < (YWIDTH / YPSTEP) * (1 << YPSTEP); j++) {
+      if(j & 1) {
+        PERCYHA[i][j] = -1;
+      }
+    }
+
+  for(int i = 0; i < (1 << LOGSIZEP); i++)
+    for(int j = 0; j < (PWIDTH / PPSTEP) * (1 << PPSTEP); j++) {
+      if(j & 1) {
+        PERCPATH[i][j] = -1;
+      }
+    }
+}
+
+
+void MTAGE::HistoryUpdate(uint64_t PC, uint8_t brtype, bool taken,
+                          uint64_t target, int& Y, folded_history* K,
+
+                          folded_history* L) {
+#define OPTYPE_BRANCH_COND 1
+  // History skeleton
+  bool V = false;
+
+  for(int i = 0; i <= 7; i++)
+    if(LastBR[i] == (int)PC)
+      V = true;
+
+  for(int i = 7; i >= 1; i--)
+    LastBR[i] = LastBR[i - 1];
+
+  LastBR[0] = PC;
+
+  if(!V)
+    YHA = (YHA << 1) ^ (taken ^ ((PC >> 5) & 1));
+
+  // Path history
+  P_phist           = (P_phist << 1) ^ (taken ^ ((PC >> 5) & 1));
+  IMLIhist[INDIMLI] = (IMLIhist[INDIMLI] << 1) ^ (taken ^ ((PC >> 5) & 1));
+
+  if(brtype == OPTYPE_BRANCH_COND) {
+    // local history
+    L_shist[INDLOCAL]   = (L_shist[INDLOCAL] << 1) + (taken);
+    Q_slhist[INDQLOCAL] = (Q_slhist[INDQLOCAL] << 1) + (taken);
+    S_slhist[INDSLOCAL] = (S_slhist[INDSLOCAL] << 1) + (taken);
+    S_slhist[INDSLOCAL] ^= ((PC >> LOGSECLOCAL) & 15);
+    T_slhist[INDTLOCAL] = (T_slhist[INDTLOCAL] << 1) + (taken);
+    T_slhist[INDTLOCAL] ^= ((PC >> LOGTLOCAL) & 15);
+    // global branch history
+    GHIST = (GHIST << 1) + taken;
+
+    if((target > PC + 64) || (target < PC - 64))
+      RHIST = (RHIST << 1) + taken;
+    if(taken)
+      if((target > PC + 64) || (target < PC + 64))
+        CHIST = (CHIST << 1) ^ (PC & 63);
+  }
+
+  // is it really useful ?
+  if((PC + 16 < lastaddr) || (PC > lastaddr + 128)) {
+    BHIST = (BHIST << 1) ^ (PC & 15);
+  }
+  lastaddr = PC;
+  // IMLI related
+  if(brtype == OPTYPE_BRANCH_COND)
+    if(target < PC) {
+      // This branch corresponds to a loop
+      if(!taken) {
+        // exit of the "loop"
+        IMLIcount = 0;
+      }
+      if(taken) {
+        if(IMLIcount < ((1 << Im[0]) - 1))
+          IMLIcount++;
+      }
+    }
+
+#define SHIFTFUTURE 9
+  // IMLI OH history, see IMLI paper at Micro 2015
+  if(brtype == OPTYPE_BRANCH_COND) {
+    if(target >= PC) {
+      PAST[PC & 63] =
+        histtable[(((PC ^ (PC >> 2)) << SHIFTFUTURE) + IMLIcount) &
+                  (HISTTABLESIZE - 1)];
+      histtable[(((PC ^ (PC >> 2)) << SHIFTFUTURE) + IMLIcount) &
+                (HISTTABLESIZE - 1)] = taken;
+    }
+  }
+
+
+  int T = ((target ^ (target >> 3) ^ PC) << 1) + taken;
+
+  int8_t DIR = (T & 127);
+
+  // update  history
+  Y--;
+  ghist[Y & (HISTBUFFERLENGTH - 1)] = DIR;
+
+  // prepare next index and tag computations
+  for(int i = 1; i <= NGEHL; i++) {
+    K[i].update(ghist, Y);
+  }
+
+  for(int i = 1; i <= NRHSP; i++) {
+    L[i].update(ghist, Y);
+  }
+}
+
+
+/////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////
+
+void MTAGE::UpdateFinalSC(uint64_t PC, bool taken) {
+  bool CRES = (taken);
+  {
+    ctrupdate(GFINALCOLT[indexfinalcolt], CRES, 8);
+    ctrupdate(GFINAL[indexfinal], CRES, 8);
+    // using only the GFINAL table would result in 0.004 MPKI more
+  }
+}
+
+/////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////
+
+bool MTAGE::FinalSCpredict(uint64_t PC, bool Tpred) {
+  int TypeSecondSum;
+  int X = abs(LSUM);
+  int Y = updatethreshold + Pupdatethreshold[INDUPD];
+  if(X < Y / 4)
+    TypeSecondSum = 0;
+  else if(X < Y / 2)
+    TypeSecondSum = 1;
+  else if(X < Y)
+    TypeSecondSum = 2;
+  else
+    TypeSecondSum = 3;
+  int CLASS = (TypeSecondSum << 2) + (TypeFirstSum) +
+              ((COPRED + (Tpred << 1) + (predSC << 2)) << 4);
+  indexfinal     = CLASS;
+  indexfinalcolt = ((PC << 7) + CLASS) & (TABSIZE - 1);
+  LFINAL         = 2 * GFINAL[indexfinal] + 1;
+  if(abs(2 * GFINALCOLT[indexfinalcolt] + 1) > 15)
+    LFINAL = 2 * GFINALCOLT[indexfinalcolt] + 1;
+
+  return (LFINAL >= 0);
+}
+
+/////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////
+
+void MTAGE::UpdateSC(uint64_t PC, bool taken, bool PRED) {
+  if((predSC != taken) ||
+     ((abs(LSUM) < updatethreshold + Pupdatethreshold[INDUPD]))) {
+    if(predSC != taken) {
+      updatethreshold += 1;
+    } else {
+      updatethreshold -= 1;
+    }
+
+    if(predSC != taken) {
+      Pupdatethreshold[INDUPD] += 1;
+    } else {
+      Pupdatethreshold[INDUPD] -= 1;
+    }
+
+    gehlupdate(PC, taken);
+    rhspupdate(PC, taken);
+    ctrupdate(BiasFull[INDBIASFULL], taken, PERCWIDTH);
+    ctrupdate(Bias[INDBIAS], taken, PERCWIDTH);
+    ctrupdate(BiasColt[INDBIASCOLT], taken, PERCWIDTH);
+
+    updateperc(taken, PERC[PC & ((1 << LOGSIZEG) - 1)], GHIST, GPSTEP, GWIDTH);
+    updateperc(taken, PERCLOC[PC & ((1 << LOGSIZEL) - 1)], L_shist[INDLOCAL],
+               LPSTEP, LWIDTH);
+    updateperc(taken, PERCBACK[PC & ((1 << LOGSIZEB) - 1)], BHIST, BPSTEP,
+               BWIDTH);
+    updateperc(taken, PERCYHA[PC & ((1 << LOGSIZEB) - 1)], YHA, YPSTEP, YWIDTH);
+    updateperc(taken, PERCPATH[PC & ((1 << LOGSIZEP) - 1)], P_phist, PPSTEP,
+               PWIDTH);
+    updateperc(taken, PERCSLOC[PC & ((1 << LOGSIZES) - 1)], S_slhist[INDSLOCAL],
+               SPSTEP, SWIDTH);
+    updateperc(taken, PERCTLOC[PC & ((1 << LOGSIZES) - 1)], T_slhist[INDTLOCAL],
+               SPSTEP, SWIDTH);
+    updateperc(taken, PERCQLOC[PC & ((1 << LOGSIZEQ) - 1)], Q_slhist[INDQLOCAL],
+               QPSTEP, QWIDTH);
+
+    Gupdate(PC, taken, L_shist[INDLOCAL], Lm, LGEHL, LNB, PERCWIDTH);
+    // for IMLI
+    Gupdate(PC, taken, (L_shist[INDLOCAL] << 16) ^ IMLIcount, LIm, LIGEHL, LINB,
+            PERCWIDTH);
+    Gupdate(PC, taken, S_slhist[INDSLOCAL], Sm, SGEHL, SNB, PERCWIDTH);
+    Gupdate(PC, taken, T_slhist[INDTLOCAL], Tm, TGEHL, TNB, PERCWIDTH);
+    Gupdate(PC, taken, IMLIhist[INDIMLI], IMLIm, IMLIGEHL, IMLINB, PERCWIDTH);
+    Gupdate(PC, taken, Q_slhist[INDQLOCAL], Qm, QGEHL, QNB, PERCWIDTH);
+
+    Gupdate(PC, taken, BHIST, Bm, BGEHL, BNB, PERCWIDTH);
+    Gupdate(PC, taken, YHA, Ym, YGEHL, YNB, PERCWIDTH);
+    Gupdate(PC, taken, (IMLIcount + (GHIST << 16)), Im, IGEHL, INB, PERCWIDTH);
+    Gupdate((PC << 8), taken, futurelocal, Fm, fGEHL, FNB, PERCWIDTH);
+
+
+    Gupdate(0, taken, Q_slhist[INDQLOCAL], QQm, QQGEHL, QQNB, PERCWIDTH);
+    Gupdate(PC, taken, CHIST, Cm, CGEHL, CNB, PERCWIDTH);
+    Gupdate(PC, taken, RHIST, Rm, RGEHL, RNB, PERCWIDTH);
+  }
+}
+
+
+/////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////
+
+
+bool MTAGE::SCpredict(uint64_t PC, bool PRED) {
+  LSUM = 0;
+  predict_gehl(PC);
+  predict_rhsp(PC);
+  LSUM += SUMGEHL;
+  LSUM += SUMRHSP;
+
+  int8_t ctr = Bias[INDBIAS];
+  LSUM += 2 * (2 * ctr + 1);
+  ctr = BiasFull[INDBIASFULL];
+  LSUM += 2 * (2 * ctr + 1);
+  ctr = BiasColt[INDBIASCOLT];
+  LSUM += 2 * (2 * ctr + 1);
+
+
+  LSUM += percpredict(PC, GHIST, PERC[PC & ((1 << LOGSIZEG) - 1)], GPSTEP,
+                      GWIDTH);
+  LSUM += percpredict(PC, L_shist[INDLOCAL],
+                      PERCLOC[PC & ((1 << LOGSIZEL) - 1)], LPSTEP, LWIDTH);
+  LSUM += percpredict(PC, BHIST, PERCBACK[PC & ((1 << LOGSIZEB) - 1)], BPSTEP,
+                      BWIDTH);
+  LSUM += percpredict(PC, YHA, PERCYHA[PC & ((1 << LOGSIZEY) - 1)], YPSTEP,
+                      YWIDTH);
+  LSUM += percpredict(PC, P_phist, PERCPATH[PC & ((1 << LOGSIZEP) - 1)], PPSTEP,
+                      PWIDTH);
+  LSUM += percpredict(PC, S_slhist[INDSLOCAL],
+                      PERCSLOC[PC & ((1 << LOGSIZES) - 1)], SPSTEP, SWIDTH);
+  LSUM += percpredict(PC, T_slhist[INDTLOCAL],
+                      PERCTLOC[PC & ((1 << LOGSIZET) - 1)], TPSTEP, TWIDTH);
+  LSUM += percpredict(PC, Q_slhist[INDQLOCAL],
+                      PERCQLOC[PC & ((1 << LOGSIZEQ) - 1)], QPSTEP, QWIDTH);
+
+  LSUM += Gpredict(PC, L_shist[INDLOCAL], Lm, LGEHL, LNB);
+  LSUM += Gpredict(PC, T_slhist[INDTLOCAL], Tm, TGEHL, TNB);
+
+  LSUM += Gpredict(PC, Q_slhist[INDQLOCAL], Qm, QGEHL, QNB);
+
+
+  LSUM += Gpredict(PC, S_slhist[INDSLOCAL], Sm, SGEHL, SNB);
+
+  LSUM += Gpredict(PC, BHIST, Bm, BGEHL, BNB);
+  LSUM += Gpredict(PC, YHA, Ym, YGEHL, BNB);
+#ifdef IMLI
+  LSUM += Gpredict(PC, IMLIhist[INDIMLI], IMLIm, IMLIGEHL, IMLINB);
+  LSUM += Gpredict(PC, (L_shist[INDLOCAL] << 16) ^ IMLIcount, LIm, LIGEHL,
+                   LINB);
+  futurelocal = -1;
+
+  for(int i = Fm[FNB - 1]; i >= 0; i--) {
+    futurelocal =
+      histtable[(((PC ^ (PC >> 2)) << SHIFTFUTURE) + IMLIcount + i) &
+                (HISTTABLESIZE - 1)] +
+      (futurelocal << 1);
+  }
+  futurelocal = PAST[PC & 63] + (futurelocal << 1);
+  LSUM += Gpredict((PC << 8), futurelocal, Fm, fGEHL, FNB);
+#else
+  IMLIcount = 0;
+
+#endif
+
+  LSUM += Gpredict(PC, (IMLIcount + (GHIST << 16)), Im, IGEHL, INB);
+
+  LSUM += Gpredict(0, Q_slhist[INDQLOCAL], QQm, QQGEHL, QQNB);
+  LSUM += Gpredict(PC, CHIST, Cm, CGEHL, CNB);
+  LSUM += Gpredict(PC, RHIST, Rm, RGEHL, RNB);
+
+  return (LSUM >= 0);
+}
+
+
+/////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////
+// Functions  for the statiscal corrector
+
+void MTAGE::predict_gehl(uint64_t PC) {
+  // index computation
+  for(int i = 1; i <= NGEHL; i++) {
+    GEHLINDEX[i] = gehlindex(PC, i);
+  }
+  GEHLINDEX[0] = PC & ((1 << LOGGEHL) - 1);
+
+  // SUMGEHL is centered
+  SUMGEHL = 0;
+  for(int i = 0; i <= NGEHL; i++) {
+    SUMGEHL += 2 * GEHL[GEHLINDEX[i]][i] + 1;
+  }
+}
+
+
+void MTAGE::gehlupdate(uint64_t PC, bool taken) {
+  // update the GEHL  predictor tables
+  for(int i = NGEHL; i >= 0; i--)
+    ctrupdate(GEHL[GEHLINDEX[i]][i], taken, PERCWIDTH);
+}
+
+
+void MTAGE::predict_rhsp(uint64_t PC) {
+  // index computation
+  for(int i = 1; i <= NRHSP; i++) {
+    RHSPINDEX[i] = rhspindex(PC, i);
+  }
+  RHSPINDEX[0] = PC & ((1 << LOGRHSP) - 1);
+
+  // SUMRHSP is centered
+  SUMRHSP = 0;
+  for(int i = 1; i <= NRHSP; i++)
+    SUMRHSP += 2 * RHSP[RHSPINDEX[i]][i] + 1;
+}
+
+
+void MTAGE::rhspupdate(uint64_t PC, bool taken) {
+  for(int i = NRHSP; i >= 1; i--)
+    ctrupdate(RHSP[RHSPINDEX[i]][i], taken, PERCWIDTH);
+}
+
+
+int MTAGE::percpredict(int PC, long long BHIST, int8_t* line, int PSTEP,
+                       int WIDTH) {
+  PERCSUM         = 0;
+  long long bhist = BHIST;
+  int       PT    = 0;
+
+  for(int i = 0; i < WIDTH; i += PSTEP) {
+    int    index = bhist & ((1 << PSTEP) - 1);
+    int8_t ctr   = line[PT + index];
+
+    PERCSUM += 2 * ctr + 1;
+
+    bhist >>= PSTEP;
+    PT += (1 << PSTEP);
+  }
+
+  return PERCSUM;
+}
+
+
+void MTAGE::updateperc(bool taken, int8_t* line, long long BHIST, int PSTEP,
+                       int WIDTH) {
+  int       PT    = 0;
+  long long bhist = BHIST;
+
+  for(int i = 0; i < WIDTH; i += PSTEP) {
+    int index = bhist & ((1 << PSTEP) - 1);
+    ctrupdate(line[PT + index], taken, PERCWIDTH);
+    bhist >>= PSTEP;
+    PT += (1 << PSTEP);
+  }
+}
+
+
+int MTAGE::Gpredict(uint64_t PC, long long BHIST, int* length, int8_t** tab,
+                    int NBR) {
+  PERCSUM = 0;
+
+  for(int i = 0; i < NBR; i++) {
+    long long bhist = BHIST & ((long long)((1 << length[i]) - 1));
+
+    int index = (((long long)PC) ^ bhist ^ (bhist >> (LOGTAB - i)) ^
+                 (bhist >> (40 - 2 * i)) ^ (bhist >> (60 - 3 * i))) &
+                (TABSIZE - 1);
+
+    int8_t ctr = tab[i][index];
+    PERCSUM += 2 * ctr + 1;
+  }
+  return PERCSUM;
+}
+
+
+void MTAGE::Gupdate(uint64_t PC, bool taken, long long BHIST, int* length,
+                    int8_t** tab, int NBR, int WIDTH) {
+  for(int i = 0; i < NBR; i++) {
+    long long bhist = BHIST & ((long long)((1 << length[i]) - 1));
+
+    int index = (((long long)PC) ^ bhist ^ (bhist >> (LOGTAB - i)) ^
+                 (bhist >> (40 - 2 * i)) ^ (bhist >> (60 - 3 * i))) &
+                (TABSIZE - 1);
+
+    ctrupdate(tab[i][index], taken, WIDTH);
+  }
+}
+
+
+int MTAGE::gehlindex(uint64_t PC, int bank) {
+  int index = PC ^ (PC >> ((mgehl[bank] % LOGGEHL) + 1)) ^ chgehl_i[bank].comp;
+  return index & ((1 << LOGGEHL) - 1);
+}
+
+
+int MTAGE::rhspindex(uint64_t PC, int bank) {
+  int index = PC ^ (PC >> ((mrhsp[bank] % LOGRHSP) + 1)) ^ chrhsp_i[bank].comp;
+  if(bank > 1) {
+    index ^= chrhsp_i[bank - 1].comp;
+  }
+  if(bank > 3) {
+    index ^= chrhsp_i[bank / 3].comp;
+  }
+  return index & ((1 << LOGRHSP) - 1);
+}
diff --git a/src/bp/mtage_unlimited.h b/src/bp/mtage_unlimited.h
new file mode 100644
index 0000000..3efe02d
--- /dev/null
+++ b/src/bp/mtage_unlimited.h
@@ -0,0 +1,384 @@
+/**
+ * @file mtage_unlimited.cc
+ * @brief P. Michaud and A. Seznec
+ * @version 0.1
+ * @date 2021-01-12
+ *
+ * Adapted to Scarab by Stephen Pruett
+ *
+ * Code is derived from P.Michaud and A. Seznec code for the CBP4 winner Sorry
+ * two very different code writing styles
+ *
+ * ////////////// STORAGE BUDGET JUSTIFICATION ////////////////
+ *   unlimited size
+ */
+
+#ifndef _MTAGE_H_
+#define _MTAGE_H_
+
+#include <assert.h>
+#include <cmath>
+#include <inttypes.h>
+#include <math.h>
+#include <stdlib.h>
+#include <string.h>
+#include <vector>
+#include "stdint.h"
+#include "stdio.h"
+
+#include "cbp_to_scarab.h"
+
+/*************Code From CBP 2016***************/
+
+// initial code by P.Michaud for the CBP4 poTAGE  and poTAGE +SC
+
+// NPRED : number of tage predictors
+#define NPRED 6
+
+
+// SPSIZE : spectrum size (number of subpaths) for each tage
+// P0 = global, P1 = per-address, P2 = per-set, P3 = per-set, P4 = frequency
+#define P0_SPSIZE 1
+#define P1_SPSIZE 4096
+#define P2_SPSIZE 64
+#define P3_SPSIZE 16
+#define P4_SPSIZE 8
+#define P5_SPSIZE 1
+// P2_PARAM and P3_PARAM are the log2 of the set sizes in the per-set tages
+#define P2_PARAM 7
+#define P3_PARAM 4
+
+// tage parameters:
+
+// NUMG = number of tagged tables
+// LOGB = log2 of the number of entries of the tagless (bimodal) table
+// LOGG = log2 of the number of entries of each tagged table
+// MAXHIST = maximum path length ("rightmost" tagged table), in branches
+// MINHIST = minimum path length ("leftmost" tagged table), in branches
+// HASHPARAM = parameter used in the hash functions (may need to be changed with
+// predictor size) RAMPUP = ramp-up period in mispredictions (should be kept
+// roughly proportional to predictor size) TAGBITS = tag width in bits CTRBITS =
+// width of the taken/not-taken counters in the tagless (bimodal) and tagged
+// tables PATHBITS = number of per-branch address bits injected in the path
+// hashing POSTPBITS = width of the taken/not-taken counters in the
+// post-predictor POSTPEXTRA = number of secondary hits feeding the
+// post-predictor ALLOCFAILMAX : used for clearing u bits (cf. ISL_TAGE, Andre
+// Seznec, MICRO 2011) MAXALLOC = maximum number of entries stolen upon a
+// misprediction (cf. ISL_TAGE) CAPHIST = path length beyond which aggressive
+// update (ramp-up) is made sligtly less aggressive
+
+// parameters specific to the global tage
+#define P0_NUMG 25
+#define P0_LOGB 21
+#define P0_LOGG 21
+#define P0_MAXHIST 5000
+#define P0_MINHIST 7
+#define P0_HASHPARAM 3
+#define P0_RAMPUP 100000
+
+// parameters specific to the per-address tage
+#define P1_NUMG 22
+#define P1_LOGB 20
+#define P1_LOGG 20
+#define P1_MAXHIST 2000
+#define P1_MINHIST 5
+#define P1_HASHPARAM 3
+#define P1_RAMPUP 100000
+
+// parameters specific to the first per-set tage
+#define P2_NUMG 21
+#define P2_LOGB 20
+#define P2_LOGG 20
+#define P2_MAXHIST 500
+#define P2_MINHIST 5
+#define P2_HASHPARAM 3
+#define P2_RAMPUP 100000
+
+// parameters specific to second per-set tage
+#define P3_NUMG 20
+#define P3_LOGB 20
+#define P3_LOGG 20
+#define P3_MAXHIST 500
+#define P3_MINHIST 5
+#define P3_HASHPARAM 3
+#define P3_RAMPUP 100000
+
+// parameters specific to the frequency-based tage
+#define P4_NUMG 20
+#define P4_LOGB 20
+#define P4_LOGG 20
+#define P4_MAXHIST 500
+#define P4_MINHIST 5
+#define P4_HASHPARAM 3
+#define P4_RAMPUP 100000
+
+// parameters specific to the  tage
+#define P5_NUMG 20
+#define P5_LOGB 20
+#define P5_LOGG 20
+#define P5_MAXHIST 400
+#define P5_MINHIST 5
+#define P5_HASHPARAM 3
+#define P5_RAMPUP 100000
+
+// parameters common to all tages
+#define TAGBITS 15
+#define CTRBITS 3
+#define PATHBITS 6
+#define POSTPBITS 5
+//#define POSTPEXTRA 1
+#define ALLOCFAILMAX 511
+#define MAXALLOC 3
+#define CAPHIST 200
+
+// BFTSIZE = number of entries in the branch frequency table (BFT)
+#define BFTSIZE (1 << 20)
+
+// FRATIOBITS = log2 of the ratio between adjacent frequency bins (predictor P3)
+#define FRATIOBITS 1
+
+// COLT parameters (each COLT entry has 2^NPRED counters)
+// LOGCOLT = log2 of the number of COLT entries
+// COLTBITS = width of the taken/not-taken COLT counters
+#define LOGCOLT 20
+#define COLTBITS 5
+
+
+using namespace std;
+
+
+class path_history {
+  // path history register
+ public:
+  int       ptr;
+  int       hlength;
+  unsigned* h;
+
+  void      init(int hlen);
+  void      insert(unsigned val);
+  unsigned& operator[](int n);
+};
+
+
+class compressed_history {
+  // used in the hash functions
+ public:
+  unsigned comp;
+  int      clength;
+  int      olength;
+  int      nbits;
+  int      outpoint;
+  unsigned mask1;
+  unsigned mask2;
+
+  compressed_history();
+  void reset();
+  void init(int original_length, int compressed_length, int injected_bits);
+  void rotateleft(unsigned& x, int m);
+  void update(path_history& ph);
+};
+
+
+class coltentry {
+  // COLT entry (holds 2^NPRED counters)
+ public:
+  int8_t c[1 << NPRED];
+  coltentry();
+  int8_t& ctr(bool predtaken[NPRED]);
+};
+
+
+class colt {
+  // This is COLT, a method invented by Gabriel Loh and Dana Henry
+  // for combining several different predictors (see PACT 2002)
+ public:
+  coltentry c[1 << LOGCOLT];
+  int8_t&   ctr(uint64_t pc, bool predtaken[NPRED]);
+  bool      predict(uint64_t pc, bool predtaken[NPRED]);
+  void      update(uint64_t pc, bool predtaken[NPRED], bool taken);
+};
+
+
+class bftable {
+  // branch frequency table (BFT)
+ public:
+  int freq[BFTSIZE];
+  bftable();
+  int& getfreq(uint64_t pc);
+};
+
+
+class subpath {
+  // path history register and hashing
+ public:
+  path_history        ph;
+  int                 numg;
+  compressed_history* chg;
+  compressed_history* chgg;
+  compressed_history* cht;
+  compressed_history* chtt;
+
+  void init(int ng, int hist[], int logg, int tagbits, int pathbits, int hp);
+  void init(int ng, int minhist, int maxhist, int logg, int tagbits,
+            int pathbits, int hp);
+  void update(uint64_t targetpc, bool taken);
+  unsigned cg(int bank);
+  unsigned cgg(int bank);
+  unsigned ct(int bank);
+  unsigned ctt(int bank);
+};
+
+
+class spectrum {
+  // path spectrum (= set of subpaths, aka first-level history)
+ public:
+  int      size;
+  subpath* p;
+
+  spectrum();
+  void init(int sz, int ng, int minhist, int maxhist, int logg, int tagbits,
+            int pathbits, int hp);
+};
+
+
+class freqbins {
+  // frequency bins for predictor P3
+ public:
+  int nbins;
+  int maxfreq;
+
+  void init(int nb);
+  int  find(int bfreq);
+  void update(int bfreq);
+};
+
+
+class gentry {
+  // tage tagged tables entry
+ public:
+  int16_t tag;
+  int8_t  ctr;
+  int8_t  u;
+  gentry();
+};
+
+
+class tage {
+  // cf. TAGE (Seznec & Michaud JILP 2006, Seznec MICRO 2011)
+ public:
+  string name;
+
+  int8_t*     b;  // tagless (bimodal) table
+  gentry**    g;  // tagged tables
+  int         bi;
+  int*        gi;
+  vector<int> hit;
+  bool        predtaken;
+  bool        altpredtaken;
+  int         ppi;
+  int8_t*     postp;  // post-predictor
+  bool        postpredtaken;
+  bool        mispred;
+  int         allocfail;
+  int         nmisp;
+
+  int numg;
+  int bsize;
+  int gsize;
+  int tagbits;
+  int ctrbits;
+  int postpbits;
+  int postpsize;
+  int rampup;
+  int hashp;
+  int caphist;
+
+  tage();
+  void    init(const char* nm, int ng, int logb, int logg, int tagb, int ctrb,
+               int ppb, int ru, int caph);
+  int     bindex(uint64_t pc);
+  int     gindex(uint64_t pc, subpath& p, int bank);
+  int     gtag(uint64_t pc, subpath& p, int bank);
+  int     postp_index();
+  gentry& getg(int i);
+  bool    condbr_predict(uint64_t pc, subpath& p);
+  void    uclear();
+  void    galloc(int i, uint64_t pc, bool taken, subpath& p);
+  void    aggressive_update(uint64_t pc, bool taken, subpath& p);
+  void    careful_update(uint64_t pc, bool taken, subpath& p);
+  bool    condbr_update(uint64_t pc, bool taken, subpath& p);
+  void    printconfig(subpath& p);
+};
+
+
+/////////////////////////////////////////////////////////////
+
+class folded_history {
+  // utility class for index computation
+  // this is the cyclic shift register for folding
+  // a long global history into a smaller number of bits; see P. Michaud's
+  // PPM-like predictor at CBP-1
+ public:
+  unsigned comp;
+  int      CLENGTH;
+  int      OLENGTH;
+  int      OUTPOINT;
+  void     init(int original_length, int compressed_length, int N);
+  void     update(uint8_t* h, int PT);
+};
+
+
+class MTAGE {
+ private:
+  bftable  bft;
+  freqbins bfreq;
+  spectrum sp[NPRED];
+  tage     pred[NPRED];
+  subpath* subp[NPRED];
+  bool     predtaken[NPRED];
+  colt     co;
+
+ public:
+  MTAGE(void);
+  bool GetPrediction(uint64_t PC);
+  void UpdatePredictor(uint64_t PC, OpType OPTYPE, bool resolveDir,
+                       bool predDir, uint64_t branchTarget);
+  void TrackOtherInst(uint64_t PC, OpType opType, bool taken,
+                      uint64_t branchTarget);
+
+  void initSC();
+  void HistoryUpdate(uint64_t PC, uint8_t brtype, bool taken, uint64_t target,
+                     int& Y, folded_history* K, folded_history* L);
+
+
+  void UpdateFinalSC(uint64_t PC, bool taken);
+  void UpdateSC(uint64_t PC, bool taken, bool PRED);
+  bool FinalSCpredict(uint64_t PC, bool Tpred);
+  bool SCpredict(uint64_t PC, bool Tpred);
+  int  percpredict(int PC, long long BHIST, int8_t* line, int PSTEP, int WIDTH);
+  void updateperc(bool taken, int8_t* line, long long BHIST, int PSTEP,
+                  int WIDTH);
+  int  Gpredict(uint64_t PC, long long BHIST, int* length, int8_t** tab,
+                int NBR);
+  void Gupdate(uint64_t PC, bool taken, long long BHIST, int* length,
+               int8_t** tab, int NBR, int WIDTH);
+
+  // index function for the GEHL and MAC-RHSP tables
+  // FGEHL serves to mix path history
+
+  int gehlindex(uint64_t PC, int bank);
+
+  int  rhspindex(uint64_t PC, int bank);
+  void predict_gehl(uint64_t PC);
+  void gehlupdate(uint64_t PC, bool taken);
+  void predict_rhsp(uint64_t PC);
+  void rhspupdate(uint64_t PC, bool taken);
+  bool getloop(uint64_t PC);
+  int  lindex(uint64_t);
+  void loopupdate(uint64_t, bool, bool);
+};
+void PrintStat(double NumInst);
+
+
+/***********************************************************/
+#endif
diff --git a/src/bp/tagescl.cc b/src/bp/tagescl.cc
index 814b847..8ddc752 100644
--- a/src/bp/tagescl.cc
+++ b/src/bp/tagescl.cc
@@ -114,3 +114,8 @@ void bp_tagescl_recover(Recovery_Info* recovery_info) {
     get_branch_type(proc_id, recovery_info->cf_type), recovery_info->new_dir,
     recovery_info->branchTarget);
 }
+
+uns8 bp_tagescl_full(Op* op) {
+  uns proc_id = op->proc_id;
+    return tagescl_predictors.at(proc_id).is_full();
+}
diff --git a/src/bp/tagescl.h b/src/bp/tagescl.h
index ad6bcdb..84f6b6e 100644
--- a/src/bp/tagescl.h
+++ b/src/bp/tagescl.h
@@ -36,6 +36,7 @@ void bp_tagescl_spec_update(Op* op);
 void bp_tagescl_update(Op* op);
 void bp_tagescl_retire(Op* op);
 void bp_tagescl_recover(Recovery_Info*);
+uns8 bp_tagescl_full(Op* op);
 
 #ifdef __cplusplus
 }
diff --git a/src/bp/template_lib/tagescl.h b/src/bp/template_lib/tagescl.h
index d34263e..a267d71 100644
--- a/src/bp/template_lib/tagescl.h
+++ b/src/bp/template_lib/tagescl.h
@@ -65,6 +65,10 @@ class Tage_SC_L {
     return branch_id;
   }
 
+  bool is_full() {
+    return prediction_info_buffer_.is_full();
+  }
+
   // It uses the speculative state of the predictor to generate a prediction.
   // Should be called before update_speculative_state.
   bool get_prediction(int64_t branch_id, uint64_t br_pc);
diff --git a/src/bp/template_lib/utils.h b/src/bp/template_lib/utils.h
index 44feb47..ff30aad 100644
--- a/src/bp/template_lib/utils.h
+++ b/src/bp/template_lib/utils.h
@@ -185,6 +185,12 @@ class Circular_Buffer {
     size_ -= 1;
   }
 
+  bool is_full() {
+    if (size_ == buffer_size_)
+      return true;
+    return false;
+  }
+
  private:
   std::vector<T> buffer_;
   int64_t        buffer_size_;
diff --git a/src/cmp_model.c b/src/cmp_model.c
index b54671b..8d511d2 100644
--- a/src/cmp_model.c
+++ b/src/cmp_model.c
@@ -87,7 +87,6 @@ void cmp_init(uns mode) {
     cmp_init_thread_data(proc_id);
 
     init_icache_stage(proc_id, "ICACHE");
-    init_icache_trace();
 
     init_decode_stage(proc_id, "DECODE");
 
@@ -104,6 +103,7 @@ void cmp_init(uns mode) {
     /* initialize the common data structures */
     init_bp_recovery_info(proc_id, &cmp_model.bp_recovery_info[proc_id]);
     init_bp_data(proc_id, &cmp_model.bp_data[proc_id]);
+    init_uop_cache();
   }
 
   cmp_model.window_size = NODE_TABLE_SIZE;
@@ -303,7 +303,6 @@ void cmp_recover() {
          bp_recovery_info->proc_id == map_data->proc_id);
   bp_recovery_info->recovery_cycle = MAX_CTR;
   bp_recovery_info->redirect_cycle = MAX_CTR;
-
   bp_recover_op(g_bp_data, bp_recovery_info->recovery_cf_type,
                 &bp_recovery_info->recovery_info);
 
diff --git a/src/cmp_model.h b/src/cmp_model.h
index 7e40d49..a0ab53d 100644
--- a/src/cmp_model.h
+++ b/src/cmp_model.h
@@ -41,6 +41,7 @@
 #include "memory/memory.h"
 #include "node_stage.h"
 #include "thread.h"
+#include "uop_cache.h"
 
 /**************************************************************************************/
 /* cmp model data  */
diff --git a/src/core.param.def b/src/core.param.def
index faea601..38ed8c0 100644
--- a/src/core.param.def
+++ b/src/core.param.def
@@ -146,8 +146,10 @@ DEF_PARAM(fu_types, FU_TYPES, char*, string, "0, 0, 0, 0", )
 DEF_PARAM(decode_cycles, DECODE_CYCLES, uns, uns, 1, )
 DEF_PARAM(map_cycles, MAP_CYCLES, uns, uns, 1, )
 
-DEF_PARAM(extra_recovery_cycles, EXTRA_RECOVERY_CYCLES, uns, uns, 0, )
+DEF_PARAM(icache_latency, ICACHE_LATENCY, uns, uns, 3, )
+DEF_PARAM(uop_cache_latency, UOP_CACHE_LATENCY, uns, uns, 1, )
 DEF_PARAM(extra_redirect_cycles, EXTRA_REDIRECT_CYCLES, uns, uns, 0, )
+DEF_PARAM(extra_recovery_cycles, EXTRA_RECOVERY_CYCLES, uns, uns, 0, )
 DEF_PARAM(extra_callsys_cycles, EXTRA_CALLSYS_CYCLES, uns, uns, 20,
           const) /* const */
 DEF_PARAM(die_on_callsys, DIE_ON_CALLSYS, Flag, Flag, FALSE, )
@@ -295,6 +297,7 @@ DEF_PARAM(horob_min_cycles, HOROB_MIN_CYCLES, uns, uns, 0, )
 DEF_PARAM(miss_count_path, MISS_COUNT_PATH, char*, string, NULL, )
 DEF_PARAM(pref_learn_deps, PREF_LEARN_DEPS, uns, uns, 0, )
 DEF_PARAM(pref_min_miss_ratio, PREF_MIN_MISS_RATIO, uns, uns, 10, )
+DEF_PARAM(lookahead_buf_size, LOOKAHEAD_BUF_SIZE, uns, uns, 640, )
 
 DEF_PARAM(dumb_core_on, DUMB_CORE_ON, Flag, Flag, FALSE, )
 DEF_PARAM(dumb_core, DUMB_CORE, uns, uns, 1, )
diff --git a/src/ctype_pin_inst.h b/src/ctype_pin_inst.h
index 37809bb..3b24008 100644
--- a/src/ctype_pin_inst.h
+++ b/src/ctype_pin_inst.h
@@ -68,6 +68,8 @@ typedef struct ctype_pin_inst_struct {
 
   uint64_t instruction_addr;  // 8 bytes
   uint8_t  size;              // 5 bits
+  uint64_t inst_binary_lsb;   // x86 instr are 1-15 bytes.
+  uint64_t inst_binary_msb;
   // uint8_t opcode;           // 6 bits
   uint8_t op_type;  // 6 bits
   uint8_t cf_type;  // 4 bits
diff --git a/src/decode_stage.c b/src/decode_stage.c
index bafe691..6a27e61 100644
--- a/src/decode_stage.c
+++ b/src/decode_stage.c
@@ -23,7 +23,8 @@
  * File         : decode_stage.c
  * Author       : HPS Research Group
  * Date         : 2/17/1999
- * Description  :
+ * Description  : simulates the latency due to decode stage. (actual uop decoding was
+                    done in frontend)
  ***************************************************************************************/
 
 #include "debug/debug_macros.h"
@@ -44,12 +45,15 @@
 #include "general.param.h"
 #include "thread.h" /* for td */
 
+#include "uop_cache.h"
+#include "statistics.h"
+#include "memory/memory.param.h"
 
 /**************************************************************************************/
 /* Macros */
 
 #define DEBUG(proc_id, args...) _DEBUG(proc_id, DEBUG_DECODE_STAGE, ##args)
-#define STAGE_MAX_OP_COUNT ISSUE_WIDTH
+#define STAGE_MAX_OP_COUNT ISSUE_WIDTH + UOP_CACHE_ADDITIONAL_ISSUE_BANDWIDTH
 #define STAGE_MAX_DEPTH DECODE_CYCLES
 
 
@@ -58,7 +62,6 @@
 
 Decode_Stage* dec = NULL;
 
-
 /**************************************************************************************/
 /* Local prototypes */
 
@@ -152,13 +155,17 @@ void debug_decode_stage() {
 
 
 /**************************************************************************************/
-/* decode_cycle: */
+/* decode_cycle: Movement of cached uops to later stages assumes that with a stalled
+ *               pipeline the same (modified src_sd is passed to update_decode_stage
+ *               What if there is a branch mispred? are instr properly flushed?
+ */
 
 void update_decode_stage(Stage_Data* src_sd) {
   Flag        stall = (dec->last_sd->op_count > 0);
   Stage_Data *cur, *prev;
   Op**        temp;
   uns         ii;
+  static Flag fetching_from_UC = FALSE;
 
   /* do all the intermediate stages */
   for(ii = 0; ii < STAGE_MAX_DEPTH - 1; ii++) {
@@ -174,6 +181,89 @@ void update_decode_stage(Stage_Data* src_sd) {
   }
 
   /* do the first decode stage */
+  /* First, insert all cached uops into stage nearest last. 
+   * Next, place the stage data minus cached instr into last stage 
+   */
+
+  /* find first available empty pipeline stage for cached uops */
+  int empty_stage_idx = STAGE_MAX_DEPTH;
+  for(int jj = STAGE_MAX_DEPTH - 1; jj >= 0; jj--) {
+    cur = &dec->sds[jj];
+    if(!cur->op_count) {
+      empty_stage_idx = jj;
+    } else {
+      break;
+    }
+  }
+
+  /* Move cached uops to later stage in pipeline if possible */
+  for (int ii = 0; ii < src_sd->op_count; ii++) {
+
+    // Cannot leave icache if not in uop cache, or no space in first stage.
+    if (!src_sd->ops[ii]->fetched_from_uop_cache) {
+      fetching_from_UC = FALSE;
+      break;
+    }
+    if (dec->sds[STAGE_MAX_DEPTH - 1].op_count == STAGE_MAX_OP_COUNT) {
+      break;
+    }
+
+    /* the next stage after the empty stage may have a few extra slots */
+    // want to be able to append to first stage if there is a stall
+    int append_to_sd = empty_stage_idx - 1 >= 0 
+                       && dec->sds[empty_stage_idx - 1].op_count < STAGE_MAX_OP_COUNT;
+    int insert_into_sd_num = -1;
+    if (append_to_sd) {
+      insert_into_sd_num = empty_stage_idx - 1;
+    } else if (empty_stage_idx < STAGE_MAX_DEPTH - 1) {
+      /* append to closest empty stage */
+      insert_into_sd_num = empty_stage_idx;
+    } else {
+      /* No empty slots in later stages, Pipeline full. Done moving individual ops */
+      continue;
+    }
+
+    // log cycles saved only for the FIRST op that is moved in seq of fetched instr from UC
+    // if inserting into first stage when first stage is partially full still save 1 cycle
+    if (!fetching_from_UC) {
+      int cycles_saved = (STAGE_MAX_DEPTH - 1) - insert_into_sd_num;
+      INC_STAT_EVENT(dec->proc_id, UOP_CACHE_CYCLES_SAVED, cycles_saved ? cycles_saved : 1);
+      fetching_from_UC = TRUE;
+    }
+
+    /* stage to insert op into */
+    Stage_Data* insert_stage = &dec->sds[insert_into_sd_num];
+
+    Op* moved_op = src_sd->ops[ii];
+    insert_stage->ops[insert_stage->op_count] = src_sd->ops[ii];
+    src_sd->ops[ii] = NULL;
+    insert_stage->op_count++;
+
+    /* process op if appended to last dec stage. Fetched from uop cache, so do not accumulate for insert */
+    ASSERT(dec->proc_id, moved_op != NULL);
+    if (empty_stage_idx - 1 == 0 && append_to_sd) {
+      stage_process_op(moved_op);
+      end_accumulate();
+    }
+  }
+
+  /* update src_sd->ops, op_count */
+  int ops_moved = 0;
+  for (int ii = 0; ii < src_sd->op_count; ii++) {
+    if (src_sd->ops[ii] == NULL) {
+      ops_moved++;
+    } else if (ops_moved > 0) {
+      src_sd->ops[ii - ops_moved] = src_sd->ops[ii];
+      src_sd->ops[ii] = NULL;
+    } else {
+      break;
+    }
+  }
+  src_sd->op_count -= ops_moved;
+
+  INC_STAT_EVENT(dec->proc_id, N_UOPS_DEC, ops_moved);
+
+  /* Place any remaining ops into first stage */
   cur = &dec->sds[STAGE_MAX_DEPTH - 1];
   if(cur->op_count == 0) {
     prev           = src_sd;
@@ -182,6 +272,9 @@ void update_decode_stage(Stage_Data* src_sd) {
     prev->ops      = temp;
     cur->op_count  = prev->op_count;
     prev->op_count = 0;
+    INC_STAT_EVENT(dec->proc_id, N_UOPS_DEC, cur->op_count);
+  } else {
+    ASSERT(0, stall);
   }
 
   /* if the last decode stage is stalled, don't re-process the ops  */
@@ -193,6 +286,13 @@ void update_decode_stage(Stage_Data* src_sd) {
     Op* op = dec->last_sd->ops[ii];
     ASSERT(dec->proc_id, op != NULL);
     stage_process_op(op);
+    // Cache all uops being emitted from decode stage
+    if (!op->fetched_from_uop_cache) {
+      STAT_EVENT(dec->proc_id, UOP_ACCUMULATE);
+      accumulate_op(op);
+    } else {
+      end_accumulate();
+    }
   }
 }
 
@@ -231,4 +331,4 @@ static inline void stage_process_op(Op* op) {
       }
     }
   }
-}
+}
\ No newline at end of file
diff --git a/src/fetch.stat.def b/src/fetch.stat.def
index 32bf6bf..cf87767 100644
--- a/src/fetch.stat.def
+++ b/src/fetch.stat.def
@@ -71,6 +71,7 @@ DEF_STAT(  INST_LOST_WAIT_FOR_MISP_RECOVERY       , DIST   , NO_RATIO  )
 DEF_STAT(  INST_LOST_WAIT_FOR_TIMER               , COUNT  , NO_RATIO  )
 DEF_STAT(  INST_LOST_WAIT_FOR_EMPTY_ROB           , COUNT  , NO_RATIO  )
 DEF_STAT(  INST_LOST_WAIT_FOR_REDIRECT            , COUNT  , NO_RATIO  )
+DEF_STAT(  INST_LOST_WAIT_FOR_FDIP                , COUNT  , NO_RATIO  )
 DEF_STAT(  INST_LOST_FETCH                        , COUNT  , NO_RATIO  )
 DEF_STAT(  INST_LOST_OFF_PATH                     , COUNT  , NO_RATIO  )
 DEF_STAT(  INST_LOST_FULL_WINDOW                  , COUNT  , NO_RATIO  )
@@ -94,7 +95,9 @@ DEF_STAT(  INST_LOST_BREAK_OFFPATH                , COUNT  , NO_RATIO  )
 DEF_STAT(  INST_LOST_BREAK_ALIGNMENT              , COUNT  , NO_RATIO  )
 DEF_STAT(  INST_LOST_BREAK_TAKEN                  , COUNT  , NO_RATIO  )
 DEF_STAT(  INST_LOST_BREAK_MODEL_BEFORE           , COUNT  , NO_RATIO  )
-DEF_STAT(  INST_LOST_BREAK_MODEL_AFTER            , DIST   , NO_RATIO  )
+DEF_STAT(  INST_LOST_BREAK_MODEL_AFTER            , COUNT  , NO_RATIO  )
+DEF_STAT(  INST_LOST_BREAK_UC_MISS				  , COUNT  , NO_RATIO  )
+DEF_STAT(  INST_LOST_BREAK_FDIP_RUNAHEAD				  , DIST   , NO_RATIO  )
 
 DEF_STAT(  INST_LOST_TOTAL                        , COUNT  , NO_RATIO  )
 
@@ -126,7 +129,9 @@ DEF_STAT(  ST_BREAK_STALL       , COUNT , NO_RATIO  )
 DEF_STAT(  ST_BREAK_BARRIER     , COUNT , NO_RATIO  )
 DEF_STAT(  ST_BREAK_OFFPATH     , COUNT , NO_RATIO  )
 DEF_STAT(  ST_BREAK_ALIGNMENT   , COUNT , NO_RATIO  )
-DEF_STAT(  ST_BREAK_TAKEN       , DIST  , NO_RATIO  )
+DEF_STAT(  ST_BREAK_TAKEN       , COUNT , NO_RATIO  )
+DEF_STAT(  ST_BREAK_UC_MISS 	, COUNT	, NO_RATIO	)
+DEF_STAT(  ST_BREAK_FDIP_RUNAHEAD, DIST , NO_RATIO  )
 
 DEF_STAT( ORACLE_ON_PATH_INST   , DIST  , NO_RATIO  )
 DEF_STAT( ORACLE_OFF_PATH_INST  , DIST  , NO_RATIO  )
diff --git a/src/frontend/frontend.c b/src/frontend/frontend.c
index 5daed2b..ba58590 100644
--- a/src/frontend/frontend.c
+++ b/src/frontend/frontend.c
@@ -39,6 +39,7 @@
 #include "pin_exec_driven_fe.h"
 #include "pin_trace_fe.h"
 #include "memtrace_fe.h"
+#include "pt_fe.h"
 #include "sim.h"
 #include "statistics.h"
 #include "thread.h"
@@ -66,6 +67,10 @@ void frontend_init() {
       trace_init();
       break;
     }
+    case FE_PT: {
+      pt_init();
+      break;
+    }
     case FE_MEMTRACE: {
       memtrace_init();
     break;
@@ -86,6 +91,10 @@ void frontend_done(Flag* retired_exit) {
       trace_done();
       break;
     }
+    case FE_PT: {
+      pt_done();
+      break;
+    }
     case FE_MEMTRACE: {
       memtrace_done();
       break;
diff --git a/src/frontend/frontend_intf.c b/src/frontend/frontend_intf.c
index 857ff07..4e5624b 100644
--- a/src/frontend/frontend_intf.c
+++ b/src/frontend/frontend_intf.c
@@ -34,6 +34,7 @@
 #include "frontend/pin_exec_driven_fe.h"
 #include "frontend/pin_trace_fe.h"
 #include "frontend/memtrace_fe.h"
+#include "frontend/pt_fe.h"
 
 Frontend_Impl frontend_table[] = {
 #define FRONTEND_IMPL(id, name, prefix) \
diff --git a/src/frontend/frontend_table.def b/src/frontend/frontend_table.def
index 9d78cdc..c76e125 100644
--- a/src/frontend/frontend_table.def
+++ b/src/frontend/frontend_table.def
@@ -29,4 +29,5 @@
 // Format: enum name, text name, function name prefix
 FRONTEND_IMPL(PIN_EXEC_DRIVEN, "pin_exec_driven", pin_exec_driven)
 FRONTEND_IMPL(TRACE,           "trace",           trace)
-FRONTEND_IMPL(MEMTRACE,	       "memtrace",	  memtrace)
\ No newline at end of file
+FRONTEND_IMPL(MEMTRACE,	       "memtrace",	  memtrace)
+FRONTEND_IMPL(PT,	       "pt",	  pt)
diff --git a/src/frontend/memtrace_fe.cc b/src/frontend/memtrace_fe.cc
index 45c0fa5..295d34d 100644
--- a/src/frontend/memtrace_fe.cc
+++ b/src/frontend/memtrace_fe.cc
@@ -51,7 +51,7 @@
 
 #define DEBUG(proc_id, args...) _DEBUG(proc_id, DEBUG_TRACE_READ, ##args)
 
-//#define PRINT_INSTRUCTION_INFO
+
 /**************************************************************************************/
 /* Global Variables */
 
@@ -74,11 +74,14 @@ void fill_in_dynamic_info(ctype_pin_inst* info, const InstInfo *insi) {
     info->inst_uid = ins_id;
 
 #ifdef PRINT_INSTRUCTION_INFO
-    std::cout << std::hex << info->instruction_addr << " Next " << info->instruction_next_addr
-	      << " size " << (uint32_t)info->size << " taken " << (uint32_t)info->actually_taken
-	      << " target " << info->branch_target << " pid " << insi->pid << " tid " << insi->tid
-	      << " asm " << std::string(xed_iclass_enum_t2str(xed_decoded_inst_get_iclass(insi->ins)))
-	      << " uid " << std::dec << info->inst_uid << std::endl;
+    std::cout << std::hex << info->instruction_addr << " Next "
+              << info->instruction_next_addr << " size " << (uint32_t)info->size
+              << " taken " << (uint32_t)info->actually_taken << " target "
+              << info->branch_target << " pid " << insi->pid << " tid "
+              << insi->tid << " asm "
+              << std::string(xed_iclass_enum_t2str(
+                   xed_decoded_inst_get_iclass(insi->ins)))
+              << " uid " << std::dec << info->inst_uid << std::endl;
 #endif
 
     if (xed_decoded_inst_get_iclass(insi->ins) == XED_ICLASS_RET_FAR ||
@@ -134,13 +137,13 @@ int memtrace_trace_read(int proc_id, ctype_pin_inst* next_pi) {
   InstInfo *insi;
 
   do {
-     insi = const_cast<InstInfo *>(trace_readers[proc_id]->nextInstruction());
-     ins_id++;
-     if (!insi->valid) {
-       insi = const_cast<InstInfo *>(trace_readers[proc_id]->nextInstruction());
-       ins_id++;
-       return 0; //end of trace
-     }
+    insi = const_cast<InstInfo*>(trace_readers[proc_id]->nextInstruction());
+    ins_id++;
+    if(!insi->valid) {
+      insi = const_cast<InstInfo*>(trace_readers[proc_id]->nextInstruction());
+      ins_id++;
+      return 0;  // end of trace
+    }
   } while (insi->pid != prior_pid || insi->tid != prior_tid);
 
   memset(next_pi, 0, sizeof(ctype_pin_inst));
@@ -214,16 +217,21 @@ void memtrace_setup(uns proc_id) {
 
   //FFWD
   const InstInfo *insi = trace_readers[proc_id]->nextInstruction();
+  ins_id++;
 
   if(FAST_FORWARD) {
     std::cout << "Enter fast forward " << ins_id << std::endl;
   }
 
-  while (!insi->valid || ffwd(*insi)) {
+  // FFWD the first instruction and as many as later ffwding parameters specify.
+  // insi is invalid for the first instruction, and once end of trace is reached.
+  // Reaching the end of the trace breaks out of the loop and segfaults later in this function.
+  while ((!insi->valid && ins_id < 10) || ffwd(*insi)) {
     insi = trace_readers[proc_id]->nextInstruction();
     ins_id++;
     if ((ins_id % 10000000) == 0)
-      std::cout << "Fast forwarded " << ins_id << " instructions." << std::endl;
+      std::cout << "Fast forwarded " << ins_id << " instructions." 
+      << (insi->valid ? " Valid" : " Invalid") << " instr." << std::endl;
   }
 
   if(FAST_FORWARD) {
diff --git a/src/frontend/memtrace_trace_reader.cc b/src/frontend/memtrace_trace_reader.cc
index 019cd65..adec3d7 100644
--- a/src/frontend/memtrace_trace_reader.cc
+++ b/src/frontend/memtrace_trace_reader.cc
@@ -238,7 +238,8 @@ void TraceReader::fillCache(uint64_t _vAddr, uint8_t _reported_size, uint8_t *in
     else res = xed_decode(ins, loc, size);
 
     if (res != XED_ERROR_NONE) {
-      warn("XED decode error for 0x%lx: %s %u", _vAddr, xed_error_enum_t2str(res), _reported_size);
+      // warn("XED decode error for 0x%lx: %s %u, replacing with nop\n", _vAddr, xed_error_enum_t2str(res), _reported_size);
+      *ins = *makeNop(_reported_size);
     }
     // Record if this instruction requires memory operands, since the trace
     // will deliver it in additional pieces
@@ -326,16 +327,21 @@ unique_ptr<xed_decoded_inst_t> TraceReader::makeNop(uint8_t _length) {
 
 void TraceReader::init_buffer() {
     //Push one dummy entry so we can pop in nextInstruction()
-    ins_buffer.emplace_back(InstInfo());
+    if(!buf_size_)
+        ins_buffer.emplace_back(InstInfo());
 
     for (uint32_t i = 0; i < buf_size_; i++) {
-        ins_buffer.emplace_back(*getNextInstruction());
+        InstInfo* tmp = getNextInstruction();
+        assert(tmp && tmp->valid);
+        ins_buffer.emplace_back(*tmp);
     }
 }
 
 const InstInfo *TraceReader::nextInstruction() {
     ins_buffer.pop_front();
-    ins_buffer.emplace_back(*getNextInstruction());
+    InstInfo* tmp = getNextInstruction();
+    // assert(tmp && tmp->valid); // fails for the first instruction
+    ins_buffer.emplace_back(*tmp);
     return &ins_buffer.front();
 }
 
diff --git a/src/frontend/memtrace_trace_reader_memtrace.cc b/src/frontend/memtrace_trace_reader_memtrace.cc
index 846a100..0e12b6c 100644
--- a/src/frontend/memtrace_trace_reader_memtrace.cc
+++ b/src/frontend/memtrace_trace_reader_memtrace.cc
@@ -109,7 +109,7 @@ void TraceReaderMemtrace::binaryGroupPathIs(const std::string &_path) {
                   error.c_str());
             return;
         }
-        module_mapper_ = module_mapper_t::create(directory_.modfile_bytes,
+        module_mapper_ = module_mapper_t::create(directory_.modfile_bytes_,
 #ifdef ZSIM_USE_YT
 						 parse_buildid_string,
 #else
@@ -146,7 +146,6 @@ bool TraceReaderMemtrace::initTrace() {
 bool TraceReaderMemtrace::getNextInstruction__(InstInfo *_info, InstInfo *_prior) {
   uint32_t prior_isize = mt_prior_isize_;
   bool complete = false;
-
   while (*mt_iter_ != *mt_end_) {
     switch (mt_state_) {
       case (MTState::INST):
@@ -208,16 +207,15 @@ bool TraceReaderMemtrace::getNextInstruction__(InstInfo *_info, InstInfo *_prior
             warn("Unexpected PID/TID/PC switch following 0x%lx\n", _info->pc);
             mt_state_ = MTState::INST;
           }
-        } else if (type_is_instr(mt_ref_.instr.type)) {
-	  //REP Instructions with REP count 0
-	  warn("REP BUG: Data size does not match instruction 0x%lx - PATCHING size, success!\n",
+        } else if(type_is_instr(mt_ref_.instr.type)) {
+          // REP Instructions with REP count 0
+          warn("REP BUG: Data size does not match instruction 0x%lx - PATCHING "
+               "size, success!\n",
                _info->pc);
-
-	  mt_state_ = MTState::INST;
-	  complete = true;
-	  goto PATCH_REP;
-        }
-	else {
+          mt_state_ = MTState::INST;
+          complete  = true;
+          goto PATCH_REP;
+        } else {
           warn("Expected data but found type '%s'\n",
                trace_type_names[mt_ref_.data.type]);
           mt_state_ = MTState::INST;
@@ -251,7 +249,7 @@ bool TraceReaderMemtrace::getNextInstruction__(InstInfo *_info, InstInfo *_prior
       break;
     }
   }
- PATCH_REP:
+PATCH_REP:
   // Compute the branch target information for the prior instruction
   _prior->target = _info->pc;  // TODO(granta): Invalid for pid/tid switch
   if (_prior->taken) {  // currently set iif conditional branch
@@ -288,7 +286,6 @@ void TraceReaderMemtrace::processInst(InstInfo *_info) {
   bool unknown_type, cond_branch;
   xed_decoded_inst_t *xed_ins;
   auto &xed_tuple = (*xed_map_iter).second;
-
   tie(mt_mem_ops_, unknown_type, cond_branch, std::ignore, std::ignore) = xed_tuple;
   mt_prior_isize_ = mt_ref_.instr.size;
   xed_ins = std::get<MAP_XED>(xed_tuple).get();
diff --git a/src/frontend/pin_trace_read.cc b/src/frontend/pin_trace_read.cc
index cbfb098..8dfd355 100644
--- a/src/frontend/pin_trace_read.cc
+++ b/src/frontend/pin_trace_read.cc
@@ -35,6 +35,7 @@
 #include "isa/isa.h"
 
 extern "C" {
+#include "globals/assert.h"
 #include "globals/utils.h"
 }
 
diff --git a/src/frontend/pt_fe.cc b/src/frontend/pt_fe.cc
new file mode 100644
index 0000000..8412949
--- /dev/null
+++ b/src/frontend/pt_fe.cc
@@ -0,0 +1,294 @@
+/* Copyright 2020 University of Michigan (implemented by Tanvir Ahmed Khan)
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/***************************************************************************************
+ * File         : frontend/pt_fe.cc
+ * Author       : Tanvir Ahmed Khan
+ * Date         : 12/05/2020
+ * Description  : Interface to simulate Intel processor trace
+ ***************************************************************************************/
+extern "C" {
+#include "debug/debug.param.h"
+#include "debug/debug_macros.h"
+#include "globals/assert.h"
+#include "globals/global_defs.h"
+#include "globals/global_types.h"
+#include "globals/global_vars.h"
+#include "globals/utils.h"
+}
+
+#include "bp/bp.h"
+#include "statistics.h"
+#include "bp/bp.param.h"
+#include "ctype_pin_inst.h"
+#include "frontend/pt_fe.h"
+#include "isa/isa.h"
+#include "./pin/pin_lib/uop_generator.h"
+#include "./pin/pin_lib/x86_decoder.h"
+
+#define DR_DO_NOT_DEFINE_int64
+#include "frontend/pt_trace_reader_pt.h"
+
+/**************************************************************************************/
+/* Macros */
+
+#define DEBUG(proc_id, args...) _DEBUG(proc_id, DEBUG_TRACE_READ, ##args)
+
+
+/**************************************************************************************/
+/* Global Variables for PT */
+
+char* pt_trace_files[MAX_NUM_PROCS];
+TraceReaderPT *pt_trace_readers[MAX_NUM_PROCS];
+ctype_pin_inst* pt_next_pi;
+uint64_t pt_ins_id = 0;
+uint64_t pt_prior_tid = 0;
+uint64_t pt_prior_pid = 0;
+
+/**************************************************************************************/
+/* Private Functions for PT */
+
+void pt_fill_in_dynamic_info(ctype_pin_inst* info, const InstInfo *insi) {
+    uint8_t ld = 0;
+    uint8_t st = 0;
+
+    info->actually_taken = insi->taken;
+    info->branch_target = insi->target;
+    info->inst_uid = pt_ins_id;
+
+#ifdef PRINT_INSTRUCTION_INFO
+    std::cout << std::hex << info->instruction_addr << " Next " << info->instruction_next_addr
+	      << " size " << (uint32_t)info->size << " taken " << (uint32_t)info->actually_taken
+	      << " target " << info->branch_target << " asm "
+	      << std::string(xed_iclass_enum_t2str(xed_decoded_inst_get_iclass(insi->ins))) <<std::endl;
+#endif
+
+    // TODO: Check whether we need to add this fix to memtrace fe
+    if (xed_decoded_inst_get_iclass(insi->ins) == XED_ICLASS_RET_FAR ||
+	xed_decoded_inst_get_iclass(insi->ins) == XED_ICLASS_RET_NEAR)
+      info->actually_taken = 1;
+
+    for (uint8_t op = 0; op < xed_decoded_inst_number_of_memory_operands(insi->ins); op++) {
+	//predicated true ld/st are handled just as regular ld/st
+	if(xed_decoded_inst_mem_read(insi->ins, op) && !insi->mem_used[op]) {
+	    //Handle predicated stores specially?
+	    info->ld_vaddr[ld++] = insi->mem_addr[op];
+	}
+	else if(xed_decoded_inst_mem_read(insi->ins, op)) {
+	    info->ld_vaddr[ld++] = insi->mem_addr[op];
+	}
+	if(xed_decoded_inst_mem_written(insi->ins, op) && !insi->mem_used[op]) {
+	    //Handle predicated stores specially?
+	    info->st_vaddr[st++] = insi->mem_addr[op];
+	}
+	else if(xed_decoded_inst_mem_written(insi->ins, op)) {
+	    info->st_vaddr[st++] = insi->mem_addr[op];
+	}
+    }
+}
+
+int pt_ffwd(const INS& ins) {
+  if (!FAST_FORWARD) {
+    return 0;
+  }
+#ifdef MEMTRACE
+  if(INS_Opcode(ins) == XED_ICLASS_XCHG && INS_OperandReg(ins, 0) == XED_REG_RCX &&
+     INS_OperandReg(ins, 1) == XED_REG_RCX) {
+    return 0;
+  }
+  return 1;
+#endif
+}
+
+int pt_roi(const INS& ins) {
+#ifdef MEMTRACE
+  if(INS_Opcode(ins) == XED_ICLASS_XCHG && INS_OperandReg(ins, 0) == XED_REG_RCX &&
+     INS_OperandReg(ins, 1) == XED_REG_RCX) {
+    return 1;
+  }
+  return 0;
+#endif
+}
+
+int pt_trace_read(int proc_id, ctype_pin_inst* pt_next_pi) {
+  InstInfo *insi;
+
+  do {
+     insi = const_cast<InstInfo *>(pt_trace_readers[proc_id]->nextInstruction());
+     pt_ins_id++;
+     if (!insi->valid)
+       return 0; //end of trace
+  } while (insi->pid != pt_prior_pid || insi->tid != pt_prior_tid);
+
+  memset(pt_next_pi, 0, sizeof(ctype_pin_inst));
+  fill_in_basic_info(pt_next_pi, *insi);
+  assert(pt_next_pi->instruction_next_addr && "instruction_next_addr not set");
+  pt_fill_in_dynamic_info(pt_next_pi, insi);
+  uint32_t max_op_width = add_dependency_info(pt_next_pi, *insi);
+  fill_in_simd_info(pt_next_pi, *insi, max_op_width);
+  apply_x87_bug_workaround(pt_next_pi, *insi);
+  fill_in_cf_info(pt_next_pi, *insi);
+  pt_next_pi->actually_taken = insi->taken;
+  // TODO: This happens for PT for example when there is some traces missing,
+  // Check whether this can also happen for memtrace fe
+  if(insi->static_target) {
+      // XED encoded target may not be right, so set it directly
+      //std::cout << "setting branch target to: " << insi->static_target << " for PC: " << insi->pc << std::endl;
+      pt_next_pi->branch_target = insi->static_target;
+  }
+  /* std::cout << "branch target for PC: " << pt_next_pi->instruction_addr << " is: " << pt_next_pi->branch_target << std::endl; */
+  print_err_if_invalid(pt_next_pi, *insi);
+
+  //End of ROI
+  if (pt_roi(*insi))
+    return 0;
+
+  return 1;
+}
+
+void pt_init(void) {
+  /*ASSERTM(0, !FETCH_OFF_PATH_OPS,
+          "Trace frontend does not support wrong path. Turn off "
+          "FETCH_OFF_PATH_OPS\n");
+  */
+
+  uop_generator_init(NUM_CORES);
+  init_pin_opcode_convert();
+  init_reg_compress_map();
+  init_x87_stack_delta();
+
+  pt_next_pi = (ctype_pin_inst*)malloc(NUM_CORES * sizeof(ctype_pin_inst));
+  for(int i = 0; i < MAX_NUM_PROCS; ++i) {
+      pt_trace_readers[i] = nullptr;
+  }
+
+  /* temp variable needed for easy initialization syntax */
+  char* tmp_trace_files[MAX_NUM_PROCS] = {
+    CBP_TRACE_R0,  CBP_TRACE_R1,  CBP_TRACE_R2,  CBP_TRACE_R3,  CBP_TRACE_R4,
+    CBP_TRACE_R5,  CBP_TRACE_R6,  CBP_TRACE_R7,  CBP_TRACE_R8,  CBP_TRACE_R9,
+    CBP_TRACE_R10, CBP_TRACE_R11, CBP_TRACE_R12, CBP_TRACE_R13, CBP_TRACE_R14,
+    CBP_TRACE_R15, CBP_TRACE_R16, CBP_TRACE_R17, CBP_TRACE_R18, CBP_TRACE_R19,
+    CBP_TRACE_R20, CBP_TRACE_R21, CBP_TRACE_R22, CBP_TRACE_R23, CBP_TRACE_R24,
+    CBP_TRACE_R25, CBP_TRACE_R26, CBP_TRACE_R27, CBP_TRACE_R28, CBP_TRACE_R29,
+    CBP_TRACE_R30, CBP_TRACE_R31, CBP_TRACE_R32, CBP_TRACE_R33, CBP_TRACE_R34,
+    CBP_TRACE_R35, CBP_TRACE_R36, CBP_TRACE_R37, CBP_TRACE_R38, CBP_TRACE_R39,
+    CBP_TRACE_R40, CBP_TRACE_R41, CBP_TRACE_R42, CBP_TRACE_R43, CBP_TRACE_R44,
+    CBP_TRACE_R45, CBP_TRACE_R46, CBP_TRACE_R47, CBP_TRACE_R48, CBP_TRACE_R49,
+    CBP_TRACE_R50, CBP_TRACE_R51, CBP_TRACE_R52, CBP_TRACE_R53, CBP_TRACE_R54,
+    CBP_TRACE_R55, CBP_TRACE_R56, CBP_TRACE_R57, CBP_TRACE_R58, CBP_TRACE_R59,
+    CBP_TRACE_R60, CBP_TRACE_R61, CBP_TRACE_R62, CBP_TRACE_R63,
+  };
+  if(DUMB_CORE_ON) {
+    // avoid errors by specifying a trace known to be good
+    tmp_trace_files[DUMB_CORE] = tmp_trace_files[0];
+  }
+
+  for(uns proc_id = 0; proc_id < MAX_NUM_PROCS; proc_id++) {
+    pt_trace_files[proc_id] = tmp_trace_files[proc_id];
+  }
+  for(uns proc_id = 0; proc_id < NUM_CORES; proc_id++) {
+    pt_setup(proc_id);
+  }
+}
+
+Addr pt_next_fetch_addr(uns proc_id) {
+    Addr next = pt_next_pi[proc_id].instruction_addr;
+  return convert_to_cmp_addr(proc_id, next);
+}
+
+Flag pt_can_fetch_op(uns proc_id) {
+  assert(proc_id == 0);
+  return !(uop_generator_get_eom(proc_id) && trace_read_done[proc_id]);
+}
+
+void pt_fetch_op(uns proc_id, Op *op) {
+  if(uop_generator_get_bom(proc_id)) {
+    uop_generator_get_uop(proc_id, op, &pt_next_pi[proc_id]);
+  } else {
+    uop_generator_get_uop(proc_id, op, NULL);
+  }
+
+  if(uop_generator_get_eom(proc_id)) {
+    int success = pt_trace_read(proc_id, &pt_next_pi[proc_id]);
+    static int ins = 0;
+    ins++;
+    if(!success) {
+      trace_read_done[proc_id] = TRUE;
+      reached_exit[proc_id]    = TRUE;
+    }
+  }
+}
+
+void pt_redirect(uns proc_id, uns64 inst_uid, Addr fetch_addr) {
+  assert(0);
+}
+
+void pt_recover(uns proc_id, uns64 inst_uid) {
+  assert(0);
+}
+
+void pt_retire(uns proc_id, uns64 inst_uid) {
+  // Similar to memtrace, PT frontend does not need to communicate to PIN to 
+  // determine which instruction are retired.
+}
+
+void pt_close_trace_file(uns proc_id) {
+  printf("Closing PT file for %u\n", proc_id);
+}
+
+void pt_done() {
+  printf("Frontend simulation finished for all PTs\n");
+  for(int i = 0; i < MAX_NUM_PROCS; ++i) {
+      delete pt_trace_readers[i];
+  }
+}
+
+void pt_setup(uns proc_id) {
+  std::string path(pt_trace_files[proc_id]);
+  std::string trace(path);
+
+  pt_trace_readers[proc_id] = new TraceReaderPT(trace);
+
+  //FFWD
+  const InstInfo *insi = pt_trace_readers[proc_id]->nextInstruction();
+  pt_ins_id++;
+
+  if(FAST_FORWARD) {
+    std::cout << "Enter fast forward " << pt_ins_id << std::endl;
+  }
+
+  while (!insi->valid || pt_ffwd(*insi)) {
+    insi = pt_trace_readers[proc_id]->nextInstruction();
+    pt_ins_id++;
+    if ((pt_ins_id % 10000000) == 0)
+      std::cout << "Fast forwarded " << pt_ins_id << " instructions." << std::endl;
+  }
+
+  if(FAST_FORWARD) {
+    std::cout << "Exit fast forward " << pt_ins_id << std::endl;
+  }
+
+  pt_prior_pid = insi->pid;
+  pt_prior_tid = insi->tid;
+  assert(pt_prior_tid);
+  assert(pt_prior_pid);
+  pt_trace_read(proc_id, &pt_next_pi[proc_id]);
+}
diff --git a/src/frontend/pt_fe.h b/src/frontend/pt_fe.h
new file mode 100644
index 0000000..140dbf5
--- /dev/null
+++ b/src/frontend/pt_fe.h
@@ -0,0 +1,67 @@
+/* Copyright 2020 University of Michigan (implemented by Tanvir Ahmed Khan)
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/***************************************************************************************
+ * File         : frontend/pt_fe.h
+ * Author       : Tanvir Ahmed Khan
+ * Date         : 12/05/2020
+ * Description  : Interface to simulate Intel processor trace
+ ***************************************************************************************/
+
+#ifndef __PT_FE_H__
+#define __PT_FE_H__
+
+#include "globals/global_types.h"
+
+/**************************************************************************************/
+/* Forward Declarations */
+
+struct Trace_Uop_struct;
+typedef struct Trace_Uop_struct Trace_Uop;
+struct Op_struct;
+
+/**************************************************************************************/
+/* Prototypes */
+
+#ifdef __cplusplus
+extern "C" {
+#endif //__cplusplus
+
+void pt_init(void);
+
+/* Implementing the frontend interface */
+Addr pt_next_fetch_addr(uns proc_id);
+Flag pt_can_fetch_op(uns proc_id);
+void pt_fetch_op(uns proc_id, Op *op);
+void pt_redirect(uns proc_id, uns64 inst_uid, Addr fetch_addr);
+void pt_recover(uns proc_id, uns64 inst_uid);
+void pt_retire(uns proc_id, uns64 inst_uid);
+
+/* For restarting of pt */
+void pt_done(void);
+void pt_close_trace_file(uns proc_id);
+void pt_setup(uns proc_id);
+
+#ifdef __cplusplus
+}
+#endif // __cplusplus
+
+#endif //__PT_FE_H__
\ No newline at end of file
diff --git a/src/frontend/pt_trace_reader_pt.h b/src/frontend/pt_trace_reader_pt.h
new file mode 100644
index 0000000..abaeacd
--- /dev/null
+++ b/src/frontend/pt_trace_reader_pt.h
@@ -0,0 +1,304 @@
+/* Copyright 2020 University of Michigan (implemented by Tanvir Ahmed Khan)
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/***************************************************************************************
+ * File         : frontend/pt_trace_reader_pt.h
+ * Author       : Tanvir Ahmed Khan
+ * Date         : 12/05/2020
+ * Notes        : This code has been adapted from zsim which was released under
+ *                GNU General Public License as published by the Free Software
+ *                Foundation, version 2.
+ * Description  : Interface to read gziped Intel processor trace
+ ***************************************************************************************/
+#ifndef __PT_TRACE_READER_PT_H__
+#define __PT_TRACE_READER_PT_H__
+#include <stdlib.h>
+#include <zlib.h>
+
+#include <map>
+#include <string>
+#include <vector>
+
+#include <boost/algorithm/string.hpp>
+
+#include "frontend/memtrace_trace_reader.h"
+
+#include "general.param.h"
+
+#define GZ_BUFFER_SIZE 80
+#define panic(...) printf(__VA_ARGS__)
+
+struct PTInst {
+  uint64_t pc;
+  uint8_t size;
+  uint8_t inst_bytes[16];
+};
+
+class TraceReaderPT : public TraceReader {
+private:
+  gzFile raw_file = NULL;
+  InstInfo inst_info_a;
+  InstInfo inst_info_b;
+  PTInst   pt_inst_a, pt_inst_b;
+  bool enable_code_bloat_effect = false;
+  bool use_info_a = true; // true when filling info a, false when filling info b
+  std::map<uint64_t, uint64_t> *prev_to_new_bbl_address_map = nullptr;
+  uint64_t num_nops_in_trace = 0, num_inserted_nops = 0;
+  uint64_t num_direct_brs_in_trace = 0, num_inserted_direct_brs = 0;
+
+public:
+  bool read_next_line(PTInst &inst) {
+      static uns64 num_nops_at_start = 0;
+      if(num_nops_at_start <= NUM_NOPS) { // = is because the last one will be overwritten as a JMP to the real instruction stream
+          inst.pc = NOPS_BB_START + num_nops_at_start++;
+          inst.size = 1;
+          inst.inst_bytes[0] = 0x90;
+          return true;
+      }
+    if (raw_file == NULL)
+      return false;
+    char buffer[GZ_BUFFER_SIZE];
+    if (gzgets(raw_file, buffer, GZ_BUFFER_SIZE) == Z_NULL)
+      return false;
+    std::string line = buffer;
+    boost::trim_if(line, boost::is_any_of("\n"));
+    std::vector<std::string> parsed;
+    boost::split(parsed, line, boost::is_any_of(" \n"),
+                 boost::token_compress_on);
+    if (parsed.size() < 3)
+      panic("TraceReaderPT: GZ File line has less than 3 items");
+    inst.pc = strtoul(parsed[0].c_str(), NULL, 16);
+    inst.size = strtoul(parsed[1].c_str(), NULL, 10);
+    for (uint8_t i = 0; i < inst.size; i++) {
+      inst.inst_bytes[i] = strtoul(parsed[i + 2].c_str(), NULL, 16);
+    }
+    if (enable_code_bloat_effect && (prev_to_new_bbl_address_map != nullptr)) {
+      uint64_t result = inst.pc;
+      auto it = prev_to_new_bbl_address_map->lower_bound(inst.pc);
+      if (it->first == inst.pc) {
+        result = it->second;
+      } else {
+        if (it == prev_to_new_bbl_address_map->begin())
+          result = inst.pc;
+        else {
+          it--;
+          result = it->second + (inst.pc - (it->first));
+        }
+      }
+      inst.pc = result;
+    }
+    return true;
+  }
+
+  // TODO: Move this to memtrace_trace_reader.h
+  xed_decoded_inst_t* createJmp(uint64_t displacement) {
+      xed_encoder_instruction_t inst;
+      xed_state_t state;
+      state.mmode = XED_MACHINE_MODE_LONG_64;
+      xed_encoder_request_t req;
+      xed_inst1(&inst, state, XED_ICLASS_JMP, 64,  xed_relbr(displacement - 5, 32)); // -5 is due to this jump insn being 5 bytes large (1 op, 4 32 bit disp)
+      xed_encoder_request_zero_set_mode(&req, &state);
+      if(!xed_convert_to_encoder_request(&req, &inst)) {
+          panic("Encoder conversion failed! Is the displacement too large?");
+          return nullptr;
+      }
+      xed_uint8_t encodedBytes[15];
+      unsigned int numBytesUsed = 0;
+      xed_error_enum_t error = xed_encode(&req, encodedBytes, sizeof(encodedBytes), &numBytesUsed);
+      if(error != XED_ERROR_NONE) {
+          panic("Failed to encode due to: %s\n", xed_error_enum_t2str(error));
+          return nullptr;
+      }
+      xed_decoded_inst_t* decoded_inst = new xed_decoded_inst_t;
+      xed_decoded_inst_zero(decoded_inst);
+      xed_decoded_inst_set_mode(decoded_inst, XED_MACHINE_MODE_LONG_64, XED_ADDRESS_WIDTH_64b);
+      error = xed_decode(decoded_inst, encodedBytes, numBytesUsed);
+      if(error == XED_ERROR_NONE) {
+          return decoded_inst;
+      }
+      delete decoded_inst;
+      panic("Could not decode due to %s\n", xed_error_enum_t2str(error));
+      return nullptr;
+  }
+
+  xed_decoded_inst_t * createNop(uint64_t length) {
+      xed_state_t state;
+      state.mmode = XED_MACHINE_MODE_LONG_64;
+      uint8_t buf[10];
+      xed_error_enum_t res = xed_encode_nop(buf, length);
+      if(res != XED_ERROR_NONE) {
+          panic("Failed to encode due to %s\n", xed_error_enum_t2str(res));
+          return nullptr;
+      }
+      xed_decoded_inst_t* decoded_inst = new xed_decoded_inst_t;
+      xed_decoded_inst_zero_set_mode(decoded_inst, &state);
+      res = xed_decode(decoded_inst, buf, sizeof(buf));
+      if(res != XED_ERROR_NONE) {
+          panic("XED NOP decode error! %s\n", xed_error_enum_t2str(res));
+          delete decoded_inst;
+          return nullptr;
+      }
+      return decoded_inst;
+  }
+
+  // ret true when insn is a syscall (and thus should be skipped)
+  bool processInst(PTInst &next_line) {
+      /* std::cout << "Processing Inst w/ PC: " << std::hex << next_line.pc << std::endl; */
+      // Get the XED info from the cache, creating it if needed
+      auto xed_map_iter = xed_map_.find(next_line.pc);
+      if (xed_map_iter == xed_map_.end()) {
+          fillCache(next_line.pc, next_line.size, next_line.inst_bytes);
+          xed_map_iter = xed_map_.find(next_line.pc);
+          assert((xed_map_iter != xed_map_.end()));
+      }
+      bool unknown_type, cond_branch;
+      int mem_ops_;
+      xed_decoded_inst_t *xed_ins;
+      auto &xed_tuple = (*xed_map_iter).second;
+      tie(mem_ops_, unknown_type, cond_branch, std::ignore, std::ignore) =
+          xed_tuple;
+      xed_ins = std::get<MAP_XED>(xed_tuple).get();
+      InstInfo& _info = (use_info_a ? inst_info_a : inst_info_b);
+      InstInfo& _prior = (use_info_a ? inst_info_b : inst_info_a);
+      auto& ins = _prior; // have to do this for the macros to work
+      if(ins.ins) {
+          if(INS_Category(ins) == XED_CATEGORY_NOP) {
+                  ++num_nops_in_trace;
+          } else if (INS_IsDirectBranchOrCall(ins)) {
+              ++num_direct_brs_in_trace;
+          }
+      }
+      bool inserted_nop = false;
+      if (_prior.valid && INS_IsRep(ins)) {
+        // repz insns aren't supported, so just nop them
+        auto length = xed_decoded_inst_get_length(_prior.ins);
+        // std::cout << xed_iclass_enum_t2str(INS_Opcode(ins)) << " with PC " << std::hex << _prior.pc << " will become a nop of length " << length << std::endl;
+        _prior.ins = createNop(length);
+        inserted_nop = true;
+        if(_prior.pc == next_line.pc) {
+            _info = _prior; // skip prior insn
+            return true;
+        }
+      }
+      bool changes_cf = ins.ins && INS_ChangeControlFlow(ins);
+      bool incorrect_branch = ins.ins && INS_IsDirectBranchOrCall(ins) && next_line.pc != INS_DirectBranchOrCallTargetAddress(ins) && next_line.pc != (ins.pc + INS_Size(ins));
+    if(incorrect_branch) {
+        // std::cout << "branch " << INS_Address(ins) << " is incorrect!" << std::endl;
+        // std::cout << "xed target: " << INS_DirectBranchOrCallTargetAddress(ins) << " next pc: " << next_line.pc << std::endl;
+    }
+    if(_prior.valid && (!changes_cf || INS_Category(ins) == XC(SYSCALL) || incorrect_branch) && next_line.pc != _prior.pc + xed_decoded_inst_get_length(_prior.ins)) {
+        // std::cout << xed_iclass_enum_t2str(INS_Opcode(ins)) << " with PC " << std::hex << _prior.pc << " will become a jump to " << std::hex << next_line.pc << std::endl;
+        int64_t diff = std::max(next_line.pc, _prior.pc) - std::min(next_line.pc, _prior.pc);
+        if(next_line.pc < _prior.pc)
+            diff *= -1;
+        // std::cout << "Jump: " << diff << std::endl;
+        xed_decoded_inst_t* new_inst = createJmp(diff);
+        _prior.ins = new_inst;
+        inserted_nop = false; // replaced nop with a jmp, so we really inserted a jmp instead of a nop
+        ++num_inserted_direct_brs;
+        _prior.static_target = next_line.pc;
+    } else if (_prior.valid && xed_decoded_inst_get_attribute(ins.ins, XED_ATTRIBUTE_REP) > 0) {
+        // repz insns aren't supported, so just nop them
+        auto length = xed_decoded_inst_get_length(_prior.ins);
+        // std::cout << xed_iclass_enum_t2str(INS_Opcode(ins)) << " with PC " << std::hex << _prior.pc << " will become a nop of length " << length << std::endl;
+        _prior.ins = createNop(length);
+        ++num_inserted_nops;
+        if(_prior.pc == next_line.pc) {
+            _info = _prior; // skip prior insn
+            return true;
+        }
+    }
+    if(inserted_nop)
+        ++num_inserted_nops;
+    _info.pc = next_line.pc;
+    _info.ins = xed_ins;
+    _info.pid = 1;
+    _info.tid = 1;
+    _info.target = 0; // Set when the next instruction is evaluated
+    _info.static_target = 0; // Set when the next instruction is evaluated
+    _prior.target = _info.pc;
+    _info.taken =
+        cond_branch; // Patched when the next instruction is evaluated
+    _info.mem_addr[0] = 0x4040;
+    _info.mem_addr[1] = 0x8080;
+    _info.mem_used[0] = false;
+    _info.mem_used[1] = false;
+    _info.unknown_type = unknown_type;
+    _info.valid = true;
+
+    for (int i = 0; i < mem_ops_; i++) {
+      if (i >= 2)
+        break;
+      _info.mem_used[i] = true;
+    }
+    // TODO add this?
+    if(_prior.valid)
+        _prior.taken = _info.pc != (_prior.pc + INS_Size(ins));
+    return false;
+  }
+  TraceReaderPT(
+      const std::string &_trace, bool _enable_code_bloat_effect = false,
+      std::map<uint64_t, uint64_t> *_prev_to_new_bbl_address_map = nullptr) {
+    raw_file = gzopen(_trace.c_str(), "rb");
+    if (!raw_file) {
+      panic("TraceReaderPT: Invalid GZ File");
+      throw "Could not open file";
+    }
+    enable_code_bloat_effect = _enable_code_bloat_effect;
+    prev_to_new_bbl_address_map = _prev_to_new_bbl_address_map;
+  }
+  const InstInfo *getNextInstruction() override {
+    PTInst& next_line = (use_info_a ? pt_inst_a : pt_inst_b);
+    do {
+        if (read_next_line(next_line) == false)
+          return &invalid_info_;
+    } while(processInst(next_line));
+    // todo: have process inst ret if I should use this info or not
+    // want to be able to skip syscalls for now
+    InstInfo& _prior = (use_info_a ? inst_info_b : inst_info_a);
+    static bool should_be_valid = false;
+    if(should_be_valid)
+        assert(_prior.valid);
+    should_be_valid = true;
+    use_info_a = !use_info_a;
+    return &_prior;
+  }
+  void binaryGroupPathIs(const std::string &_path) override {
+    // do nothing
+  }
+  bool initTrace() override {
+    getNextInstruction(); // fill in info a, will lack BP information (hopefully we won't need it...)
+    return true;
+  }
+  bool locationForVAddr(uint64_t _vaddr, uint8_t **_loc,
+                        uint64_t *_size) override {
+    // do nothing
+    return true;
+  }
+  ~TraceReaderPT() {
+      std::cout << std::dec << "num trace nops: " << num_nops_in_trace << " , num added nops: " << num_inserted_nops << ", ratio: " << double(num_inserted_nops) / double(num_nops_in_trace) << std::endl;
+      std::cout << "num trace direct brs: " << num_direct_brs_in_trace << " , num added direct brs: " << num_inserted_direct_brs << ", ratio: " << double(num_inserted_direct_brs) / double(num_direct_brs_in_trace) << std::endl;
+    if (raw_file != NULL)
+      gzclose(raw_file);
+  }
+};
+
+#endif // __PT_TRACE_READER_PT_H__
diff --git a/src/general.param.def b/src/general.param.def
index 93216f8..dcac40d 100644
--- a/src/general.param.def
+++ b/src/general.param.def
@@ -62,7 +62,6 @@ DEF_PARAM( fast_forward                 , FAST_FORWARD              , uns64    ,
 DEF_PARAM( fast_forward_trace_ins       , FAST_FORWARD_TRACE_INS    , uns64    , uns64   , 0        ,       )
 DEF_PARAM( fast_forward_until_addr      , FAST_FORWARD_UNTIL_ADDR   , uns      , uns     , 0        ,       )
 DEF_PARAM( warmup                       , WARMUP                    , uns64    , uns64   , 0        ,       )
-
 DEF_PARAM( heartbeat_interval           , HEARTBEAT_INTERVAL        , uns    , uns       , 1000000  ,       ) 
 DEF_PARAM( num_heartbeats               , NUM_HEARTBEATS            , uns    , uns       , 0        ,       ) 
  
@@ -96,3 +95,5 @@ DEF_PARAM( use_unsure_free_lists        , USE_UNSURE_FREE_LISTS     , Flag   , F
 
 DEF_PARAM( optimizer2_max_num_slaves    , OPTIMIZER2_MAX_NUM_SLAVES , uns    , uns       , 64       ,       )
 DEF_PARAM( optimizer2_perfect_memoryless, OPTIMIZER2_PERFECT_MEMORYLESS, Flag, Flag      , FALSE    ,       )
+DEF_PARAM( num_nops                     , NUM_NOPS                   , uns64  , uns64    , 0        ,       )
+DEF_PARAM( nops_bb_start                , NOPS_BB_START              , uns64  , uns64    , 0x5000000,       )
diff --git a/src/globals/global_types.h b/src/globals/global_types.h
index 080c132..9c4c3fc 100644
--- a/src/globals/global_types.h
+++ b/src/globals/global_types.h
@@ -76,7 +76,7 @@ typedef int64 SCounter;
 typedef int64 Quad;
 typedef int32 Long;
 typedef int16 Word;
-typedef int8  Byte;
+//typedef int8  Byte;
 typedef uns64 UQuad;
 typedef uns32 ULong;
 typedef uns16 UWord;
diff --git a/src/globals/global_vars.h b/src/globals/global_vars.h
index 4feb9ef..75d6d69 100644
--- a/src/globals/global_vars.h
+++ b/src/globals/global_vars.h
@@ -34,6 +34,7 @@
 #include <stdio.h>
 #include "globals/global_types.h"
 
+#include "libs/hash_lib.h"
 
 /**************************************************************************************/
 
@@ -60,6 +61,8 @@ extern int   mystatus_fd;
 extern Flag frontend_gated;
 extern uns  num_fetched_lowconf_brs;
 
+extern Hash_Table per_branch_stat;
+
 /**************************************************************************************/
 
 #endif /* #ifndef __GLOBAL_VARS_H__ */
diff --git a/src/icache_stage.c b/src/icache_stage.c
index a7eff2c..aad3ab8 100644
--- a/src/icache_stage.c
+++ b/src/icache_stage.c
@@ -52,6 +52,7 @@
 #include "prefetcher/l2l1pref.h"
 #include "prefetcher/stream_pref.h"
 #include "statistics.h"
+#include "libs/list_lib.h"
 
 #include "prefetcher/fdip.h"
 #include "prefetcher/pref.param.h"
@@ -60,26 +61,40 @@
 
 #define DEBUG(proc_id, args...) _DEBUG(proc_id, DEBUG_ICACHE_STAGE, ##args)
 
-#define STAGE_MAX_OP_COUNT ISSUE_WIDTH
-
+#define STAGE_MAX_OP_COUNT  UC_ISSUE_WIDTH
+#define DUMMY_ADDR_UC_FETCH 0x1
 
 /**************************************************************************************/
 /* Global Variables */
 
 Icache_Stage* ic = NULL;
+List op_buf;
+Counter                last_issued_op_num = 0;
 
 extern Cmp_Model              cmp_model;
 extern Memory*                mem;
 extern Rob_Stall_Reason       rob_stall_reason;
 extern Rob_Block_Issue_Reason rob_block_issue_reason;
+extern Addr                   runahead_pc;
+extern Flag                   runahead_disable;
+extern uns64                  last_runahead_uid;
+extern uns64                  max_runahead_uid;
+extern Counter                last_runahead_op;
+extern Counter                max_runahead_op;
+extern Counter                max_op_num;
+extern Flag                   mem_req_failed;
+extern Counter                last_recover_cycle;
 
 /**************************************************************************************/
 /* Local prototypes */
 
 static inline Icache_State icache_issue_ops(Break_Reason*, uns*,
-                                            Inst_Info** line);
+                                            Inst_Info** line,
+                                            Flag uop_cache_issue_ops);
+static inline Inst_Info**  lookup_cache(Flag* uop_cache_fetch);
 static Inst_Info**         ic_pref_cache_access(void);
 int32_t                    inst_lost_get_full_window_reason(void);
+static inline void         log_stats_ic_miss(void);
 
 /**************************************************************************************/
 /* set_icache_stage: */
@@ -108,8 +123,8 @@ void init_icache_stage(uns8 proc_id, const char* name) {
   ic->sd.name = (char*)strdup(name);
 
   /* initialize the ops array */
-  ic->sd.max_op_count = ISSUE_WIDTH;
-  ic->sd.ops          = (Op**)malloc(sizeof(Op*) * ISSUE_WIDTH);
+  ic->sd.max_op_count = STAGE_MAX_OP_COUNT;
+  ic->sd.ops          = (Op**)malloc(sizeof(Op*) * STAGE_MAX_OP_COUNT);
 
   /* initialize the cache structure */
   init_cache(&ic->icache, "ICACHE", ICACHE_SIZE, ICACHE_ASSOC, ICACHE_LINE_SIZE,
@@ -149,7 +164,32 @@ void init_icache_stage(uns8 proc_id, const char* name) {
 /* icache_init_trace:  */
 
 void init_icache_trace() {
-  ic->next_fetch_addr = frontend_next_fetch_addr(ic->proc_id);
+  if (FDIP_ENABLE)
+    ASSERT(0, LOOKAHEAD_BUF_SIZE && PERFECT_NT_BTB);
+  if (LOOKAHEAD_BUF_SIZE) {
+    ASSERT(0, MEMTRACE); //Lookahead buffer only works in trace mode
+    init_list(&op_buf, "op_buf", sizeof(Op*), TRUE);
+    while (list_get_count(&op_buf) < LOOKAHEAD_BUF_SIZE) {
+      Op* new_op = alloc_op(ic->proc_id);
+      frontend_fetch_op(ic->proc_id, new_op);
+      Op** ptr = dl_list_add_tail(&op_buf);
+      *ptr = new_op;
+      if (new_op->table_info->cf_type)
+        max_runahead_uid = new_op->inst_uid;
+      max_runahead_op = new_op->op_num;
+      op_count[ic->proc_id]++;          /* increment instruction counters */
+      unique_count_per_core[ic->proc_id]++;
+      unique_count++;
+    }
+    Op **ptr = list_get_head(&op_buf);
+    ic->next_fetch_addr = (*ptr)->inst_info->addr;
+    runahead_pc = (*ptr)->inst_info->addr;
+    runahead_disable = FALSE;
+    if (FDIP_ENABLE)
+      fdip_update();
+  } else {
+    ic->next_fetch_addr = frontend_next_fetch_addr(ic->proc_id);
+  }
   ASSERT_PROC_ID_IN_ADDR(ic->proc_id, ic->next_fetch_addr)
 }
 
@@ -158,7 +198,7 @@ void init_icache_trace() {
 
 void reset_icache_stage() {
   uns ii;
-  for(ii = 0; ii < ISSUE_WIDTH; ii++)
+  for(ii = 0; ii < STAGE_MAX_OP_COUNT; ii++)
     ic->sd.ops[ii] = NULL;
   ic->sd.op_count = 0;
 
@@ -175,7 +215,7 @@ void reset_icache_stage() {
 // CMP used for bogus run: may be combined with reset_icache_stage
 void reset_all_ops_icache_stage() {
   uns ii;
-  for(ii = 0; ii < ISSUE_WIDTH; ii++)
+  for(ii = 0; ii < STAGE_MAX_OP_COUNT; ii++)
     ic->sd.ops[ii] = NULL;
   ic->sd.op_count = 0;
 
@@ -224,7 +264,7 @@ void recover_icache_stage() {
       ic->next_state = IC_FETCH;
     }
   }
-  op_count[ic->proc_id] = bp_recovery_info->recovery_op_num + 1;
+  op_count[ic->proc_id] = bp_recovery_info->recovery_op_num + 1 + LOOKAHEAD_BUF_SIZE;
   ic->next_fetch_addr   = bp_recovery_info->recovery_fetch_addr;
   if(ic->proc_id)
     ASSERT(ic->proc_id, ic->next_fetch_addr);
@@ -271,6 +311,35 @@ void debug_icache_stage() {
                  ic->sd.op_count);
 }
 
+/**************************************************************************************/
+/* in_icache: returns whether instr in icache 
+ *            Used for branch stat collection
+ */
+
+Flag in_icache(Addr addr) {
+  Addr line_addr;
+  return cache_access(&ic->icache, addr, &line_addr, FALSE) != NULL;
+}
+
+/**************************************************************************************/
+/* lookup_cache: returns instr if found in either uop cache or icache 
+ *                If icache miss but UC hit, set ic->line to non-null  
+ */
+Inst_Info** lookup_cache(Flag* uop_cache_fetch) {
+  Inst_Info** line = NULL;
+  line = (Inst_Info**)cache_access(&ic->icache, ic->fetch_addr,
+                                             &ic->line_addr, TRUE);
+  if(PERFECT_ICACHE && !line)
+    line = (Inst_Info**)INIT_CACHE_DATA_VALUE;
+  if (in_uop_cache(ic->fetch_addr, NULL, FALSE)) {
+    *uop_cache_fetch = TRUE;
+    // Should return Inst_Info, but op hasn't been fetched yet, so just give it any 
+    // value. This is not used anyway
+    if (!line) 
+      line = (Inst_Info**)DUMMY_ADDR_UC_FETCH;
+  }
+  return line;
+}
 
 /**************************************************************************************/
 /* icache_cycle: */
@@ -282,7 +351,7 @@ void update_icache_stage() {
 
   STAT_EVENT(ic->proc_id, ICACHE_CYCLE);
   STAT_EVENT(ic->proc_id, ICACHE_CYCLE_ONPATH + ic->off_path);
-  INC_STAT_EVENT(ic->proc_id, INST_LOST_TOTAL, ISSUE_WIDTH);
+  INC_STAT_EVENT(ic->proc_id, INST_LOST_TOTAL, IC_ISSUE_WIDTH);
 
   ic->state = ic->next_state;
 
@@ -290,13 +359,14 @@ void update_icache_stage() {
     STAT_EVENT(ic->proc_id, FETCH_0_OPS);
     INC_STAT_EVENT(ic->proc_id,
                    INST_LOST_FULL_WINDOW + inst_lost_get_full_window_reason(),
-                   ISSUE_WIDTH);
+                   IC_ISSUE_WIDTH);
     return;
   }
 
   switch(ic->state) {
     case IC_FETCH: {
       Break_Reason break_fetch = BREAK_DONT;
+      Flag uop_cache_fetch = FALSE;
 
       ic->off_path &= !ic->back_on_path;
       ic->back_on_path = FALSE;
@@ -316,11 +386,7 @@ void update_icache_stage() {
           ASSERTM(ic->proc_id, ic->fetch_addr, "ic fetch addr: %llu\n",
                   ic->fetch_addr);
 
-        ic->line = (Inst_Info**)cache_access(&ic->icache, ic->fetch_addr,
-                                             &ic->line_addr, TRUE);
-
-        if(PERFECT_ICACHE && !ic->line)
-          ic->line = INIT_CACHE_DATA_VALUE;
+        ic->line = lookup_cache(&uop_cache_fetch);
 
         if(WP_COLLECT_STATS)  // CMP remove?
           line_info = (Icache_Data*)cache_access(
@@ -331,9 +397,11 @@ void update_icache_stage() {
           ic->line = ic_pref_cache_access();
 
 
-        STAT_EVENT(ic->proc_id, POWER_ITLB_ACCESS);
-        STAT_EVENT(ic->proc_id, POWER_ICACHE_ACCESS);
-        STAT_EVENT(ic->proc_id, POWER_BTB_READ);
+        if (!uop_cache_fetch) {
+          STAT_EVENT(ic->proc_id, POWER_ITLB_ACCESS);
+          STAT_EVENT(ic->proc_id, POWER_ICACHE_ACCESS);
+          STAT_EVENT(ic->proc_id, POWER_BTB_READ);
+        }
 
         // ideal L2 Icache prefetcher
         if(IDEAL_L2_ICACHE_PREFETCHER) {
@@ -351,13 +419,14 @@ void update_icache_stage() {
             STAT_EVENT(ic->proc_id, L2_IDEAL_MISS_ICACHE);
         }
 
-        if(ic->line == NULL) { /* cache miss */
+        if(FDIP_ENABLE && last_issued_op_num == max_op_num && last_runahead_op != max_runahead_op) {
+          ic->next_state = IC_WAIT_FOR_FDIP;
+          break_fetch = BREAK_FDIP_RUNAHEAD;
+        } else if(ic->line == NULL) { /* cache miss */
           DEBUG(ic->proc_id, "Cache miss on op_num:%s @ 0x%s\n",
                 unsstr64(op_count[ic->proc_id]), hexstr64s(ic->fetch_addr));
 
-          STAT_EVENT(ic->proc_id, ICACHE_MISS);
-          STAT_EVENT(ic->proc_id, POWER_ICACHE_MISS);
-          STAT_EVENT(ic->proc_id, ICACHE_MISS_ONPATH + ic->off_path);
+          log_stats_ic_miss();
 
           /* if the icache is available, wait for a miss */
           /* otherwise, refetch next cycle */
@@ -365,10 +434,9 @@ void update_icache_stage() {
             if(ic->proc_id)
               ASSERTM(ic->proc_id, ic->line_addr, "ic fetch addr: %llu\n",
                       ic->fetch_addr);
-
             ASSERT_PROC_ID_IN_ADDR(ic->proc_id, ic->line_addr)
             if(new_mem_req(MRT_IFETCH, ic->proc_id, ic->line_addr,
-                           ICACHE_LINE_SIZE, 0, NULL, icache_fill_line,
+                           ICACHE_LINE_SIZE, 0, NULL, instr_fill_line,
                            unique_count,
                            0)) {  // CMP maybe unique_count_per_core[proc_id]?
               ic->next_state = IC_WAIT_FOR_MISS;
@@ -401,7 +469,16 @@ void update_icache_stage() {
             }
           }
           break_fetch = BREAK_ICACHE_MISS;
-        } else { /* cache hit */
+        } else if (ic->line == (Inst_Info**) DUMMY_ADDR_UC_FETCH) { // icache miss, uc hit
+          log_stats_ic_miss();
+          // start a memreq to fill icache, but do not cause any stalls. 
+          // Use for more inclusivity between IC and UC
+          new_mem_req(MRT_IFETCH, ic->proc_id, ic->line_addr,
+                           ICACHE_LINE_SIZE, 0, NULL, instr_fill_line,
+                           unique_count,
+                           0);
+          ic->next_state = icache_issue_ops(&break_fetch, &cf_num, ic->line, uop_cache_fetch);
+        } else { /* icache hit. Can be either UC hit or miss */
           DEBUG(ic->proc_id, "Cache hit on op_num:%s @ 0x%s \n",
                 unsstr64(op_count[ic->proc_id]), hexstr64s(ic->fetch_addr));
           STAT_EVENT(ic->proc_id, ICACHE_HIT);
@@ -410,40 +487,50 @@ void update_icache_stage() {
             ASSERT(ic->proc_id, line_info);
             wp_process_icache_hit(line_info, ic->fetch_addr);
           }
-          ic->next_state = icache_issue_ops(&break_fetch, &cf_num, ic->line);
+          ic->next_state = icache_issue_ops(&break_fetch, &cf_num, ic->line, uop_cache_fetch);
         }
       }
       INC_STAT_EVENT(ic->proc_id, INST_LOST_BREAK_DONT + break_fetch,
-                     ISSUE_WIDTH - ic->sd.op_count);
+                     IC_ISSUE_WIDTH > ic->sd.op_count ? IC_ISSUE_WIDTH - ic->sd.op_count
+                                                           : 0);
       STAT_EVENT(ic->proc_id, FETCH_0_OPS + ic->sd.op_count);
       STAT_EVENT(ic->proc_id, ST_BREAK_DONT + break_fetch);
     } break;
 
     case IC_WAIT_FOR_MISS: {
-      INC_STAT_EVENT(ic->proc_id, INST_LOST_BREAK_ICACHE_MISS, ISSUE_WIDTH);
+      INC_STAT_EVENT(ic->proc_id, INST_LOST_BREAK_ICACHE_MISS, IC_ISSUE_WIDTH - 1);
       STAT_EVENT(ic->proc_id, FETCH_0_OPS);
     } break;
 
     case IC_WAIT_FOR_REDIRECT: {
-      INC_STAT_EVENT(ic->proc_id, INST_LOST_WAIT_FOR_REDIRECT, ISSUE_WIDTH);
+      INC_STAT_EVENT(ic->proc_id, INST_LOST_WAIT_FOR_REDIRECT, IC_ISSUE_WIDTH);
       STAT_EVENT(ic->proc_id, FETCH_0_OPS);
     } break;
 
     case IC_WAIT_FOR_EMPTY_ROB: {
       DEBUG(ic->proc_id, "Ifetch barrier: Waiting for ROB to become empty \n");
-      INC_STAT_EVENT(ic->proc_id, INST_LOST_WAIT_FOR_EMPTY_ROB, ISSUE_WIDTH);
+      INC_STAT_EVENT(ic->proc_id, INST_LOST_WAIT_FOR_EMPTY_ROB, IC_ISSUE_WIDTH);
       STAT_EVENT(ic->proc_id, FETCH_0_OPS);
       if(td->seq_op_list.count == 0)
         ic->next_state = IC_FETCH;
     } break;
 
     case IC_WAIT_FOR_TIMER: {
-      INC_STAT_EVENT(ic->proc_id, INST_LOST_WAIT_FOR_TIMER, ISSUE_WIDTH);
+      INC_STAT_EVENT(ic->proc_id, INST_LOST_WAIT_FOR_TIMER, IC_ISSUE_WIDTH);
       STAT_EVENT(ic->proc_id, FETCH_0_OPS);
       if(cycle_count >= ic->timer_cycle)
         ic->next_state = IC_FETCH;
     } break;
 
+    case IC_WAIT_FOR_FDIP: {
+      INC_STAT_EVENT(ic->proc_id, INST_LOST_WAIT_FOR_FDIP, IC_ISSUE_WIDTH);
+      STAT_EVENT(ic->proc_id, FETCH_0_OPS);
+      if(last_issued_op_num < max_op_num)
+        ic->next_state = IC_FETCH;
+      if(last_runahead_op == max_runahead_op)
+        ic->next_state = IC_FETCH;
+    } break;
+
     default:
       FATAL_ERROR(ic->proc_id, "Invalid icache state.\n");
   }
@@ -461,7 +548,8 @@ void update_icache_stage() {
    becomes true. */
 
 static inline Icache_State icache_issue_ops(Break_Reason* break_fetch,
-                                            uns* cf_num, Inst_Info** line) {
+                                            uns* cf_num, Inst_Info** line, 
+                                            Flag uop_cache_issue_ops) {
   Packet_Build_Condition packet_break = PB_BREAK_DONT;
   static uns last_icache_issue_time = 0; /* for computing fetch break latency */
   static Counter issued_real_inst   = 0;
@@ -474,15 +562,35 @@ static inline Icache_State icache_issue_ops(Break_Reason* break_fetch,
   last_icache_issue_time = cycle_count;
 
   while(1) {
-    Op*        op   = alloc_op(ic->proc_id);
+    Op*        op = NULL;
     Inst_Info* inst = 0;
     UNUSED(inst);
 
-    if(frontend_can_fetch_op(ic->proc_id)) {
-      frontend_fetch_op(ic->proc_id, op);
-      ASSERTM(ic->proc_id, ic->next_fetch_addr == op->inst_info->addr,
-              "Fetch address 0x%llx does not match op address 0x%llx\n",
-              ic->next_fetch_addr, op->inst_info->addr);
+    if(frontend_can_fetch_op(ic->proc_id) ||
+        (!frontend_can_fetch_op(ic->proc_id) && LOOKAHEAD_BUF_SIZE && list_get_count(&op_buf))) {
+      if (LOOKAHEAD_BUF_SIZE) {
+        Op** ptr = NULL;
+        if (frontend_can_fetch_op(ic->proc_id)) {
+          Op* new_op = alloc_op(ic->proc_id);
+          frontend_fetch_op(ic->proc_id, new_op);
+          ptr = dl_list_add_tail(&op_buf);
+          *ptr = new_op;
+          if (new_op->table_info->cf_type)
+            max_runahead_uid = new_op->inst_uid;
+          max_runahead_op = new_op->op_num;
+        }
+        ptr = dl_list_remove_head(&op_buf);
+        op = *ptr;
+        last_issued_op_num = op->op_num;
+      }
+      else {
+        op   = alloc_op(ic->proc_id);
+        frontend_fetch_op(ic->proc_id, op);
+      }
+      // Disable this to allow system interrupts.
+      // ASSERTM(ic->proc_id, ic->next_fetch_addr == op->inst_info->addr,
+      //         "Fetch address 0x%llx does not match op address 0x%llx\n",
+      //         ic->next_fetch_addr, op->inst_info->addr);
       op->fetch_addr = ic->next_fetch_addr;
       ASSERT_PROC_ID_IN_ADDR(ic->proc_id, op->fetch_addr)
       op->off_path  = ic->off_path;
@@ -494,7 +602,6 @@ static inline Icache_State icache_issue_ops(Break_Reason* break_fetch,
       }
       inst = op->inst_info;
     } else {
-      free_op(op);
       *break_fetch = BREAK_BARRIER;
       return IC_FETCH;
     }
@@ -511,7 +618,7 @@ static inline Icache_State icache_issue_ops(Break_Reason* break_fetch,
       ASSERT(ic->proc_id, op->table_info->cf_type != CF_SYS);
     }
 
-    packet_break = packet_build(ic_pb_data, break_fetch, op, 0);
+    packet_break = packet_build(ic_pb_data, break_fetch, op, uop_cache_issue_ops);
     if(packet_break == PB_BREAK_BEFORE) {
       free_op(op);
       break;
@@ -567,30 +674,27 @@ static inline Icache_State icache_issue_ops(Break_Reason* break_fetch,
       if(*break_fetch == BREAK_BARRIER) {
         // for fetch barriers (including syscalls), we do not want to do
         // redirect/recovery, BUT we still want to update the branch predictor.
-	if (FDIP_ENABLE) {
-	  fdip_pred(ic->fetch_addr, op);
-	} else {
-	  Addr target = bp_predict_op(g_bp_data, op, (*cf_num)++, ic->fetch_addr);
-	  bp_predict_op_evaluate(g_bp_data, op, target);
-
-	  //bp_predict_op(g_bp_data, op, (*cf_num)++, ic->fetch_addr);
-	  op->oracle_info.mispred   = 0;
-	  op->oracle_info.misfetch  = 0;
-	  op->oracle_info.btb_miss  = 0;
-	  op->oracle_info.no_target = 0;
-	}
+        if (FDIP_ENABLE) {
+          fdip_pred(ic->fetch_addr, op);
+        } else {
+          bp_predict_op(g_bp_data, op, (*cf_num)++, ic->fetch_addr);
+
+          //bp_predict_op(g_bp_data, op, (*cf_num)++, ic->fetch_addr);
+          op->oracle_info.mispred   = 0;
+          op->oracle_info.misfetch  = 0;
+          op->oracle_info.btb_miss  = 0;
+          op->oracle_info.no_target = 0;
+        }
         ic->next_fetch_addr       = ADDR_PLUS_OFFSET(
           ic->next_fetch_addr, op->inst_info->trace_info.inst_size);
         ASSERT_PROC_ID_IN_ADDR(ic->proc_id, ic->next_fetch_addr)
       } else {
-        //
-	if (FDIP_ENABLE) {
-	  ic->next_fetch_addr = fdip_pred(ic->fetch_addr, op);
-	}
-	else {
-	  Addr target = bp_predict_op(g_bp_data, op, (*cf_num)++, ic->fetch_addr);
-	  ic->next_fetch_addr = bp_predict_op_evaluate(g_bp_data, op, target);
-	}
+        if (FDIP_ENABLE) {
+          ic->next_fetch_addr = fdip_pred(ic->fetch_addr, op);
+        }
+        else {
+          ic->next_fetch_addr = bp_predict_op(g_bp_data, op, (*cf_num)++, ic->fetch_addr);
+        }
 
         // initially bp_predict_op can return a garbage, for multi core run,
         // addr must follow cmp addr convention
@@ -607,6 +711,8 @@ static inline Icache_State icache_issue_ops(Break_Reason* break_fetch,
       const uns8 misfetch      = op->oracle_info.misfetch;
       const uns8 late_misfetch = op->oracle_info.late_misfetch;
 
+      inc_bstat_fetched(op);
+
       /* if it's a mispredict, kick the oracle off path */
       if(mispred || misfetch ||
          (USE_LATE_BP && (late_mispred || late_misfetch))) {
@@ -650,15 +756,18 @@ static inline Icache_State icache_issue_ops(Break_Reason* break_fetch,
         ///////////////////////////////////////
       }
 
-
+      
       /* if it's a btb miss, quit fetching and wait for redirect */
       if(op->oracle_info.btb_miss) {
         *break_fetch = BREAK_BTB_MISS;
         DEBUG(ic->proc_id, "Changed icache to wait for redirect %llu\n",
               cycle_count);
+        // During a resteer, do not perform off-path prefetching
+        if (FDIP_ENABLE)
+          runahead_disable = TRUE;
         return IC_WAIT_FOR_REDIRECT;
       }
-
+      
       /* if it's a taken branch, wait for timer */
       if(FETCH_BREAK_ON_TAKEN && op->oracle_info.pred &&
          *break_fetch != BREAK_BARRIER) {
@@ -749,7 +858,6 @@ Flag icache_fill_line(Mem_Req* req)  // cmp FIXME maybe needed to be optimized
       line_info = (Icache_Data*)cache_insert(&ic->icache_line_info, ic->proc_id,
                                              ic->fetch_addr, &dummy_addr2,
                                              &repl_line_addr2);
-
       line_info->fetched_by_offpath = USE_CONFIRMED_OFF ?
                                         req->off_path_confirmed :
                                         req->off_path;
@@ -757,7 +865,12 @@ Flag icache_fill_line(Mem_Req* req)  // cmp FIXME maybe needed to be optimized
       line_info->offpath_op_unique = req->oldest_op_unique_num;
       line_info->fetch_cycle       = cycle_count;
       line_info->onpath_use_cycle  = req->off_path ? 0 : cycle_count;
-      line_info->HW_prefetch       = (req->type == MRT_IPRF);
+      line_info->read_count[0]     = 0;
+      line_info->read_count[1]     = 0;
+      if (FDIP_ENABLE)
+        line_info->HW_prefetch     = (req->type == MRT_IFETCH);
+      else
+        line_info->HW_prefetch       = (req->type == MRT_IPRF);
       wp_process_icache_fill(line_info, req);
     }
 
@@ -796,7 +909,12 @@ Flag icache_fill_line(Mem_Req* req)  // cmp FIXME maybe needed to be optimized
       line_info->offpath_op_unique = req->oldest_op_unique_num;
       line_info->fetch_cycle       = cycle_count;
       line_info->onpath_use_cycle  = req->off_path ? 0 : cycle_count;
-      line_info->HW_prefetch       = (req->type == MRT_IPRF);
+      line_info->read_count[0]     = 0;
+      line_info->read_count[1]     = 0;
+      if (FDIP_ENABLE)
+        line_info->HW_prefetch     = (req->type == MRT_IFETCH);
+      else
+        line_info->HW_prefetch       = (req->type == MRT_IPRF);
       wp_process_icache_fill(line_info, req);
     }
 
@@ -950,3 +1068,243 @@ int32_t inst_lost_get_full_window_reason() {
 
   return 0;
 }
+
+Op* find_op(Addr pc) {
+  Op* op;
+  Op** op_p = (Op**)list_get_current(&op_buf);
+
+  if (op_p)
+    op = *op_p;
+
+  if (!op_p) {
+    op_p = (Op**)list_start_head_traversal(&op_buf);
+    op = *op_p;
+    if (last_runahead_op && op->op_num <= last_runahead_op) {
+      for(; op_p; op_p = (Op**)list_next_element(&op_buf)) {
+        op = *op_p;
+        if (op->op_num == last_runahead_op) {
+          op_p = (Op**)list_next_element(&op_buf);
+          op = *op_p;
+          break;
+        }
+      }
+    }
+  }
+
+  for(; op_p; op_p = (Op**)list_next_element(&op_buf)) {
+    op = *op_p;
+    if (op->table_info->cf_type) { // first branch after the last predicted branch
+      if (op->fetch_addr == pc) {
+        last_runahead_uid = op->inst_uid;
+        op_p = (Op**)list_next_element(&op_buf);
+        last_runahead_op = op->op_num;
+        return op;
+      } else {
+        Op** op_p_head = (Op**)list_get_head(&op_buf);
+        if (op_p == op_p_head)
+          break;
+        op_p = (Op**)list_prev_element(&op_buf);
+        op = *op_p;
+        last_runahead_op = op->op_num;
+        op_p = (Op**)list_next_element(&op_buf);
+        op = *op_p;
+        break;
+      }
+    }
+  }
+  if (!op_p) {
+    op_p = (Op**)list_get_tail(&op_buf);
+    op = *op_p;
+    last_runahead_op = op->op_num;
+  }
+  return NULL;
+}
+
+void set_max_op_num(Flag is_branch, Addr last_cl_prefetched) {
+  Counter move_cnt = 0;
+  Op* op;
+  Op** op_p;
+  if (max_runahead_op == last_runahead_op) {
+    op_p = (Op**)list_get_tail(&op_buf);
+    ASSERT(ic->proc_id, op_p);
+    op = *op_p;
+    max_op_num = op->op_num;
+  } else if (is_branch) {
+    op_p = (Op**)list_prev_element(&op_buf);
+    move_cnt++;
+    ASSERT(ic->proc_id, op_p);
+    op = *op_p;
+    ASSERT(ic->proc_id, op->table_info->cf_type);
+    max_op_num = op->op_num;
+  } else if (!is_branch) {
+    op_p = (Op**)list_get_current(&op_buf);
+    ASSERT(ic->proc_id, op_p);
+    op = *op_p;
+    ASSERT(ic->proc_id, op->table_info->cf_type);
+    Op** op_head = (Op**)list_get_head(&op_buf);
+    if (op_head == op_p)
+      return;
+    op_p = (Op**)list_prev_element(&op_buf);
+    move_cnt++;
+    op = *op_p;
+    while (op) {
+      Addr cl_addr = get_cache_line_addr(&ic->icache, op->fetch_addr);
+      if (op->table_info->cf_type)
+        break;
+      if (cl_addr == last_cl_prefetched) {
+        max_op_num = op->op_num;
+        break;
+      }
+      if (op_head != op_p) {
+        op_p = (Op**)list_prev_element(&op_buf);
+        move_cnt++;
+        op = *op_p;
+      } else {
+        max_op_num = op->op_num;
+        break;
+      }
+    }
+  }
+
+  // restore the current pointer of the buffer
+  while (move_cnt) {
+    list_next_element(&op_buf);
+    move_cnt--;
+  }
+}
+
+void move_to_prev_op(void) {
+  list_prev_element(&op_buf);
+}
+
+Flag will_be_accessed(Addr pc) {
+  Op* op;
+  Op** op_p = (Op**)list_get_current(&op_buf);
+  uns64 save_inst_uid = 0;
+  Addr save_pc = 0;
+  Flag found = FALSE;
+
+  if (op_p) {
+    op = *op_p;
+    save_inst_uid = op->inst_uid;
+    save_pc = op->fetch_addr;
+    op_p = (Op**)list_next_element(&op_buf);
+    op = *op_p;
+  }
+
+  if (!op_p) {
+    op_p = (Op**)list_start_head_traversal(&op_buf);
+    op = *op_p;
+    if (last_runahead_uid && op->inst_uid <= last_runahead_uid) {
+      for(; op_p; op_p = (Op**)list_next_element(&op_buf)) {
+        op = *op_p;
+        if (op->table_info->cf_type && op->inst_uid == last_runahead_uid) {
+          op_p = (Op**)list_next_element(&op_buf);
+          op = *op_p;
+          break;
+        }
+      }
+    }
+  }
+
+  // find the instruction among all the later ops in the lookahead buffer
+  for(; op_p; op_p = (Op**)list_next_element(&op_buf)) {
+    op = *op_p;
+    if (op->fetch_addr == pc) {
+      found = TRUE;
+      break;
+    }
+  }
+
+  // recover the current pointer for the future find_op
+  if (save_inst_uid) {
+    op_p = (Op**)list_start_head_traversal(&op_buf);
+    for(; op_p; op_p = (Op**)list_next_element(&op_buf)) {
+      op = *op_p;
+      if (op->inst_uid == save_inst_uid && op->fetch_addr == save_pc)
+        break;
+    }
+  } else {
+    (&op_buf)->current = NULL;
+  }
+
+  return found;
+}
+
+// Retrieve the prediction window by looking into the lookahead buffer.
+// This should only be called when on-path because if FDIP is off-path, nothing will be found in a trace. 
+Uop_Cache_Data get_pw_lookahead_buffer(Addr start_addr) {
+  Uop_Cache_Data pw;
+  memset(&pw, 0, sizeof(pw));
+
+
+  // It is not always true that the branch for which I am prefetching target
+  // is still in lookahead buffer - see FDIP_PRED_FTQ_EMPTY
+
+  List_Entry* cur = op_buf.current;
+  Op** op_p = (Op**)list_get_current(&op_buf);
+
+  // Move the op pointer to the op just after the last runahead branch.
+  // If the branch was just-consumed this is the first element in the buffer.
+  // This is the branch's correct target.
+  if (op_p) {
+    ASSERT(ic->proc_id, (*op_p)->table_info->cf_type);
+    op_p = (Op**)list_next_element(&op_buf);
+  } else {
+    op_p = (Op**)list_start_head_traversal(&op_buf);
+    if (last_runahead_uid && (*op_p)->inst_uid <= last_runahead_uid) {
+      for(; op_p; op_p = (Op**)list_next_element(&op_buf)) {
+        if ((*op_p)->table_info->cf_type && (*op_p)->inst_uid == last_runahead_uid) {
+          op_p = (Op**)list_next_element(&op_buf);
+          break;
+        }
+      }
+    }
+  }
+
+  // ASSERT that the op identified matches the target/runahead_pc
+  ASSERT(ic->proc_id, (*op_p)->inst_info->addr == start_addr);
+  pw.first = start_addr;
+
+  // Next move down runahead buffer, incrementing n_uops until a branch is found.
+  for(; op_p; op_p = (Op**)list_next_element(&op_buf)) {
+    pw.n_uops++;
+    pw.last = (*op_p)->inst_info->addr;
+    if ((*op_p)->table_info->cf_type) { // first branch after the last predicted branch, end of pw
+      break;
+    }
+  }
+
+  // Reset current to prevent side effects
+  op_buf.current = cur;
+  ASSERT(ic->proc_id, (*op_p)->table_info->cf_type);
+  ASSERT(ic->proc_id, pw.n_uops);
+
+  return pw;
+}
+
+void log_stats_ic_miss() {
+  STAT_EVENT(ic->proc_id, ICACHE_MISS);
+  STAT_EVENT(ic->proc_id, POWER_ICACHE_MISS);
+  STAT_EVENT(ic->proc_id, ICACHE_MISS_ONPATH + ic->off_path);
+}
+
+// Wrapper callback for any instruction memreq.
+// This must always return TRUE so that memreq is satisfied that
+// done_func is finished and does not need to be retried.
+// (uop_cache_fill_prefetch will fail for never-seen PWs on the off-path).
+Flag instr_fill_line(Mem_Req* req) {
+  ASSERT(ic->proc_id, req->type == MRT_IPRF || req->type == MRT_FDIPPRF || req->type == MRT_UOCPRF || req->type == MRT_IFETCH);
+
+  if (mem_req_is_type(req, MRT_IPRF) || mem_req_is_type(req, MRT_IFETCH) || mem_req_is_type(req, MRT_FDIPPRF)) {
+    icache_fill_line(req);
+  }
+  if (mem_req_is_type(req, MRT_FDIPPRF)) {
+    fdip_dec_outstanding_prefs(req->addr, req->off_path, req->emitted_cycle);
+    // TODO: comment out the line above and uncomment the line below to swith 1-outstanding prefetch counter.
+    /*fdip_dec_outstanding_prefs(req->addr);*/
+  }
+  if (mem_req_is_type(req, MRT_UOCPRF))
+    uop_cache_fill_prefetch(req->addr, !req->off_path);
+  return TRUE;
+}
diff --git a/src/icache_stage.h b/src/icache_stage.h
index 98dda97..4e97e62 100644
--- a/src/icache_stage.h
+++ b/src/icache_stage.h
@@ -33,6 +33,9 @@
 #include "libs/cache_lib.h"
 #include "stage_data.h"
 
+#define IC_ISSUE_WIDTH      ISSUE_WIDTH
+#define UC_ISSUE_WIDTH      ISSUE_WIDTH + UOP_CACHE_ADDITIONAL_ISSUE_BANDWIDTH
+
 /**************************************************************************************/
 /* Forward Declarations */
 
@@ -52,6 +55,7 @@ typedef enum Icache_State_enum {
   IC_WAIT_FOR_REDIRECT,
   IC_WAIT_FOR_EMPTY_ROB,
   IC_WAIT_FOR_TIMER,
+  IC_WAIT_FOR_FDIP,
 } Icache_State;
 
 typedef struct Icache_Stage_struct {
@@ -128,6 +132,16 @@ Flag icache_fill_line(Mem_Req*);
 void wp_process_icache_hit(Icache_Data* line, Addr fetch_addr);
 void wp_process_icache_fill(Icache_Data* line, Mem_Req* req);
 Flag icache_off_path(void);
+Op* find_op(Addr pc);
+void set_max_op_num(Flag is_branch, Addr last_cl_prefetched);
+void move_to_prev_op(void);
+Flag will_be_accessed(Addr pc);
+Flag instr_fill_line(Mem_Req* req);
+Uop_Cache_Data get_pw_lookahead_buffer(Addr start_addr);
+Flag instr_fill_line(Mem_Req* req);
+
+// For branch stat collection
+Flag in_icache(Addr addr);
 
 /**************************************************************************************/
 
diff --git a/src/libs/cache_lib.c b/src/libs/cache_lib.c
index 0a66f54..cea059f 100644
--- a/src/libs/cache_lib.c
+++ b/src/libs/cache_lib.c
@@ -252,6 +252,47 @@ void* cache_access(Cache* cache, Addr addr, Addr* line_addr, Flag update_repl) {
   return NULL;
 }
 
+/**************************************************************************************/
+/* cache_access: Does a cache lookup based on the address in order to update repl
+ * Needed for uop cache, where a single PW entry can span multiple lines.
+ * Touch/update update_repl for ALL lines containing this PW
+ */
+
+int cache_access_all(Cache* cache, Addr addr, Addr* line_addr, Flag update_repl, void** line_data) {
+  Addr tag;
+  uns  set = cache_index(cache, addr, &tag, line_addr);
+  uns  ii;
+  int lines_found = 0;
+
+  for(ii = 0; ii < cache->assoc; ii++) {
+    Cache_Entry* line = &cache->entries[set][ii];
+
+    if(line->valid && line->tag == tag && line->pw_start_addr == addr) {
+      /* update replacement state if necessary */
+      ASSERT(0, line->data);
+      DEBUG(0, "Found line in cache '%s' at (set %u, way %u, base 0x%s)\n",
+            cache->name, set, ii, hexstr64s(line->base));
+
+      if(update_repl) {
+        if(line->pref) {
+          line->pref = FALSE;
+        }
+        cache->num_demand_access++;
+        update_repl_policy(cache, line, set, ii, FALSE);
+      }
+      *line_data = line->data;
+      lines_found++;
+    }
+  }
+  
+  if (lines_found == 0) {
+    DEBUG(0, "Didn't find line in set %u in cache '%s' base 0x%s\n", set,
+        cache->name, hexstr64s(addr));
+  }
+
+  return lines_found;
+}
+ 
 /**************************************************************************************/
 /* cache_insert: returns a pointer to the data section of the new cache line.
    Sets line_addr to the address of the first block of the new line.  Sets
@@ -311,9 +352,10 @@ void* cache_insert_replpos(Cache* cache, uns8 proc_id, Addr addr,
   new_line->base             = *line_addr;
   new_line->last_access_time = sim_time;  // FIXME: this fixes valgrind warnings
                                           // in update_prf_
-
   new_line->pref = isPrefetch;
 
+  new_line->pw_start_addr = addr; // only means anything for uop cache
+
   switch(insert_repl_policy) {
     case INSERT_REPL_DEFAULT:
       update_repl_policy(cache, new_line, set, repl_index, TRUE);
@@ -406,8 +448,7 @@ void* cache_insert_replpos(Cache* cache, uns8 proc_id, Addr addr,
 
 
 /**************************************************************************************/
-/* invalidate_line: Does a cache lookup based on the address.  Returns a pointer
-   to the cache line data if it is found.  */
+/* invalidate_line: Invalidates based on the address.  */
 
 void cache_invalidate(Cache* cache, Addr addr, Addr* line_addr) {
   Addr tag;
diff --git a/src/libs/cache_lib.h b/src/libs/cache_lib.h
index 840ea2b..84f5829 100644
--- a/src/libs/cache_lib.h
+++ b/src/libs/cache_lib.h
@@ -66,8 +66,9 @@ typedef struct Cache_Entry_struct {
   Counter insertion_time;   /* for replacement policy */
   void*   data;             /* pointer to arbitrary data */
   Flag    pref;             /* extra replacement info */
-  Flag dirty; /* Dirty bit should have been here, however this is used only in
+  Flag    dirty; /* Dirty bit should have been here, however this is used only in
                  warmup now */
+  Addr pw_start_addr; /* for uop cache: start addr of prediction window */
 } Cache_Entry;
 
 // DO NOT CHANGE THIS ORDER
@@ -126,6 +127,7 @@ typedef struct Cache_struct {
 
 void  init_cache(Cache*, const char*, uns, uns, uns, uns, Repl_Policy);
 void* cache_access(Cache*, Addr, Addr*, Flag);
+int   cache_access_all(Cache*, Addr, Addr*, Flag, void**);
 void* cache_insert(Cache*, uns8, Addr, Addr*, Addr*);
 void* cache_insert_replpos(Cache* cache, uns8 proc_id, Addr addr,
                            Addr* line_addr, Addr* repl_line_addr,
diff --git a/src/libs/list_lib.c b/src/libs/list_lib.c
index a26b49c..6539033 100644
--- a/src/libs/list_lib.c
+++ b/src/libs/list_lib.c
@@ -306,8 +306,12 @@ void* sl_list_remove_head(List* list) {
     if(list->tail == list->head) {
       list->head = NULL;
       list->tail = NULL;
-    } else
+      list->current = NULL;
+    } else {
+      if (list->head == list->current)
+        list->current = NULL;
       list->head = list->head->next;
+    }
     free_list_entry(list, free);
   } else {
     verify_list_counts(list);
@@ -335,7 +339,10 @@ void* dl_list_remove_head(List* list) {
     if(list->tail == list->head) {
       list->head = NULL;
       list->tail = NULL;
+      list->current = NULL;
     } else {
+      if (list->head == list->current)
+        list->current = NULL;
       list->head       = list->head->next;
       list->head->prev = NULL;
     }
@@ -365,7 +372,10 @@ void* dl_list_remove_tail(List* list) {
     if(list->tail == list->head) {
       list->head = NULL;
       list->tail = NULL;
+      list->current = NULL;
     } else {
+      if (list->tail == list->current)
+        list->current = NULL;
       list->tail       = list->tail->prev;
       list->tail->next = NULL;
     }
diff --git a/src/map_stage.c b/src/map_stage.c
index 17b18a5..cd93c2d 100644
--- a/src/map_stage.c
+++ b/src/map_stage.c
@@ -50,7 +50,7 @@
 /* Macros */
 
 #define DEBUG(proc_id, args...) _DEBUG(proc_id, DEBUG_MAP_STAGE, ##args)
-#define STAGE_MAX_OP_COUNT ISSUE_WIDTH
+#define STAGE_MAX_OP_COUNT ISSUE_WIDTH + UOP_CACHE_ADDITIONAL_ISSUE_BANDWIDTH
 #define STAGE_MAX_DEPTH MAP_CYCLES
 
 
diff --git a/src/memory/mem_req.c b/src/memory/mem_req.c
index ad01ea5..118601c 100644
--- a/src/memory/mem_req.c
+++ b/src/memory/mem_req.c
@@ -56,7 +56,7 @@ Flag mem_req_type_is_demand(Mem_Req_Type type) {
 /* mem_req_type_is_prefetch */
 
 Flag mem_req_type_is_prefetch(Mem_Req_Type type) {
-  return type == MRT_IPRF || type == MRT_DPRF;
+  return type == MRT_IPRF || type == MRT_DPRF || type == MRT_UOCPRF || type == MRT_FDIPPRF;
 }
 
 /**************************************************************************************/
@@ -66,3 +66,15 @@ Flag mem_req_type_is_stalling(Mem_Req_Type type) {
   return type == MRT_IFETCH || type == MRT_DFETCH ||
          (!STORES_DO_NOT_BLOCK_WINDOW && type == MRT_DSTORE);
 }
+
+Flag mem_req_is_type(Mem_Req* req, Mem_Req_Type type) {
+  return TESTBIT(req->types, type);
+}
+
+void mem_req_set_types(Mem_Req* req, Mem_Req_Type type) {
+  SETBIT(req->types, type);
+}
+
+void mem_req_clr_types(Mem_Req* req, Mem_Req_Type type) {
+  CLRBIT(req->types, type);
+}
\ No newline at end of file
diff --git a/src/memory/mem_req.h b/src/memory/mem_req.h
index 95478a4..1c6f0da 100644
--- a/src/memory/mem_req.h
+++ b/src/memory/mem_req.h
@@ -68,6 +68,8 @@ typedef enum Mem_Req_State_enum {
     elem(DFETCH)       /* data fetch */              \
     elem(DSTORE)       /* data store */              \
     elem(IPRF)         /* instruction prefetch */    \
+    elem(UOCPRF)       /* only uop cache prefetch */ \
+    elem(FDIPPRF)      /* FDIP instruction prefetch */ \
     elem(DPRF)         /* data prefetch */           \
     elem(WB)           /* writeback of dirty data */ \
     elem(WB_NODIRTY)   /* writeback of clean data */ \
@@ -102,6 +104,9 @@ struct Mem_Req_struct {
                               set after the branch resolves */
   Mem_Req_State            state;    /* what state is the miss in? */
   Mem_Req_Type             type;     /* what kind of miss is it? */
+  /* Bit string recording all Mem_Req_Type(s) that were coalesced into this request. */
+  uns                      types;
+  Counter                  emitted_cycle; /* cycle when the request is emitted. */
   struct Mem_Queue_struct* queue;    /* Pointer to the queue this entry is in */
   Counter                  priority; /* priority of the miss */
   Addr                     addr;     /* address to fetch */
@@ -194,6 +199,11 @@ Flag mem_req_type_is_demand(Mem_Req_Type type);
 Flag mem_req_type_is_prefetch(Mem_Req_Type type);
 Flag mem_req_type_is_stalling(Mem_Req_Type type);
 
+/* Returns whether this type either started or was ever coalesced into this mem_req. */
+Flag mem_req_is_type(Mem_Req* req, Mem_Req_Type type);
+void mem_req_set_types(Mem_Req* req, Mem_Req_Type type);
+void mem_req_clr_types(Mem_Req* req, Mem_Req_Type type);
+
 /**************************************************************************************/
 /* Externs */
 
diff --git a/src/memory/memory.c b/src/memory/memory.c
index 2c1699e..19f71ff 100644
--- a/src/memory/memory.c
+++ b/src/memory/memory.c
@@ -56,6 +56,7 @@
 #include "prefetcher/pref.param.h"
 #include "prefetcher/pref_common.h"
 #include "prefetcher/stream_pref.h"
+#include "prefetcher/fdip.h"
 #include "statistics.h"
 //#include "dram.h"
 //#include "dram.param.h"
@@ -94,6 +95,7 @@ static uns      mem_req_wb_entries     = 0;
 
 Memory*              mem = NULL;
 extern Icache_Stage* ic;
+extern Counter  last_recover_cycle;
 
 Counter Mem_Req_Priority[MRT_NUM_ELEMS];
 Counter Mem_Req_Priority_Offset[MRT_NUM_ELEMS];
@@ -263,6 +265,12 @@ void init_mem_req_type_priorities() {
       case MRT_DPRF:
         priority = MEM_PRIORITY_DPRF;
         break;
+      case MRT_UOCPRF:
+        priority = MEM_PRIORITY_UOCPRF;
+        break;
+      case MRT_FDIPPRF:
+        priority = MEM_PRIORITY_FDIPPRF;
+        break;
       case MRT_WB:
         priority = MEM_PRIORITY_WB;
         break;
@@ -1523,7 +1531,7 @@ static Flag mem_complete_l1_access(Mem_Req*         req,
   }
 
   if(!PREFETCH_UPDATE_LRU_L1 &&
-     (req->type == MRT_DPRF || req->type == MRT_IPRF))
+     (req->type == MRT_DPRF || req->type == MRT_IPRF || req->type == MRT_UOCPRF || req->type == MRT_FDIPPRF))
     update_l1_lru = FALSE;
   data = (L1_Data*)cache_access(&L1(req->proc_id)->cache, req->addr, &line_addr,
                                 update_l1_lru);  // access L2
@@ -1532,7 +1540,7 @@ static Flag mem_complete_l1_access(Mem_Req*         req,
     data = NULL;
 
   // cmp FIXME prefetchers
-  if((req->type == MRT_DPRF || req->type == MRT_IPRF ||
+  if((req->type == MRT_DPRF || req->type == MRT_IPRF || req->type == MRT_UOCPRF || req->type == MRT_FDIPPRF ||
       req->demand_match_prefetch) &&
      req->prefetcher_id != 0) {
     STAT_EVENT(req->proc_id, L1_PREF_ACCESS);
@@ -1679,7 +1687,7 @@ static Flag mem_complete_l1_access(Mem_Req*         req,
           ASSERTM(0,
                   req->type == MRT_DSTORE || req->type == MRT_IFETCH ||
                     req->type == MRT_DFETCH || req->type == MRT_IPRF ||
-                    req->type == MRT_DPRF,
+                    req->type == MRT_DPRF || req->type == MRT_UOCPRF || req->type == MRT_FDIPPRF,
                   "ERROR: Issuing a currently unhandled request type (%s) to "
                   "Ramulator\n",
                   Mem_Req_Type_str(req->type));
@@ -1705,7 +1713,7 @@ static Flag mem_complete_l1_access(Mem_Req*         req,
       }
 
       // cmp FIXME prefetchers
-      if((req->type == MRT_DPRF || req->type == MRT_IPRF ||
+      if((req->type == MRT_DPRF || req->type == MRT_IPRF || req->type == MRT_UOCPRF || req->type == MRT_FDIPPRF ||
           req->demand_match_prefetch) &&
          req->prefetcher_id !=
            0) {  // cmp FIXME What can I do for the prefetcher?
@@ -1743,7 +1751,7 @@ static Flag mem_complete_mlc_access(Mem_Req*         req,
   Flag      update_mlc_lru = TRUE;
 
   if(!PREFETCH_UPDATE_LRU_MLC &&
-     (req->type == MRT_DPRF || req->type == MRT_IPRF))
+     (req->type == MRT_DPRF || req->type == MRT_IPRF || req->type == MRT_UOCPRF || req->type == MRT_FDIPPRF))
     update_mlc_lru = FALSE;
   data = (MLC_Data*)cache_access(&MLC(req->proc_id)->cache, req->addr,
                                  &line_addr, update_mlc_lru);  // access MLC
@@ -1863,7 +1871,7 @@ static void mem_process_l1_reqs() {
     if(req->state == MRS_L1_NEW) {
       mem_start_l1_access(req);
       STAT_EVENT(req->proc_id, L1_ACCESS);
-      if(req->type == MRT_DPRF || req->type == MRT_IPRF)
+      if(req->type == MRT_DPRF || req->type == MRT_IPRF || req->type == MRT_UOCPRF || req->type == MRT_FDIPPRF)
         STAT_EVENT(req->proc_id, L1_PREF_ACCESS);
       else
         STAT_EVENT(req->proc_id, L1_DEMAND_ACCESS);
@@ -1955,7 +1963,7 @@ static void mem_process_mlc_reqs() {
     if(req->state == MRS_MLC_NEW) {
       mem_start_mlc_access(req);
       STAT_EVENT(req->proc_id, MLC_ACCESS);
-      if(req->type == MRT_DPRF || req->type == MRT_IPRF)
+      if(req->type == MRT_DPRF || req->type == MRT_IPRF || req->type == MRT_UOCPRF || req->type == MRT_FDIPPRF)
         STAT_EVENT(req->proc_id, MLC_PREF_ACCESS);
       else
         STAT_EVENT(req->proc_id, MLC_DEMAND_ACCESS);
@@ -2365,7 +2373,7 @@ void mem_complete_bus_in_access(Mem_Req* req, Counter priority) {
     // this stat
     INC_STAT_EVENT(req->proc_id, CORE_MEM_LATENCY_IFETCH + req->type,
                    req->rdy_cycle - req->mem_queue_cycle);
-    if(req->type != MRT_DPRF && req->type != MRT_IPRF &&
+    if(req->type != MRT_DPRF && req->type != MRT_IPRF && req->type != MRT_UOCPRF && req->type != MRT_FDIPPRF &&
        !req->demand_match_prefetch) {
       INC_STAT_EVENT_ALL(TOTAL_MEM_LATENCY_DEMAND,
                          req->rdy_cycle - req->mem_queue_cycle);
@@ -2700,7 +2708,7 @@ static inline Mem_Req* mem_search_queue(
       } else {
         switch(req->type) {
           case MRT_IFETCH:
-            if(type == MRT_IPRF)
+            if(type == MRT_IPRF || type == MRT_UOCPRF || type == MRT_FDIPPRF)
               match = TRUE;
             if(collect_stats && ((type == MRT_WB) || (type == MRT_WB_NODIRTY)))
               STAT_EVENT(req->proc_id, WB_MATCH_DEMAND);
@@ -2721,10 +2729,28 @@ static inline Mem_Req* mem_search_queue(
             if(type == MRT_IFETCH) {
               match                = TRUE;
               *demand_hit_prefetch = TRUE;
+            } else if (type == MRT_UOCPRF || type == MRT_FDIPPRF) {
+              match = TRUE;
             }
             if(collect_stats && ((type == MRT_WB) || (type == MRT_WB_NODIRTY)))
               STAT_EVENT(req->proc_id, WB_MATCH_PREF);
             break;
+          case MRT_UOCPRF:
+            if (type == MRT_IFETCH) {
+              match                = TRUE;
+              *demand_hit_prefetch = TRUE;
+            } else if (type == MRT_IPRF) {
+              match = TRUE;
+            }
+            break;
+          case MRT_FDIPPRF:
+            if (type == MRT_IFETCH) {
+              match                = TRUE;
+              *demand_hit_prefetch = TRUE;
+            } else if (type == MRT_IPRF) {
+              match = TRUE;
+            }
+            break;
           case MRT_DPRF:
             if((type == MRT_DFETCH) || (type == MRT_DSTORE)) {
               match                = TRUE;
@@ -2861,6 +2887,10 @@ Flag mem_adjust_matching_request(Mem_Req* req, Mem_Req_Type type, Addr addr,
   higher_priority = current_priority < old_priority;
 
   STAT_EVENT(req->proc_id, MEM_REQ_BUFFER_HIT);
+  if (req->type == MRT_FDIPPRF && type == MRT_IFETCH) {
+    STAT_EVENT(req->proc_id, MEM_REQ_FDIP_BUFFER_HIT);
+    INC_STAT_EVENT(req->proc_id, MEM_REQ_FDIP_CYCLE_DELTA, cycle_count - req->emitted_cycle);
+  }
   wp_process_reqbuf_match(req, op);
 
   if(ALLOW_TYPE_MATCHES && demand_hit_writeback) {
@@ -2895,6 +2925,11 @@ Flag mem_adjust_matching_request(Mem_Req* req, Mem_Req_Type type, Addr addr,
 
   Flag old_off_path_confirmed = req->off_path_confirmed;
   Flag old_type               = req->type;
+  Flag old_off_path           = req->off_path;
+  Flag type_added             = FALSE;
+
+  mem_req_set_types(req, type);  // Update record of types that merged into this memreq
+  type_added                  = TRUE;
 
   /* Adjust op related fields in the request */
   if(op) {
@@ -3054,6 +3089,25 @@ Flag mem_adjust_matching_request(Mem_Req* req, Mem_Req_Type type, Addr addr,
     req->off_path_confirmed = FALSE;
   }
 
+  // If the original type has higher priority (IFETCH) and the new req has lower priority (FDIP), the new req type bit should be cleared if two requests are on the different path (one on path and one off path).
+  if(req->type == MRT_IFETCH && type == MRT_FDIPPRF) {
+    if ((!req->off_path && fdip_pref_off_path()) ||
+        (req->off_path && !fdip_pref_off_path())) {
+      mem_req_clr_types(req, MRT_FDIPPRF);
+      type_added = FALSE;
+    }
+  }
+
+  Flag off_path_changed = FALSE;
+  // If the mem requests for the same FDIP type are on the different paths, give priority to the on-path one.
+  if(req->off_path &&
+      req->type == MRT_FDIPPRF && type == MRT_FDIPPRF &&
+      !fdip_pref_off_path()) {
+    req->off_path           = FALSE;
+    req->off_path_confirmed = FALSE;
+    off_path_changed        = TRUE;
+  }
+
   update_mem_req_occupancy_counter(old_type, -1);
   update_mem_req_occupancy_counter(
     req->type, +1);  // BUG? req->type does not always match type
@@ -3062,7 +3116,7 @@ Flag mem_adjust_matching_request(Mem_Req* req, Mem_Req_Type type, Addr addr,
   // in case a demand matches an L2 prefetch, for example
   req->destination = MIN2(req->destination, destination);
 
-  if((old_type == MRT_DPRF || old_type == MRT_IPRF) &&
+  if((old_type == MRT_DPRF || old_type == MRT_IPRF || old_type == MRT_UOCPRF || old_type == MRT_FDIPPRF) &&
      (type == MRT_IFETCH || type == MRT_DFETCH || type == MRT_DSTORE) &&
      req->l1_miss && req->state <= MRS_FILL_L1) {
     perf_pred_off_chip_effect_start(req);
@@ -3079,14 +3133,30 @@ Flag mem_adjust_matching_request(Mem_Req* req, Mem_Req_Type type, Addr addr,
   }
 
   req->req_count++;
-  return TRUE;
+  if (old_type != type && !type_added) {
+    return SUCCESS_DIFF_TYPE;
+  }
+  else if (old_type != type && type_added) {
+    req->emitted_cycle = cycle_count;
+    return SUCCESS_DIFF_TYPE_ADDED;
+  }
+  else if (old_type == type && !off_path_changed) {
+    return SUCCESS_SAME_TYPE;
+  }
+  else if (old_type == type && off_path_changed) {
+    req->emitted_cycle = cycle_count;
+    if (req->emitted_cycle < last_recover_cycle)
+      return SUCCESS_SAME_TYPE_INVALID_OFF_PATH_CHANGED;
+    else
+      return SUCCESS_SAME_TYPE_VALID_OFF_PATH_CHANGED;
+  }
 }
 
 /**************************************************************************************/
 /* mem_can_allocate_req_buffer: */
 
 Flag mem_can_allocate_req_buffer(uns proc_id, Mem_Req_Type type) {
-  if(type == MRT_IPRF || type == MRT_DPRF) {
+  if(type == MRT_IPRF || type == MRT_DPRF || type == MRT_UOCPRF || type == MRT_FDIPPRF) {
     if(PRIVATE_MSHR_ON &&
        mem->num_req_buffers_per_core[proc_id] + MEM_REQ_BUFFER_PREF_WATERMARK >=
          MEM_REQ_BUFFER_ENTRIES) {
@@ -3168,7 +3238,7 @@ static Mem_Req* mem_kick_out_prefetch_from_queue(uns mem_bank, Mem_Queue* queue,
     if(KICKOUT_OLDEST_PREFETCH_WITHIN_BANK) {
       for(ii = 0; ii < queue->entry_count; ii++) {
         Mem_Req* req = &(mem->req_buffer[queue->base[ii].reqbuf]);
-        if(req->type != MRT_IPRF && req->type != MRT_DPRF)
+        if(req->type != MRT_IPRF && req->type != MRT_DPRF && req->type != MRT_UOCPRF && req->type != MRT_FDIPPRF)
           continue;
         if(oldest_req_age > req->start_cycle &&
            mem_bank == req->mem_flat_bank) {
@@ -3186,7 +3256,7 @@ static Mem_Req* mem_kick_out_prefetch_from_queue(uns mem_bank, Mem_Queue* queue,
       // Search for the oldest prefetch
       for(ii = 0; ii < queue->entry_count; ii++) {
         Mem_Req* req = &(mem->req_buffer[queue->base[ii].reqbuf]);
-        if(req->type != MRT_IPRF && req->type != MRT_DPRF)
+        if(req->type != MRT_IPRF && req->type != MRT_DPRF && req->type != MRT_UOCPRF && req->type != MRT_FDIPPRF)
           continue;
         if(oldest_req_age > req->start_cycle) {
           if(req->state < MRS_MEM_WAIT) {
@@ -3352,12 +3422,13 @@ static void mem_init_new_req(
   new_req->off_path_confirmed = FALSE;
   new_req->state              = to_mlc ? MRS_MLC_NEW : MRS_L1_NEW;
   new_req->type               = type;
+  new_req->types              = 0;
+  new_req->emitted_cycle      = cycle_count;
+  mem_req_set_types(new_req, type);
   new_req->queue              = to_mlc ? &mem->mlc_queue : &mem->l1_queue;
   new_req->proc_id            = proc_id;
   new_req->addr               = addr;
   new_req->phys_addr          = addr_translate(addr);
-  // if (DRAM_RANDOM_ADDR) new_req->phys_addr = convert_to_cmp_addr(proc_id,
-  // rand()*MEMORY_INTERLEAVE_FACTOR);
   if(MEMORY_RANDOM_ADDR)
     new_req->phys_addr = convert_to_cmp_addr(proc_id,
                                              rand() * MEMORY_INTERLEAVE_FACTOR);
@@ -3430,6 +3501,10 @@ static void mem_init_new_req(
 
   if(new_req->type == MRT_IFETCH && icache_off_path())
     new_req->off_path = TRUE;
+  // All oracle (correct-path) prefetches are on path.
+  if (((new_req->type == MRT_UOCPRF && !UOC_ORACLE_PREF) || new_req->type == MRT_FDIPPRF) &&
+      fdip_pref_off_path())
+    new_req->off_path = TRUE;
 
   STAT_EVENT(proc_id, MEM_REQ_INIT_IFETCH + type);
   STAT_EVENT(proc_id, MEM_REQ_INIT);
@@ -3514,7 +3589,7 @@ void mem_insert_req_round_robin() {
                                  order_num;
         mem_insert_req_into_queue(
           *req_ptr, (*req_ptr)->queue,
-          ((*req_ptr)->type == MRT_DPRF || (*req_ptr)->type == MRT_IPRF) ?
+          ((*req_ptr)->type == MRT_DPRF || (*req_ptr)->type == MRT_IPRF || (*req_ptr)->type == MRT_UOCPRF || (*req_ptr)->type == MRT_FDIPPRF) ?
             0 :
             order_num);
         order_num++;
@@ -3656,7 +3731,7 @@ Flag new_mem_req(Mem_Req_Type type, uns8 proc_id, Addr addr, uns size,
      out of there) */
   if(new_req == NULL) {
     // cmp IGNORE (MLC IGNORE too =)
-    if(KICKOUT_PREFETCHES && (type != MRT_IPRF) && (type != MRT_DPRF)) {
+    if(KICKOUT_PREFETCHES && (type != MRT_IPRF) && (type != MRT_DPRF) && (type != MRT_UOCPRF) && (type != MRT_FDIPPRF)) {
       if(!KICKOUT_LOOK_FOR_OLDEST_FIRST)
         new_req = mem_kick_out_prefetch_from_queues(
           MEMORY_BANK_XOR_BANK ?
@@ -4085,7 +4160,7 @@ static Flag new_mem_l1_wb_req(Mem_Req_Type type, uns8 proc_id, Addr addr,
   if(new_req == NULL) {
     // cmp FIXME prefechers // MLC IGNORE
     if(KICKOUT_PREFETCHES &&
-       ((type != MRT_IPRF) && (type != MRT_DPRF))) {  // FIXME: do we kick out
+       ((type != MRT_IPRF) && (type != MRT_DPRF) && (type != MRT_UOCPRF) && (type != MRT_FDIPPRF))) {  // FIXME: do we kick out
                                                       // stuff for writebacks
                                                       // also?
       if(KICKOUT_LOOK_FOR_OLDEST_FIRST)
@@ -4404,7 +4479,7 @@ Flag l1_fill_line(Mem_Req* req) {
 
   // Put prefetches in the right position for replacement
   // cmp FIXME prefetchers
-  if(req->type == MRT_DPRF || req->type == MRT_IPRF) {
+  if(req->type == MRT_DPRF || req->type == MRT_IPRF || req->type == MRT_UOCPRF || req->type == MRT_FDIPPRF) {
     mem->pref_replpos = INSERT_REPL_DEFAULT;
     if(PREF_INSERT_LRU) {
       mem->pref_replpos = INSERT_REPL_LRU;
@@ -4756,7 +4831,7 @@ Flag mlc_fill_line(Mem_Req* req) {
                  (req->state != MRS_FILL_MLC));  // write back can fill mlc
                                                  // directly - reqs filling core
                                                  // should not dirty the line
-  data->prefetch = req->type == MRT_DPRF || req->type == MRT_IPRF ||
+  data->prefetch = req->type == MRT_DPRF || req->type == MRT_IPRF || req->type == MRT_UOCPRF || req->type == MRT_FDIPPRF ||
                    req->demand_match_prefetch;
   data->seen_prefetch = req->demand_match_prefetch; /* If demand matches
                                                        prefetch, then it is
@@ -5342,6 +5417,8 @@ static void update_mem_req_occupancy_counter(Mem_Req_Type type, int delta) {
       break;
     case MRT_IPRF:
     case MRT_DPRF:
+    case MRT_UOCPRF:
+    case MRT_FDIPPRF:
       counter = &mem_req_pref_entries;
       break;
     case MRT_WB:
@@ -5361,3 +5438,20 @@ uns num_offchip_stall_reqs(uns proc_id) {
   return 0;  // Ramulator_todo: replicate this in Ramulator. Currently is used
              // only to collect statistics
 }
+
+Counter count_fdip_mem_l1_reqs() {
+  Mem_Req* req = NULL;
+  int      ii;
+  int      reqbuf_id;
+  Counter  num_fdip_reqs = 0;
+
+  for(ii = 0; ii < mem->l1_queue.entry_count; ii++) {
+    reqbuf_id = mem->l1_queue.base[ii].reqbuf;
+    req       = &(mem->req_buffer[reqbuf_id]);
+
+    if(mem_req_is_type(req, MRT_FDIPPRF))
+      num_fdip_reqs++;
+  }
+
+  return num_fdip_reqs;
+}
diff --git a/src/memory/memory.h b/src/memory/memory.h
index d48d12f..5101218 100644
--- a/src/memory/memory.h
+++ b/src/memory/memory.h
@@ -44,8 +44,6 @@
 /**************************************************************************************/
 /* Defines */
 
-//#define MAX_OPS_PER_REQ    2048
-
 
 /**************************************************************************************/
 /* Types */
@@ -76,6 +74,22 @@ typedef struct L1_Data_struct {
 
 typedef L1_Data MLC_Data; /* Use the same data structure for simplicity */
 
+typedef enum Mem_Queue_Req_Result_enum {
+  FAILED,       // Request could not be adjusted because of a merge conflict between requests
+  SUCCESS_NEW,  // A new mem request is successfully created
+  //SUCCESS_DIFF, // Requests of different types successfully adjusted
+  //SUCCESS_DIFF_TYPE_SAME_PATH, // Requests of different types and same path (both on or both off) successfully adjusted
+  //SUCCESS_DIFF_TYPE_DIFF_PATH, // Requests of different types and different path (one on and one off) successfully adjusted
+  SUCCESS_DIFF_TYPE,
+  SUCCESS_DIFF_TYPE_ADDED,
+  //SUCCESS_SAME, // Requests of same types successfully adjusted
+  //SUCCESS_SAME_TYPE_SAME_PATH, // Requests of the same type and same path (both on or both off) successfully adjusted
+  //SUCCESS_SAME_TYPE_DIFF_PATH, // Requests of the same type and different path (one on and one off) successfully adjusted
+  SUCCESS_SAME_TYPE,
+  SUCCESS_SAME_TYPE_INVALID_OFF_PATH_CHANGED,
+  SUCCESS_SAME_TYPE_VALID_OFF_PATH_CHANGED,
+} Mem_Queue_Req_Result;
+
 typedef enum Mem_Queue_Type_enum {
   QUEUE_L1        = 1 << 0,
   QUEUE_BUS_OUT   = 1 << 1,
@@ -259,6 +273,8 @@ void wp_process_reqbuf_match(Mem_Req* req, Op* op);
 uns num_chip_demands(void);
 uns num_offchip_stall_reqs(uns proc_id);
 
+Counter count_fdip_mem_l1_reqs(void);
+
 /**************************************************************************************/
 /* Externs */
 
diff --git a/src/memory/memory.param.def b/src/memory/memory.param.def
index ced4af2..c623b45 100644
--- a/src/memory/memory.param.def
+++ b/src/memory/memory.param.def
@@ -104,6 +104,19 @@ DEF_PARAM(  icache_write_ports             , ICACHE_WRITE_PORTS              , u
 DEF_PARAM(  icache_banks                   , ICACHE_BANKS                    , uns    , uns     , 8                      ,       )
 DEF_PARAM(  icache_repl                    , ICACHE_REPL                     , uns    , uns     , 0                      ,       )
 
+/* Uop Cache */
+DEF_PARAM(  uop_cache_additional_issue_bandwidth, UOP_CACHE_ADDITIONAL_ISSUE_BANDWIDTH, uns, uns, 1                      ,       )
+DEF_PARAM(  oracle_perfect_uop_cache       , ORACLE_PERFECT_UOP_CACHE        , Flag   , Flag    , FALSE                  ,       )
+DEF_PARAM(  inf_size_uop_cache             , INF_SIZE_UOP_CACHE              , Flag   , Flag    , FALSE                  ,       )
+DEF_PARAM(  inf_size_uop_cache_pw_size_lim , INF_SIZE_UOP_CACHE_PW_SIZE_LIM  , uns    , uns     , 0                      ,       )
+// uop_cache_size = the number of entries in a uop cache (uop cache lines).
+// Total uop capacity is uop_cache_size * uop_cache_max_uops_line.
+DEF_PARAM(  uop_cache_size                 , UOP_CACHE_SIZE                  , uns    , uns     , 2048                   ,       )
+DEF_PARAM(  uop_cache_assoc                , UOP_CACHE_ASSOC                 , uns    , uns     , 8                      ,       )
+DEF_PARAM(  uop_cache_max_uops_line        , UOP_CACHE_MAX_UOPS_LINE         , uns    , uns     , 4                      ,       )
+DEF_PARAM(  uop_cache_max_imm_disp_line    , UOP_CACHE_MAX_IMM_DISP_LINE     , uns    , uns     , 4                      ,       )
+
+
 /* L0 data */
 DEF_PARAM(  dcache_size                    , DCACHE_SIZE                     , uns    , uns     , (64 * 1024)            ,       )
 DEF_PARAM(  dcache_assoc                   , DCACHE_ASSOC                    , uns    , uns     , 1                      ,       )
@@ -132,6 +145,8 @@ DEF_PARAM(  mem_priority_ifetch               , MEM_PRIORITY_IFETCH
 DEF_PARAM(  mem_priority_dfetch               , MEM_PRIORITY_DFETCH                         , uns    , uns      , 0                      ,       )
 DEF_PARAM(  mem_priority_dstore               , MEM_PRIORITY_DSTORE                         , uns    , uns      , 0                      ,       )
 DEF_PARAM(  mem_priority_iprf                 , MEM_PRIORITY_IPRF                           , uns    , uns      , 9                      ,       )
+DEF_PARAM(  mem_priority_uocprf               , MEM_PRIORITY_UOCPRF                         , uns    , uns      , 0                      ,       )
+DEF_PARAM(  mem_priority_fdipprf              , MEM_PRIORITY_FDIPPRF                        , uns    , uns      , 0                      ,       )
 DEF_PARAM(  mem_priority_dprf                 , MEM_PRIORITY_DPRF                           , uns    , uns      , 10                     ,       )
 DEF_PARAM(  mem_priority_wb                   , MEM_PRIORITY_WB                             , uns    , uns      , 0                      ,       )
 DEF_PARAM(  mem_priority_wb_nodirty           , MEM_PRIORITY_WB_NODIRTY                     , uns    , uns      , 0                      ,       )
diff --git a/src/memory/memory.stat.def b/src/memory/memory.stat.def
index a113108..f422b66 100644
--- a/src/memory/memory.stat.def
+++ b/src/memory/memory.stat.def
@@ -69,6 +69,37 @@ DEF_STAT(  ICACHE_HIT_ONPATH_SAT_BY_ONPATH		   , DIST  , NO_RATIO  )
 DEF_STAT(  ICACHE_HIT_OFFPATH_SAT_BY_OFFPATH		   , DIST  , NO_RATIO  )
 DEF_STAT(  ICACHE_HIT_OFFPATH_SAT_BY_ONPATH		   , DIST  , NO_RATIO  )
 
+
+DEF_STAT(  UOP_CACHE_MISS               , DIST    , NO_RATIO)
+DEF_STAT(  UOP_CACHE_HIT                , DIST    , NO_RATIO)
+
+DEF_STAT(  UOP_CACHE_CYCLES_SAVED       , COUNT   , NO_RATIO) // cycles saved by 1st op in sequence of ops in deconde stage
+DEF_STAT(  IN_UOP_CACHE_CALLED          , COUNT   , NO_RATIO)
+DEF_STAT(  N_UOPS_DEC                   , COUNT   , NO_RATIO)
+DEF_STAT(  UOP_ACCUMULATE               , COUNT   , NO_RATIO)
+DEF_STAT(  UOP_CACHE_PWS_INSERTED       , COUNT   , NO_RATIO)
+DEF_STAT(  UOP_CACHE_LINES_INSERTED     , COUNT   , NO_RATIO)
+DEF_STAT(  UOP_CACHE_LINES_USED         , COUNT   , NO_RATIO)
+DEF_STAT(  UOP_CACHE_PREFETCH           , COUNT   , NO_RATIO)
+DEF_STAT(  UOP_CACHE_PREFETCH_FAILED_PW_NEVER_SEEN, COUNT, NO_RATIO)
+DEF_STAT(  UOP_CACHE_PREFETCH_FAILED_MEMREQ_FAILED, COUNT, NO_RATIO)
+DEF_STAT(  UOP_CACHE_PREFETCH_USED      , COUNT   , NO_RATIO)
+DEF_STAT(  UOP_CACHE_PW_INSERT_FAILED_TOO_LONG    , COUNT, NO_RATIO)
+DEF_STAT(  UOP_CACHE_PW_PREFETCH_INSERT_FAILED_TOO_LONG     , COUNT, NO_RATIO)
+DEF_STAT(  UOP_CACHE_PW_INSERT_FAILED_CACHE_HIT   , COUNT, NO_RATIO)
+DEF_STAT(  UOP_CACHE_PW_PREFETCH_INSERT_FAILED_CACHE_HIT    , COUNT, NO_RATIO)
+
+DEF_STAT(  UOP_CACHE_ICACHE_SWITCH_BR_NOT_TAKEN_RESTEERED   , DIST    , NO_RATIO)
+DEF_STAT(  UOP_CACHE_ICACHE_SWITCH_BR_NOT_TAKEN_CORRECT_PRED, COUNT   , NO_RATIO)
+DEF_STAT(  UOP_CACHE_ICACHE_SWITCH_BR_TAKEN_RESTEERED       , COUNT   , NO_RATIO)
+DEF_STAT(  UOP_CACHE_ICACHE_SWITCH_BR_TAKEN_CORRECT_PRED    , DIST    , NO_RATIO)
+
+DEF_STAT(  BP_RECOVERY_FETCH_CYCLES_UC       , DIST    , NO_RATIO)
+DEF_STAT(  BP_RECOVERY_FETCH_CYCLES_IC       , DIST    , NO_RATIO)
+DEF_STAT(  BP_REDIRECT_FETCH_CYCLES_UC       , DIST    , NO_RATIO)
+DEF_STAT(  BP_REDIRECT_FETCH_CYCLES_IC       , DIST    , NO_RATIO)
+
+
      /* these three are on path */
 DEF_STAT(  DCACHE_MISS		   , DIST  , NO_RATIO  )
 DEF_STAT(  DCACHE_ST_BUFFER_HIT	   , COUNT , NO_RATIO  )
@@ -431,6 +462,9 @@ DEF_STAT(  MEM_REQ_DPRF		 , COUNT , NO_RATIO  )
 DEF_STAT(  MEM_REQ_WB		 , COUNT , NO_RATIO  )
 DEF_STAT(  MEM_REQ_WB_NODIRTY	 , DIST	 , NO_RATIO  )
 
+DEF_STAT(  MEM_REQ_FDIP_BUFFER_HIT    , COUNT ,  NO_RATIO  )
+DEF_STAT(  MEM_REQ_FDIP_CYCLE_DELTA   , COUNT ,  NO_RATIO  )
+
 DEF_STAT(  LD_NO_FORWARD	 , DIST	 , NO_RATIO  )
 DEF_STAT(  FORWARDED_LD	         , DIST	 , NO_RATIO  )
 
diff --git a/src/node_stage.c b/src/node_stage.c
index 56e47f1..265f822 100644
--- a/src/node_stage.c
+++ b/src/node_stage.c
@@ -61,7 +61,7 @@
 #define PRINT_RETIRED_UOP(proc_id, args...) \
   _DEBUG_LEAN(proc_id, DEBUG_RETIRED_UOPS, ##args)
 
-#define DEBUG_NODE_WIDTH ISSUE_WIDTH
+#define DEBUG_NODE_WIDTH ISSUE_WIDTH + UOP_CACHE_ADDITIONAL_ISSUE_BANDWIDTH
 #define OP_IS_IN_RS(op) (op->state >= OS_IN_RS && op->state < OS_SCHEDULED)
 
 /**************************************************************************************/
@@ -299,7 +299,10 @@ void debug_print_node_table() {
         print_open_op_array(GLOBAL_DEBUG_STREAM, temp, DEBUG_NODE_WIDTH,
                             DEBUG_NODE_WIDTH);
       }
-      memset(temp, 0, DEBUG_NODE_WIDTH * sizeof(Op*));
+      // For some reason this does not zero out the entire array.
+      // (Assert fails and verified in gdb).
+      // memset(temp, 0, DEBUG_NODE_WIDTH * sizeof(Op*));
+      for (int i=0; i<DEBUG_NODE_WIDTH; i++) temp[i] = 0;
       empty = TRUE;
     }
   }
diff --git a/src/op.h b/src/op.h
index da0022e..94c7eb4 100644
--- a/src/op.h
+++ b/src/op.h
@@ -90,6 +90,27 @@ typedef struct Wake_Up_Entry_struct {
 } Wake_Up_Entry;
 // }}}
 
+
+// per branch stats
+typedef struct Per_Branch_Stat_struct {
+  Addr addr;
+  Cf_Type cf_type;
+  Addr target;
+  int bpu_hit_uc_hit;
+  int bpu_hit_uc_miss;
+  int bpu_hit_uc_ic_miss;
+  int mispred_uc_hit;
+  int mispred_uc_miss;
+  int mispred_uc_ic_miss;   // mispred that miss in both uc and ic
+  int misfetch_uc_hit;
+  int misfetch_uc_miss;
+  int misfetch_uc_ic_miss;
+  int btb_miss_uc_hit;
+  int btb_miss_uc_miss;
+  int btb_miss_uc_ic_miss;
+  int recover_redirect_extra_fetch_latency; // extra stall cycles due to target not being in UC
+} Per_Branch_Stat;
+
 /*------------------------------------------------------------------------------------*/
 // {{{ Recovery_Info
 // this information is used when the op mispredicts
@@ -279,6 +300,9 @@ struct Op_struct {
   struct Mbp7gshare_Info_struct* mbp7_info;  // multiple branch predictor
                                              // information
 
+  Addr pred_target; // last predicted target for this op.
+  Addr pc_plus_offset;
+
   // {{{ temporary fields -> will be deleted later (move these)
   int  derived_from_prog_input;  // derivation level from program read()
   int  min_input_id;
@@ -289,9 +313,22 @@ struct Op_struct {
   Addr pred_addr;
   Flag recovery_scheduled;
   // }}}
+
+  // {{{ uop cache
+  Flag fetched_from_uop_cache;
+  // }}}
 };
 // }}}
 
+// Contains the first and last addresses in the prediction window
+typedef struct Uop_Cache_Data_struct {
+  Addr first;
+  Addr last;
+  Counter n_uops;
+  Flag prefetch;
+  Counter used;
+} Uop_Cache_Data;
+
 /**************************************************************************************/
 
 #endif  // #ifndef __OP_H__
diff --git a/src/op_pool.c b/src/op_pool.c
index fb37745..8f14645 100644
--- a/src/op_pool.c
+++ b/src/op_pool.c
@@ -58,6 +58,7 @@ allocates them once and then hands out pointers every time 'alloc_op' is called.
 #define DEBUG(proc_id, args...) _DEBUG(proc_id, DEBUG_OP_POOL, ##args)
 #define DEBUGU(proc_id, args...) _DEBUGU(proc_id, DEBUG_OP_POOL, ##args)
 
+// TODO: it should be increased to 512 to use more than 50,000 FDIP lookahead buffer entries
 #define OP_POOL_ENTRIES_INC 128 /* default 128 */
 
 /**************************************************************************************/
@@ -246,6 +247,7 @@ void op_pool_setup_op(uns proc_id, Op* op) {
   op->engine_info.num_srcs           = 0;
   op->engine_info.update_fpcr        = FALSE;
 
+  op->fetched_from_uop_cache         = FALSE;
 
   for(ii = 0; ii < NUM_DEP_TYPES; ii++)
     op->wake_up_signaled[ii] = FALSE;
diff --git a/src/packet_build.c b/src/packet_build.c
index 5612831..454ef82 100644
--- a/src/packet_build.c
+++ b/src/packet_build.c
@@ -33,10 +33,12 @@
 #include "globals/utils.h"
 
 #include "icache_stage.h"
+#include "uop_cache.h"
 #include "op.h"
 #include "packet_build.h"
 
 #include "bp/bp.param.h"
+#include "prefetcher/fdip.h"
 #include "core.param.h"
 #include "debug/debug.param.h"
 #include "memory/memory.param.h"
@@ -166,7 +168,7 @@ inline void reset_packet_build(Pb_Data* pb_data) {
 */
 
 Flag packet_build(Pb_Data* pb_data, Break_Reason* break_fetch, Op* const op,
-                  uns const index) {
+                  Flag uop_cache_issue_ops) {
   Break_Reason model_break_result;
 
   ASSERT(pb_data->proc_id, pb_data->proc_id == op->proc_id);
@@ -208,6 +210,31 @@ Flag packet_build(Pb_Data* pb_data, Break_Reason* break_fetch, Op* const op,
       return PB_BREAK_BEFORE;
     }
 
+    // log UC access. This statement must be reached for each uop fetched.
+    // break if in uc fetch mode and cur instr is NOT from UC
+    // ideally we should break and we try same cycle to fetch from IC
+    if (!in_uop_cache(op->inst_info->addr, &op->op_num, TRUE) && uop_cache_issue_ops) {
+      *break_fetch = BREAK_UC_MISS;
+      op->fetched_from_uop_cache = FALSE;
+      // Inaccuracy: should break before op. But how does this op get refetched from frontend?
+      // by breaking after I always fetch one extra op per PW from UC
+      Flag frontend_resteer = op->oracle_info.mispred || op->oracle_info.btb_miss || op->oracle_info.misfetch;
+      // UOC to IC switches can happen at any branch, whether or not the branch causes a frontend resteer.
+      // The reason we distinguish between correct and incorrect predictions is that FDIP should be able
+      // to prefetch correct predictions. Incorrect predictions require other techniques.
+      // FDIP cannot prefetch ahead of time on neither a misprediction nor BTB miss nor misfetch.
+      STAT_EVENT(ic->proc_id, UOP_CACHE_ICACHE_SWITCH_BR_NOT_TAKEN_RESTEERED
+                 + 2 * op->oracle_info.dir + !frontend_resteer);
+      return PB_BREAK_AFTER;
+    } else if (uop_cache_issue_ops) {
+      op->fetched_from_uop_cache = TRUE;
+    }
+
+    if (fdip_is_max_op(op)) {
+      *break_fetch = BREAK_FDIP_RUNAHEAD;
+      return PB_BREAK_AFTER;
+    }
+
     // hit fetch barrier
     if(IS_CALLSYS(op->table_info) || op->table_info->bar_type & BAR_FETCH) {
       *break_fetch = BREAK_BARRIER;
@@ -237,8 +264,9 @@ Flag packet_build(Pb_Data* pb_data, Break_Reason* break_fetch, Op* const op,
     }
 
     // issue width reached
+    const int issue_width = uop_cache_issue_ops ? UC_ISSUE_WIDTH : IC_ISSUE_WIDTH;
     if(pb_data->pb_ident == PB_ICACHE) {
-      if(ic->sd.op_count + 1 == ISSUE_WIDTH) {
+      if(ic->sd.op_count + 1 == issue_width) {
         *break_fetch = BREAK_ISSUE_WIDTH;
         return PB_BREAK_AFTER;
       }
diff --git a/src/packet_build.h b/src/packet_build.h
index 832c276..70b98ce 100644
--- a/src/packet_build.h
+++ b/src/packet_build.h
@@ -108,6 +108,8 @@ typedef enum Break_Reason_enum {
   BREAK_TAKEN,         // break because of nonsequential control flow
   BREAK_MODEL_BEFORE,  // break because of model hook
   BREAK_MODEL_AFTER,   // break because of model hook
+  BREAK_UC_MISS,       // break because during UC fetch, a following instr was not found in UC
+  BREAK_FDIP_RUNAHEAD, // break because a following instr is the last
 } Break_Reason;
 
 
@@ -116,7 +118,7 @@ typedef enum Break_Reason_enum {
 
 void init_packet_build(Pb_Data*, Packet_Build_Identifier);
 void reset_packet_build(Pb_Data*);
-Flag packet_build(Pb_Data*, Break_Reason*, Op* const, uns const);
+Flag packet_build(Pb_Data*, Break_Reason*, Op* const, Flag uop_cache_issue_ops);
 
 
 /**************************************************************************************/
diff --git a/src/pin/pin_lib/Makefile b/src/pin/pin_lib/Makefile
index ae28b40..38134f3 100644
--- a/src/pin/pin_lib/Makefile
+++ b/src/pin/pin_lib/Makefile
@@ -25,7 +25,7 @@ WARN_CCFLAGS= -Wall -Wunused -Wno-long-long -Wpointer-arith -Werror
 NO_STAT ?=
 PIN_COMPILE ?=
 INCLUDES = -I$(SCARAB_DIR) -I$(PIN_ROOT)/
-OPTCFLAGS   = -std=gnu99 -O3 -g $(ARCHTYPE) -funroll-loops -Werror $(INCLUDES) $(DEBUG_FLAGS) $(WARN_CFLAGS) $(XFLAGS) $(NO_STAT) $(PIN_COMPILE)
+OPTCFLAGS   = -std=gnu99 -O3 -g $(ARCHTYPE) -funroll-loops -Werror $(INCLUDES) $(DEBUG_FLAGS) $(WARN_CFLAGS) $(XFLAGS) $(NO_STAT) $(PIN_COMPILE) 
 OPTCCFLAGS   = -std=c++14 -O3 -g $(ARCHTYPE) -fno-inline -funroll-loops -Werror $(INCLUDES) $(DEBUG_FLAGS) $(WARN_CCFLAGS) $(XFLAGS) $(NO_STAT) $(PIN_COMPILE)
 
 DEPDIR := $(OBJDIR)
diff --git a/src/pin/pin_lib/README.md b/src/pin/pin_lib/README.md
index f993408..d543515 100644
--- a/src/pin/pin_lib/README.md
+++ b/src/pin/pin_lib/README.md
@@ -309,7 +309,7 @@ You then create a new `iclass_to_scarab` struct entry and place it at
 the index of the corresponding `xed_iclass_enum_t` enum member in the
 array `iclass_to_scarab_map`. This is done inside
 `init_pin_opcode_convert()` in
-[src/pin/pin_lib/decoder.cc](../../../src/pin/pin_lib/decoder.cc). Here are some examples for some
+[src/pin/pin_lib/x86_decoder.cc](../../../src/pin/pin_lib/x86_decoder.cc). Here are some examples for some
 of the instructions we've talked about in this tutorial:
 
 `iclass_to_scarab_map[XED_ICLASS_ADD] = {OP_IADD, -1, 1, NONE};`
diff --git a/src/pin/pin_lib/pin_api_to_xed.h b/src/pin/pin_lib/pin_api_to_xed.h
index 398c47d..620979b 100644
--- a/src/pin/pin_lib/pin_api_to_xed.h
+++ b/src/pin/pin_lib/pin_api_to_xed.h
@@ -53,6 +53,7 @@ struct InstInfo {
   uint64_t pid;                   // process ID
   uint64_t tid;                   // thread ID
   uint64_t target;                // branch target
+  uint64_t static_target;         // encoded branch target (not dynamic). Only non-zero when the information differs from what XED tells you.
   uint64_t mem_addr[2];           // memory addresses
   bool mem_used[2];               // mem address usage flags
   CustomOp custom_op;             // Special or non-x86 ISA instruction
@@ -66,13 +67,14 @@ struct InstInfo {
 
 #define XED_OP_NAME(ins, op) xed_operand_name(xed_inst_operand(xed_decoded_inst_inst(ins), op))
 
+#define INS_Byte(ins, idx) (xed_decoded_inst_get_byte(ins.ins, idx))
 #define INS_Nop(ins) (INS_Category(ins)) == XED_CATEGORY_NOP || INS_Category(ins) == XED_CATEGORY_WIDENOP)
 #define INS_LEA(ins) (INS_Opcode(ins)) == XO(LEA))
 #define INS_Opcode(ins) xed_decoded_inst_get_iclass(ins.ins)
 #define INS_Category(ins) xed_decoded_inst_get_category(ins.ins)
 #define INS_IsAtomicUpdate(ins) xed_decoded_inst_get_attribute(ins.ins), XED_ATTRIBUTE_LOCKED)
 //FIXME: Check if REPs are translated correctly
-#define INS_IsRep(ins) xed_decoded_inst_get_attribute(ins.ins), XED_ATTRIBUTE_REP)
+#define INS_IsRep(ins) xed_decoded_inst_get_attribute((ins.ins), XED_ATTRIBUTE_REP)
 #define INS_HasRealRep(ins) xed_operand_values_has_real_rep(xed_decoded_inst_operands((xed_decoded_inst_t *)ins.ins))
 #define INS_OperandCount(ins) xed_decoded_inst_noperands(ins.ins)
 #define INS_OperandIsImmediate(ins, op) XED_IS_IMM(ins, op)
@@ -91,10 +93,11 @@ struct InstInfo {
 #define INS_MemoryOperandIsWritten(ins, op) XED_MEM_WRITTEN(ins, op)
 #define INS_MemoryOperandCount(ins) xed_decoded_inst_number_of_memory_operands(ins.ins)
 #define INS_IsDirectBranch(ins) xed3_operand_get_brdisp_width(ins.ins)
+/* #define INS_IsDirectBranch(ins) !xed_decoded_inst_get_attribute(ins.ins, XED_ATTRIBUTE_INDIRECT_BRANCH) */
 #define INS_Size(ins) xed_decoded_inst_get_length(ins.ins)
 #define INS_Valid(ins) xed_decoded_inst_valid(ins.ins)
 /* Just like PIN we break BBLs on a number of additional instructions such as REP */
-#define INS_ChangeControlFlow(ins) (INS_Category(ins) == XC(COND_BR) || INS_Category(ins) == XC(UNCOND_BR) || INS_Category(ins) == XC(CALL) || INS_Category(ins) == XC(RET) || INS_Category(ins) == XC(SYSCALL) || INS_Category(ins) == XC(SYSRET) || INS_Opcode(ins) == XO(CPUID) || INS_Opcode(ins) == XO(POPF) || INS_Opcode(ins) == XO(POPFD) || INS_Opcode(ins) == XO(POPFQ) || INS_IsRep(ins)
+#define INS_ChangeControlFlow(ins) (INS_Category(ins) == XC(COND_BR) || INS_Category(ins) == XC(UNCOND_BR) || INS_Category(ins) == XC(CALL) || INS_Category(ins) == XC(RET) || INS_Category(ins) == XC(SYSCALL) || INS_Category(ins) == XC(SYSRET) || INS_Opcode(ins) == XO(CPUID) || INS_Opcode(ins) == XO(POPF) || INS_Opcode(ins) == XO(POPFD) || INS_Opcode(ins) == XO(POPFQ) || INS_IsRep(ins))
 #define REG_FullRegName(reg) xed_get_largest_enclosing_register(reg)
 
 #define UINT32 uint32_t
diff --git a/src/pin/pin_lib/reg.h b/src/pin/pin_lib/reg.h
new file mode 100644
index 0000000..b86efe5
--- /dev/null
+++ b/src/pin/pin_lib/reg.h
@@ -0,0 +1,998 @@
+#ifndef PIN_PIN_LIB_REG_H
+#define PIN_PIN_LIB_REG_H
+#define TARGET_IA32E
+typedef enum
+{
+    REG_INVALID_ = 0,
+# if !defined(TARGET_DOXYGEN)
+    REG_NONE = 1,
+    REG_FIRST = 2,
+
+    // base for all kinds of registers (application, machine, pin)
+    REG_RBASE,
+
+    // Machine registers are individual real registers on the machine
+    REG_MACHINE_BASE = REG_RBASE,
+
+    // Application registers are registers used in the application binary
+    // Application registers include all machine registers. In addition,
+    // they include some aggregrate registers that can be accessed by
+    // the application in a single instruction
+    // Essentially, application registers = individual machine registers + aggregrate registers
+
+    REG_APPLICATION_BASE = REG_RBASE,
+
+    /* !@ todo: should save scratch mmx and fp registers */
+    // The machine registers that form a context. These are the registers
+    // that need to be saved in a context switch.
+    REG_PHYSICAL_INTEGER_BASE = REG_RBASE,
+
+    REG_TO_SPILL_BASE = REG_RBASE,
+
+# endif // not TARGET_DOXYGEN
+
+    REG_GR_BASE = REG_RBASE,
+# if defined(TARGET_IA32E)
+    // Context registers in the Intel(R) 64 architecture
+    REG_RDI = REG_GR_BASE,  ///< rdi
+    REG_GDI = REG_RDI,      ///< edi on a 32 bit machine, rdi on 64
+    REG_RSI,                ///< rsi
+    REG_GSI = REG_RSI,      ///< esi on a 32 bit machine, rsi on 64
+    REG_RBP,                ///< rbp
+    REG_GBP = REG_RBP,      ///< ebp on a 32 bit machine, rbp on 64
+    REG_RSP,                ///< rsp
+    REG_STACK_PTR = REG_RSP,///< esp on a 32 bit machine, rsp on 64
+    REG_RBX,                ///< rbx
+    REG_GBX = REG_RBX,      ///< ebx on a 32 bit machine, rbx on 64
+    REG_RDX,                ///< rdx
+    REG_GDX = REG_RDX,      ///< edx on a 32 bit machine, rdx on 64
+    REG_RCX,                ///< rcx
+    REG_GCX = REG_RCX,      ///< ecx on a 32 bit machine, rcx on 64
+    REG_RAX,                ///< rax
+    REG_GAX = REG_RAX,      ///< eax on a 32 bit machine, rax on 64
+    REG_R8,
+    REG_R9,
+    REG_R10,
+    REG_R11,
+    REG_R12,
+    REG_R13,
+    REG_R14,
+    REG_R15,
+    REG_GR_LAST = REG_R15,
+
+    REG_SEG_BASE,
+    REG_SEG_CS = REG_SEG_BASE,
+    REG_SEG_SS,
+    REG_SEG_DS,
+    REG_SEG_ES,
+    REG_SEG_FS,
+    REG_SEG_GS,
+    REG_SEG_LAST = REG_SEG_GS,
+
+    REG_RFLAGS,
+    REG_GFLAGS=REG_RFLAGS,
+    REG_RIP,
+    REG_INST_PTR = REG_RIP,
+# else // not defined(TARGET_IA32E)
+    // Context registers in the IA-32 architecture
+    REG_EDI = REG_GR_BASE,
+    REG_GDI = REG_EDI,
+    REG_ESI,
+    REG_GSI = REG_ESI,
+    REG_EBP,
+    REG_GBP = REG_EBP,
+    REG_ESP,
+    REG_STACK_PTR = REG_ESP,
+    REG_EBX,
+    REG_GBX = REG_EBX,
+    REG_EDX,
+    REG_GDX = REG_EDX,
+    REG_ECX,
+    REG_GCX = REG_ECX,
+    REG_EAX,
+    REG_GAX = REG_EAX,
+    REG_GR_LAST = REG_EAX,
+
+    REG_SEG_BASE,
+    REG_SEG_CS = REG_SEG_BASE,
+    REG_SEG_SS,
+    REG_SEG_DS,
+    REG_SEG_ES,
+    REG_SEG_FS,
+    REG_SEG_GS,
+    REG_SEG_LAST = REG_SEG_GS,
+
+    REG_EFLAGS,
+    REG_GFLAGS=REG_EFLAGS,
+    REG_EIP,
+    REG_INST_PTR = REG_EIP,
+# endif // not defined(TARGET_IA32E)
+
+# if !defined(TARGET_DOXYGEN)
+    REG_PHYSICAL_INTEGER_END = REG_INST_PTR,
+# endif // not TARGET_DOXYGEN
+
+    // partial registers common to both the IA-32 and Intel(R) 64 architectures.
+    REG_AL,
+    REG_AH,
+    REG_AX,
+
+    REG_CL,
+    REG_CH,
+    REG_CX,
+
+    REG_DL,
+    REG_DH,
+    REG_DX,
+
+    REG_BL,
+    REG_BH,
+    REG_BX,
+
+    REG_BP,
+    REG_SI,
+    REG_DI,
+
+    REG_SP,
+    REG_FLAGS,
+    REG_IP,
+
+# if defined(TARGET_IA32E)
+    // partial registers in the Intel(R) 64 architecture
+    REG_EDI,
+    REG_DIL,
+    REG_ESI,
+    REG_SIL,
+    REG_EBP,
+    REG_BPL,
+    REG_ESP,
+    REG_SPL,
+    REG_EBX,
+    REG_EDX,
+    REG_ECX,
+    REG_EAX,
+    REG_EFLAGS,
+    REG_EIP,
+
+    REG_R8B,
+    REG_R8W,
+    REG_R8D,
+    REG_R9B,
+    REG_R9W,
+    REG_R9D,
+    REG_R10B,
+    REG_R10W,
+    REG_R10D,
+    REG_R11B,
+    REG_R11W,
+    REG_R11D,
+    REG_R12B,
+    REG_R12W,
+    REG_R12D,
+    REG_R13B,
+    REG_R13W,
+    REG_R13D,
+    REG_R14B,
+    REG_R14W,
+    REG_R14D,
+    REG_R15B,
+    REG_R15W,
+    REG_R15D,
+# endif // not defined(TARGET_IA32E)
+
+    REG_MM_BASE,
+    REG_MM0 = REG_MM_BASE,
+    REG_MM1,
+    REG_MM2,
+    REG_MM3,
+    REG_MM4,
+    REG_MM5,
+    REG_MM6,
+    REG_MM7,
+    REG_MM_LAST = REG_MM7,
+
+    REG_XMM_BASE,
+    REG_FIRST_FP_REG = REG_XMM_BASE,
+    REG_XMM0 = REG_XMM_BASE,
+    REG_XMM1,
+    REG_XMM2,
+    REG_XMM3,
+    REG_XMM4,
+    REG_XMM5,
+    REG_XMM6,
+    REG_XMM7,
+
+# if defined(TARGET_IA32E)
+    // additional xmm registers in the Intel(R) 64 architecture
+    REG_XMM8,
+    REG_XMM9,
+    REG_XMM10,
+    REG_XMM11,
+    REG_XMM12,
+    REG_XMM13,
+    REG_XMM14,
+    REG_XMM15,
+    REG_XMM_SSE_LAST = REG_XMM15,
+    REG_XMM_AVX_LAST = REG_XMM_SSE_LAST,
+    REG_XMM_AVX512_HI16_FIRST,
+    REG_XMM16 = REG_XMM_AVX512_HI16_FIRST,
+    REG_XMM17,
+    REG_XMM18,
+    REG_XMM19,
+    REG_XMM20,
+    REG_XMM21,
+    REG_XMM22,
+    REG_XMM23,
+    REG_XMM24,
+    REG_XMM25,
+    REG_XMM26,
+    REG_XMM27,
+    REG_XMM28,
+    REG_XMM29,
+    REG_XMM30,
+    REG_XMM31,
+    REG_XMM_AVX512_HI16_LAST = REG_XMM31,
+    REG_XMM_AVX512_LAST = REG_XMM_AVX512_HI16_LAST,
+    REG_XMM_LAST = REG_XMM_AVX512_LAST,
+# else // not TARGET_IA32E
+    REG_XMM_SSE_LAST = REG_XMM7,
+    REG_XMM_AVX_LAST = REG_XMM_SSE_LAST,
+    REG_XMM_AVX512_LAST = REG_XMM_AVX_LAST,
+    REG_XMM_LAST = REG_XMM_AVX512_LAST,
+# endif // not TARGET_IA32E
+
+    REG_YMM_BASE,
+    REG_YMM0 = REG_YMM_BASE,
+    REG_YMM1,
+    REG_YMM2,
+    REG_YMM3,
+    REG_YMM4,
+    REG_YMM5,
+    REG_YMM6,
+    REG_YMM7,
+
+# if defined(TARGET_IA32E)
+    // additional ymm registers in the Intel(R) 64 architecture
+    REG_YMM8,
+    REG_YMM9,
+    REG_YMM10,
+    REG_YMM11,
+    REG_YMM12,
+    REG_YMM13,
+    REG_YMM14,
+    REG_YMM15,
+    REG_YMM_AVX_LAST = REG_YMM15,
+    REG_YMM_AVX512_HI16_FIRST,
+    REG_YMM16 = REG_YMM_AVX512_HI16_FIRST,
+    REG_YMM17,
+    REG_YMM18,
+    REG_YMM19,
+    REG_YMM20,
+    REG_YMM21,
+    REG_YMM22,
+    REG_YMM23,
+    REG_YMM24,
+    REG_YMM25,
+    REG_YMM26,
+    REG_YMM27,
+    REG_YMM28,
+    REG_YMM29,
+    REG_YMM30,
+    REG_YMM31,
+    REG_YMM_AVX512_HI16_LAST = REG_YMM31,
+    REG_YMM_AVX512_LAST = REG_YMM_AVX512_HI16_LAST,
+    REG_YMM_LAST = REG_YMM_AVX512_LAST,
+# else // not TARGET_IA32E
+    REG_YMM_AVX_LAST = REG_YMM7,
+    REG_YMM_AVX512_LAST = REG_YMM_AVX_LAST,
+    REG_YMM_LAST = REG_YMM_AVX512_LAST,
+# endif // not TARGET_IA32E
+
+    REG_ZMM_BASE,
+    REG_ZMM0 = REG_ZMM_BASE,
+    REG_ZMM1,
+    REG_ZMM2,
+    REG_ZMM3,
+    REG_ZMM4,
+    REG_ZMM5,
+    REG_ZMM6,
+    REG_ZMM7,
+# if defined(TARGET_IA32E)
+    REG_ZMM8,
+    REG_ZMM9,
+    REG_ZMM10,
+    REG_ZMM11,
+    REG_ZMM12,
+    REG_ZMM13,
+    REG_ZMM14,
+    REG_ZMM15,
+    REG_ZMM_AVX512_SPLIT_LAST = REG_ZMM15,
+    REG_ZMM_AVX512_HI16_FIRST,
+    REG_ZMM16 = REG_ZMM_AVX512_HI16_FIRST,
+    REG_ZMM17,
+    REG_ZMM18,
+    REG_ZMM19,
+    REG_ZMM20,
+    REG_ZMM21,
+    REG_ZMM22,
+    REG_ZMM23,
+    REG_ZMM24,
+    REG_ZMM25,
+    REG_ZMM26,
+    REG_ZMM27,
+    REG_ZMM28,
+    REG_ZMM29,
+    REG_ZMM30,
+    REG_ZMM31,
+    REG_ZMM_AVX512_HI16_LAST = REG_ZMM31,
+    REG_ZMM_AVX512_LAST = REG_ZMM_AVX512_HI16_LAST,
+    REG_ZMM_LAST = REG_ZMM_AVX512_LAST,
+# else // not defined(TARGET_IA32E)
+    REG_ZMM_AVX512_SPLIT_LAST = REG_ZMM7,
+    REG_ZMM_AVX512_LAST = REG_ZMM_AVX512_SPLIT_LAST,
+    REG_ZMM_LAST = REG_ZMM_AVX512_LAST,
+# endif // not defined(TARGET_IA32E)
+
+    REG_K_BASE,
+    REG_K0 = REG_K_BASE,
+    // The K0 opmask register cannot be used as the write mask operand of an AVX512 instruction.
+    // However the encoding of K0 as the write mask operand is legal and is used as an implicit full mask.
+    REG_IMPLICIT_FULL_MASK = REG_K0,
+    REG_K1,
+    REG_K2,
+    REG_K3,
+    REG_K4,
+    REG_K5,
+    REG_K6,
+    REG_K7,
+    REG_K_LAST = REG_K7,
+
+    REG_MXCSR,
+    REG_MXCSRMASK,
+
+    // This corresponds to the "orig_eax" register that is visible
+    // to some debuggers.
+# if defined(TARGET_IA32E)
+    REG_ORIG_RAX,
+    REG_ORIG_GAX = REG_ORIG_RAX,
+# else // not defined(TARGET_IA32E)
+    REG_ORIG_EAX,
+    REG_ORIG_GAX = REG_ORIG_EAX,
+# endif // not defined(TARGET_IA32E)
+
+    REG_FPST_BASE,
+    REG_FPSTATUS_BASE = REG_FPST_BASE,
+    REG_FPCW = REG_FPSTATUS_BASE,
+    REG_FPSW,
+    REG_FPTAG,          ///< Abridged 8-bit version of x87 tag register.
+    REG_FPIP_OFF,
+    REG_FPIP_SEL,
+    REG_FPOPCODE,
+    REG_FPDP_OFF,
+    REG_FPDP_SEL,
+    REG_FPSTATUS_LAST = REG_FPDP_SEL,
+
+    REG_FPTAG_FULL,     ///< Full 16-bit version of x87 tag register.
+
+    REG_ST_BASE,
+    REG_ST0 = REG_ST_BASE,
+    REG_ST1,
+    REG_ST2,
+    REG_ST3,
+    REG_ST4,
+    REG_ST5,
+    REG_ST6,
+    REG_ST7,
+    REG_ST_LAST = REG_ST7,
+    REG_FPST_LAST = REG_ST_LAST,
+
+    REG_DR_BASE,
+    REG_DR0 = REG_DR_BASE,
+    REG_DR1,
+    REG_DR2,
+    REG_DR3,
+    REG_DR4,
+    REG_DR5,
+    REG_DR6,
+    REG_DR7,
+    REG_DR_LAST = REG_DR7,
+
+    REG_CR_BASE,
+    REG_CR0 = REG_CR_BASE,
+    REG_CR1,
+    REG_CR2,
+    REG_CR3,
+    REG_CR4,
+    REG_CR_LAST = REG_CR4,
+
+    REG_TSSR,
+    REG_LDTR,
+
+    REG_TR_BASE,
+    REG_TR = REG_TR_BASE,
+    REG_TR3,
+    REG_TR4,
+    REG_TR5,
+    REG_TR6,
+    REG_TR7,
+    REG_TR_LAST = REG_TR7,
+
+    # if !defined(TARGET_DOXYGEN)
+
+    REG_MACHINE_LAST = REG_TR_LAST, /* last machine register */
+
+    /* these are the two registers implementing the eflags in pin
+       REG_STATUS_FLAGS represents the OF, SF, ZF, AF, PF and CF flags.
+       REG_DF_FLAG      represents the DF flag.
+       flag splitting is done because the DF flag spilling and filling is rather expensive,
+       and the DF flag is not read/written by most instructions - therefore it is
+       not necessary to spill/fill it on most instructions that read/write the flags.
+       (prior to flag splitting, whenever any of the flags needed to be spilled/filled
+       both the DF and all the above status flags were spilled/filled).
+       NOTE - this flag splitting is not done if the pushf/popf sequence is being used
+              rather than the sahf/lahf sequence (some early Intel64 processors do not
+              support sahf/lahf instructions). Also the KnobRegFlagsSplit can be used
+              to disable the flags splitting when the sahf/lahf sequence is being used
+       Flags splitting is not done at the INS operand level - it is done when building
+       the vreglist for register allocation. So tools see the architectural flags registers
+       in INSs. See the functions MakeRegisterList and REG_InsertReadRegToVreglist to see
+       how the split flags are inserted into the vreglist.
+       See jit_flags_spillfill_ia32.cpp file comments to learn how they are spilled/filled
+     */
+    REG_STATUS_FLAGS,
+    REG_DF_FLAG,
+
+    // NOTE: although REG_X87 is outside REG_APPLICATION_LAST scope it is part of app registers
+    // therefore any traversal of all application regs need to have special handling for REG_X87
+    REG_APPLICATION_LAST = REG_DF_FLAG, /* last register name used by the application */
+
+    /* Pin's virtual register names */
+    REG_TOOL_BASE,
+
+# endif  // not TARGET_DOXYGEN
+
+    // Virtual registers reg holding memory addresses pointed by GS/FS registers
+    // These registers are visible for tool writers
+    REG_SEG_GS_BASE  = REG_TOOL_BASE, ///< Base address for GS segment
+    REG_SEG_FS_BASE, ///< Base address for FS segment
+
+    // ISA-independent Pin virtual regs needed for instrumentation
+    // These are pin registers visible to the pintool writers.
+    REG_INST_BASE,
+    REG_INST_SCRATCH_BASE = REG_INST_BASE,  ///< First available scratch register
+    REG_INST_G0 = REG_INST_SCRATCH_BASE,    ///< Scratch register used in pintools
+    REG_INST_G1,                            ///< Scratch register used in pintools
+    REG_INST_G2,                            ///< Scratch register used in pintools
+    REG_INST_G3,                            ///< Scratch register used in pintools
+    REG_INST_G4,                            ///< Scratch register used in pintools
+    REG_INST_G5,                            ///< Scratch register used in pintools
+    REG_INST_G6,                            ///< Scratch register used in pintools
+    REG_INST_G7,                            ///< Scratch register used in pintools
+    REG_INST_G8,                            ///< Scratch register used in pintools
+    REG_INST_G9,                            ///< Scratch register used in pintools
+    REG_INST_G10,                           ///< Scratch register used in pintools
+    REG_INST_G11,                           ///< Scratch register used in pintools
+    REG_INST_G12,                           ///< Scratch register used in pintools
+    REG_INST_G13,                           ///< Scratch register used in pintools
+    REG_INST_G14,                           ///< Scratch register used in pintools
+    REG_INST_G15,                           ///< Scratch register used in pintools
+    REG_INST_G16,                           ///< Scratch register used in pintools
+    REG_INST_G17,                           ///< Scratch register used in pintools
+    REG_INST_G18,                           ///< Scratch register used in pintools
+    REG_INST_G19,                           ///< Scratch register used in pintools
+    REG_INST_G20,                           ///< Scratch register used in pintools
+    REG_INST_G21,                           ///< Scratch register used in pintools
+    REG_INST_G22,                           ///< Scratch register used in pintools
+    REG_INST_G23,                           ///< Scratch register used in pintools
+    REG_INST_G24,                           ///< Scratch register used in pintools
+    REG_INST_G25,                           ///< Scratch register used in pintools
+    REG_INST_G26,                           ///< Scratch register used in pintools
+    REG_INST_G27,                           ///< Scratch register used in pintools
+    REG_INST_G28,                           ///< Scratch register used in pintools
+    REG_INST_G29,                           ///< Scratch register used in pintools
+    REG_INST_TOOL_FIRST = REG_INST_G0,
+    REG_INST_TOOL_LAST = REG_INST_G29,
+
+    REG_BUF_BASE0,
+    REG_BUF_BASE1,
+    REG_BUF_BASE2,
+    REG_BUF_BASE3,
+    REG_BUF_BASE4,
+    REG_BUF_BASE5,
+    REG_BUF_BASE6,
+    REG_BUF_BASE7,
+    REG_BUF_BASE8,
+    REG_BUF_BASE9,
+    REG_BUF_BASE_LAST = REG_BUF_BASE9,
+
+    REG_BUF_END0,
+    REG_BUF_END1,
+    REG_BUF_END2,
+    REG_BUF_END3,
+    REG_BUF_END4,
+    REG_BUF_END5,
+    REG_BUF_END6,
+    REG_BUF_END7,
+    REG_BUF_END8,
+    REG_BUF_END9,
+    REG_BUF_ENDLAST = REG_BUF_END9,
+    REG_BUF_LAST = REG_BUF_ENDLAST,
+
+    REG_INST_SCRATCH_LAST = REG_BUF_LAST,
+
+# if defined(TARGET_IA32E)
+    // DWORD versions of the above G0-G29 scratch regs
+    REG_INST_G0D,                           ///< Scratch register used in pintools
+    REG_INST_G1D,                           ///< Scratch register used in pintools
+    REG_INST_G2D,                           ///< Scratch register used in pintools
+    REG_INST_G3D,                           ///< Scratch register used in pintools
+    REG_INST_G4D,                           ///< Scratch register used in pintools
+    REG_INST_G5D,                           ///< Scratch register used in pintools
+    REG_INST_G6D,                           ///< Scratch register used in pintools
+    REG_INST_G7D,                           ///< Scratch register used in pintools
+    REG_INST_G8D,                           ///< Scratch register used in pintools
+    REG_INST_G9D,                           ///< Scratch register used in pintools
+    REG_INST_G10D,                          ///< Scratch register used in pintools
+    REG_INST_G11D,                          ///< Scratch register used in pintools
+    REG_INST_G12D,                          ///< Scratch register used in pintools
+    REG_INST_G13D,                          ///< Scratch register used in pintools
+    REG_INST_G14D,                          ///< Scratch register used in pintools
+    REG_INST_G15D,                          ///< Scratch register used in pintools
+    REG_INST_G16D,                          ///< Scratch register used in pintools
+    REG_INST_G17D,                          ///< Scratch register used in pintools
+    REG_INST_G18D,                          ///< Scratch register used in pintools
+    REG_INST_G19D,                          ///< Scratch register used in pintools
+    REG_INST_G20D,                          ///< Scratch register used in pintools
+    REG_INST_G21D,                          ///< Scratch register used in pintools
+    REG_INST_G22D,                          ///< Scratch register used in pintools
+    REG_INST_G23D,                          ///< Scratch register used in pintools
+    REG_INST_G24D,                          ///< Scratch register used in pintools
+    REG_INST_G25D,                          ///< Scratch register used in pintools
+    REG_INST_G26D,                          ///< Scratch register used in pintools
+    REG_INST_G27D,                          ///< Scratch register used in pintools
+    REG_INST_G28D,                          ///< Scratch register used in pintools
+    REG_INST_G29D,                          ///< Scratch register used in pintools
+    REG_TOOL_LAST = REG_INST_G29D,
+# else // end of defined(TARGET_IA32E)
+    REG_TOOL_LAST = REG_BUF_LAST,
+# endif // end of !(defined(TARGET_IA32E))
+
+# if !defined(TARGET_DOXYGEN)
+    REG_SPECIAL_BASE,
+
+    // REG_X87 is a representative of the X87 fp state - it is NOT available for explicit use in ANY
+    // of the Pin APIs.
+    // This register is set/get internally using xsave/xrstor or fxsave/fxrstor.
+    // In order to allow proper work of xsave/xrstor, the size of this register includes all the legacy xfeatures
+    // and the extended header. see @ref REG_X87_SIZE.
+    REG_X87 = REG_SPECIAL_BASE,
+
+    REG_SPECIAL_LAST = REG_X87,
+
+    REG_PIN_BASE,
+
+    REG_PIN_SEG_GS_VAL = REG_PIN_BASE,  // virtual reg holding actual value of gs
+    REG_PIN_SEG_FS_VAL,                 // virtual reg holding actual value of fs
+
+    REG_LAST_CONTEXT_REG = REG_PIN_SEG_FS_VAL,  // Last register in the canonical SPILL AREA Based CONTEXT
+
+    REG_PIN_GR_BASE,
+
+    // ia32-specific Pin gr regs
+    REG_PIN_EDI = REG_PIN_GR_BASE,
+
+#  if defined(TARGET_IA32)
+    REG_PIN_GDI = REG_PIN_EDI,                  // PIN_GDI == PIN_EDI on 32 bit, PIN_RDI on 64 bit.
+#  endif // defined(TARGET_IA32)
+
+    REG_PIN_ESI,
+
+#  if defined(TARGET_IA32)
+    REG_PIN_GSI = REG_PIN_ESI,
+#  endif // defined(TARGET_IA32)
+
+    REG_PIN_EBP,
+
+#  if defined(TARGET_IA32)
+    REG_PIN_GBP = REG_PIN_EBP,
+#  endif // defined(TARGET_IA32)
+
+    REG_PIN_ESP,
+
+#  if defined (TARGET_IA32)
+    REG_PIN_STACK_PTR = REG_PIN_ESP,
+#  endif // defined(TARGET_IA32)
+
+    REG_PIN_EBX,
+
+#  if defined(TARGET_IA32)
+    REG_PIN_GBX = REG_PIN_EBX,
+#  endif // defined(TARGET_IA32)
+
+    REG_PIN_EDX,
+
+#  if defined(TARGET_IA32)
+    REG_PIN_GDX = REG_PIN_EDX,
+#  endif // defined(TARGET_IA32)
+
+    REG_PIN_ECX,
+
+#  if defined(TARGET_IA32)
+    REG_PIN_GCX = REG_PIN_ECX,                  // PIN_GCX == PIN_ECX on 32 bit, PIN_RCX on 64 bit.
+#  endif // defined(TARGET_IA32)
+
+    REG_PIN_EAX,
+
+#  if defined(TARGET_IA32)
+    REG_PIN_GAX = REG_PIN_EAX,                  // PIN_GAX == PIN_EAX on 32 bit, PIN_RAX on 64 bit.
+#  endif // defined(TARGET_IA32)
+
+    REG_PIN_AL,
+    REG_PIN_AH,
+    REG_PIN_AX,
+    REG_PIN_CL,
+    REG_PIN_CH,
+    REG_PIN_CX,
+    REG_PIN_DL,
+    REG_PIN_DH,
+    REG_PIN_DX,
+    REG_PIN_BL,
+    REG_PIN_BH,
+    REG_PIN_BX,
+    REG_PIN_BP,
+    REG_PIN_SI,
+    REG_PIN_DI,
+    REG_PIN_SP,
+
+#  if defined(TARGET_IA32E)
+    // Intel(R) 64 architecture specific pin gr regs
+    REG_PIN_RDI,
+    REG_PIN_GDI = REG_PIN_RDI,
+    REG_PIN_RSI,
+    REG_PIN_GSI = REG_PIN_RSI,
+    REG_PIN_RBP,
+    REG_PIN_GBP = REG_PIN_RBP,
+    REG_PIN_RSP,
+
+    REG_PIN_STACK_PTR = REG_PIN_RSP,
+
+    REG_PIN_RBX,
+    REG_PIN_GBX = REG_PIN_RBX,
+    REG_PIN_RDX,
+    REG_PIN_GDX = REG_PIN_RDX,
+    REG_PIN_RCX,
+    REG_PIN_GCX = REG_PIN_RCX,
+    REG_PIN_RAX,
+    REG_PIN_GAX = REG_PIN_RAX,
+    REG_PIN_R8,
+    REG_PIN_R9,
+    REG_PIN_R10,
+    REG_PIN_R11,
+    REG_PIN_R12,
+    REG_PIN_R13,
+    REG_PIN_R14,
+    REG_PIN_R15,
+
+    REG_PIN_DIL,
+    REG_PIN_SIL,
+    REG_PIN_BPL,
+    REG_PIN_SPL,
+
+    REG_PIN_R8B,
+    REG_PIN_R8W,
+    REG_PIN_R8D,
+
+    REG_PIN_R9B,
+    REG_PIN_R9W,
+    REG_PIN_R9D,
+
+    REG_PIN_R10B,
+    REG_PIN_R10W,
+    REG_PIN_R10D,
+
+    REG_PIN_R11B,
+    REG_PIN_R11W,
+    REG_PIN_R11D,
+
+    REG_PIN_R12B,
+    REG_PIN_R12W,
+    REG_PIN_R12D,
+
+    REG_PIN_R13B,
+    REG_PIN_R13W,
+    REG_PIN_R13D,
+
+    REG_PIN_R14B,
+    REG_PIN_R14W,
+    REG_PIN_R14D,
+
+    REG_PIN_R15B,
+    REG_PIN_R15W,
+    REG_PIN_R15D,
+#  endif // defined(TARGET_IA32E)
+
+    // Every thread is assigned an index so we can implement tls
+    REG_PIN_THREAD_ID,
+
+    // ISA-independent gr regs
+    REG_PIN_INDIRREG,  // virtual reg holding indirect jmp target value
+    REG_PIN_IPRELADDR, // virtual reg holding ip-rel address value
+    REG_PIN_SYSENTER_RESUMEADDR, // virtual reg holding the resume address from sysenter
+    REG_PIN_SYSCALL_NEXT_PC,  // virtual reg holding the next PC when Pin emulates a system call
+    REG_PIN_VMENTER, // virtual reg holding the address of VmEnter
+                     // actually it is the spill slot of this register that holds
+                     // the address
+
+    // ISA-independent gr regs holding temporary values
+    REG_PIN_T_BASE,
+#ifdef TARGET_IA32E
+    REG_PIN_T0 = REG_PIN_T_BASE,
+    REG_PIN_T1,
+    REG_PIN_T2,
+    REG_PIN_T3,
+    REG_PIN_T0D,    // lower 32 bits of temporary register
+    REG_PIN_T1D,
+    REG_PIN_T2D,
+    REG_PIN_T3D,
+#else // not TARGET_IA32E
+    REG_PIN_T0 = REG_PIN_T_BASE,
+    REG_PIN_T0D = REG_PIN_T0,
+    REG_PIN_T1,
+    REG_PIN_T1D = REG_PIN_T1,
+    REG_PIN_T2,
+    REG_PIN_T2D = REG_PIN_T2,
+    REG_PIN_T3,
+    REG_PIN_T3D = REG_PIN_T3,
+#endif // not TARGET_IA32E
+    REG_PIN_T0W,    // lower 16 bits of temporary register
+    REG_PIN_T1W,
+    REG_PIN_T2W,
+    REG_PIN_T3W,
+    REG_PIN_T0L,    // lower 8 bits of temporary register
+    REG_PIN_T1L,
+    REG_PIN_T2L,
+    REG_PIN_T3L,
+    REG_PIN_T_LAST = REG_PIN_T3L,
+    REG_PIN_THREAD_IDD,    // REG_PIN_THREAD_ID 32 half part
+    REG_TO_SPILL_LAST = REG_PIN_THREAD_IDD,
+    REG_PIN_INST_COND,     // for conditional instrumentation.
+
+    // Used for memory rewriting, these are not live outside the region
+    // but cannot use general purpose scratch registers, because they're
+    // used during instrumentation generation, rather than region generation.
+#ifdef TARGET_IA32E
+    REG_PIN_INST_T0,
+    REG_PIN_INST_T1,
+    REG_PIN_INST_T2,
+    REG_PIN_INST_T3,
+    REG_PIN_INST_T0D,    // lower 32 bits of temporary register
+    REG_PIN_INST_T1D,
+    REG_PIN_INST_T2D,
+    REG_PIN_INST_T3D,
+#else // not TARGET_IA32E
+    REG_PIN_INST_T0,
+    REG_PIN_INST_T0D = REG_PIN_INST_T0,
+    REG_PIN_INST_T1,
+    REG_PIN_INST_T1D = REG_PIN_INST_T1,
+    REG_PIN_INST_T2,
+    REG_PIN_INST_T2D = REG_PIN_INST_T2,
+    REG_PIN_INST_T3,
+    REG_PIN_INST_T3D = REG_PIN_INST_T3,
+#endif // not TARGET_IA32E
+    REG_PIN_INST_T0W,    // lower 16 bits of temporary register
+    REG_PIN_INST_T1W,
+    REG_PIN_INST_T2W,
+    REG_PIN_INST_T3W,
+    REG_PIN_INST_T0L,    // lower 8 bits of temporary register
+    REG_PIN_INST_T1L,
+    REG_PIN_INST_T2L,
+    REG_PIN_INST_T3L,
+
+    // Used to preserve the predicate value around repped string ops
+    REG_PIN_INST_PRESERVED_PREDICATE,
+
+    // Used when the AC flag needs to be cleared before analysis routine
+    REG_PIN_FLAGS_BEFORE_AC_CLEARING,
+
+    // Virtual regs used by Pin inside instrumentation bridges.
+    // Unlike REG_INST_BASE to REG_INST_LAST, these registers are
+    // NOT visible to  Pin clients.
+    REG_PIN_BRIDGE_ORIG_SP,    // hold the stack ptr value before the bridge
+    REG_PIN_BRIDGE_APP_IP, // hold the application (not code cache) IP to resume
+    REG_PIN_BRIDGE_SP_BEFORE_ALIGN, // hold the stack ptr value before the stack alignment
+    REG_PIN_BRIDGE_SP_BEFORE_CALL, // hold the stack ptr value before call to replaced function in probe mode
+    REG_PIN_BRIDGE_SP_BEFORE_MARSHALLING_FRAME, // hold the stack ptr value before allocating the marshalling frame
+    REG_PIN_BRIDGE_MARSHALLING_FRAME, // hold the address of the marshalled reference registers
+    REG_PIN_BRIDGE_ON_STACK_CONTEXT_FRAME, // hold the address of the on stack context frame
+    REG_PIN_BRIDGE_ON_STACK_CONTEXT_SP, // hold the sp at which the on stack context was pushed
+    REG_PIN_BRIDGE_MULTI_MEMORYACCESS_FRAME, // hold the address of the on stack PIN_MULTI_MEM_ACCESS_INFO frame
+    REG_PIN_BRIDGE_MULTI_MEMORYACCESS_SP, // hold the sp at which the PIN_MULTI_MEM_ACCESS_INFO was pushed
+    // hold the address of the on stack MULTI_MEM_ACCESS_AND_REWRITE_EMULATION_INFO frame
+    REG_PIN_MULTI_MEM_ACCESS_AND_REWRITE_EMULATION_INFO_FRAME,
+    REG_PIN_BRIDGE_TRANS_MEMORY_CALLBACK_FRAME, // hold the address of the on stack PIN_MEM_TRANS_INFO frame
+    REG_PIN_BRIDGE_TRANS_MEMORY_CALLBACK_SP, // hold the sp at which the PIN_MEM_TRANS_INFO was pushed
+    REG_PIN_TRANS_MEMORY_CALLBACK_READ_ADDR, // hold the result of read memory address calculation
+    REG_PIN_TRANS_MEMORY_CALLBACK_READ2_ADDR, // hold the result of read2 memory address calculation
+    REG_PIN_TRANS_MEMORY_CALLBACK_WRITE_ADDR, // hold the result of write memory address calculation
+    REG_PIN_BRIDGE_SPILL_AREA_CONTEXT_FRAME, // hold the address of the spill area context frame
+    REG_PIN_BRIDGE_SPILL_AREA_CONTEXT_SP, // hold the sp at which the spill area context was pushed
+
+    REG_PIN_SPILLPTR,  // ptr to the pin spill area
+    REG_PIN_GR_LAST = REG_PIN_SPILLPTR,
+    REG_PIN_X87,
+    REG_PIN_MXCSR,
+
+    // REG_PIN_FLAGS is x86-specific, but since it is not a gr, we put it out of
+    // REG_PIN_GR_BASE and REG_PIN_GR_LAST
+
+    /* these are the two registers implementing the PIN flags in pin
+       REG_PIN_STATUS_FLAGS represents the OF, SF, ZF, AF, PF and CF flags.
+       REG_PIN_DF_FLAG      represents the DF flag.
+     */
+    REG_PIN_STATUS_FLAGS,
+    REG_PIN_DF_FLAG,
+
+    /* REG_PIN_FLAGS is used only in the case when the pushf/popf sequence is used
+       for flags spill/fill rather than the sahf/lahf sequence.
+     */
+    REG_PIN_FLAGS,
+
+    REG_PIN_XMM_BASE,
+    REG_PIN_XMM0 = REG_PIN_XMM_BASE,
+    REG_PIN_XMM1,
+    REG_PIN_XMM2,
+    REG_PIN_XMM3,
+    REG_PIN_XMM4,
+    REG_PIN_XMM5,
+    REG_PIN_XMM6,
+    REG_PIN_XMM7,
+#  if defined(TARGET_IA32E)
+    // additional xmm registers in the Intel(R) 64 architecture
+    REG_PIN_XMM8,
+    REG_PIN_XMM9,
+    REG_PIN_XMM10,
+    REG_PIN_XMM11,
+    REG_PIN_XMM12,
+    REG_PIN_XMM13,
+    REG_PIN_XMM14,
+    REG_PIN_XMM15,
+    REG_PIN_XMM_SSE_LAST = REG_PIN_XMM15,
+    REG_PIN_XMM_AVX_LAST = REG_PIN_XMM_SSE_LAST,
+    REG_PIN_XMM_AVX512_HI16_FIRST,
+    REG_PIN_XMM16 = REG_PIN_XMM_AVX512_HI16_FIRST,
+    REG_PIN_XMM17,
+    REG_PIN_XMM18,
+    REG_PIN_XMM19,
+    REG_PIN_XMM20,
+    REG_PIN_XMM21,
+    REG_PIN_XMM22,
+    REG_PIN_XMM23,
+    REG_PIN_XMM24,
+    REG_PIN_XMM25,
+    REG_PIN_XMM26,
+    REG_PIN_XMM27,
+    REG_PIN_XMM28,
+    REG_PIN_XMM29,
+    REG_PIN_XMM30,
+    REG_PIN_XMM31,
+    REG_PIN_XMM_AVX512_HI16_LAST = REG_PIN_XMM31,
+    REG_PIN_XMM_AVX512_LAST = REG_PIN_XMM_AVX512_HI16_LAST,
+    REG_PIN_XMM_LAST = REG_PIN_XMM_AVX512_LAST,
+#  else // not TARGET_IA32E
+    REG_PIN_XMM_SSE_LAST = REG_PIN_XMM7,
+    REG_PIN_XMM_AVX_LAST = REG_PIN_XMM_SSE_LAST,
+    REG_PIN_XMM_AVX512_LAST = REG_PIN_XMM_AVX_LAST,
+    REG_PIN_XMM_LAST = REG_PIN_XMM_AVX512_LAST,
+#  endif // TARGET_IA32E
+
+    REG_PIN_YMM_BASE,
+    REG_PIN_YMM0 = REG_PIN_YMM_BASE,
+    REG_PIN_YMM1,
+    REG_PIN_YMM2,
+    REG_PIN_YMM3,
+    REG_PIN_YMM4,
+    REG_PIN_YMM5,
+    REG_PIN_YMM6,
+    REG_PIN_YMM7,
+#  if defined(TARGET_IA32E)
+    // additional ymm registers in the Intel(R) 64 architecture
+    REG_PIN_YMM8,
+    REG_PIN_YMM9,
+    REG_PIN_YMM10,
+    REG_PIN_YMM11,
+    REG_PIN_YMM12,
+    REG_PIN_YMM13,
+    REG_PIN_YMM14,
+    REG_PIN_YMM15,
+    REG_PIN_YMM_AVX_LAST = REG_PIN_YMM15,
+    REG_PIN_YMM_AVX512_HI16_FIRST,
+    REG_PIN_YMM16 = REG_PIN_YMM_AVX512_HI16_FIRST,
+    REG_PIN_YMM17,
+    REG_PIN_YMM18,
+    REG_PIN_YMM19,
+    REG_PIN_YMM20,
+    REG_PIN_YMM21,
+    REG_PIN_YMM22,
+    REG_PIN_YMM23,
+    REG_PIN_YMM24,
+    REG_PIN_YMM25,
+    REG_PIN_YMM26,
+    REG_PIN_YMM27,
+    REG_PIN_YMM28,
+    REG_PIN_YMM29,
+    REG_PIN_YMM30,
+    REG_PIN_YMM31,
+    REG_PIN_YMM_AVX512_HI16_LAST = REG_PIN_YMM31,
+    REG_PIN_YMM_AVX512_LAST = REG_PIN_YMM_AVX512_HI16_LAST,
+    REG_PIN_YMM_LAST = REG_PIN_YMM_AVX512_LAST,
+#  else // not TARGET_IA32E
+    REG_PIN_YMM_AVX_LAST = REG_PIN_YMM7,
+    REG_PIN_YMM_AVX512_LAST = REG_PIN_YMM_AVX_LAST,
+    REG_PIN_YMM_LAST = REG_PIN_YMM_AVX512_LAST,
+#  endif // not TARGET_IA32E
+
+    REG_PIN_ZMM_BASE,
+    REG_PIN_ZMM0 = REG_PIN_ZMM_BASE,
+    REG_PIN_ZMM1,
+    REG_PIN_ZMM2,
+    REG_PIN_ZMM3,
+    REG_PIN_ZMM4,
+    REG_PIN_ZMM5,
+    REG_PIN_ZMM6,
+    REG_PIN_ZMM7,
+#  ifndef TARGET_IA32
+    REG_PIN_ZMM8,
+    REG_PIN_ZMM9,
+    REG_PIN_ZMM10,
+    REG_PIN_ZMM11,
+    REG_PIN_ZMM12,
+    REG_PIN_ZMM13,
+    REG_PIN_ZMM14,
+    REG_PIN_ZMM15,
+    REG_PIN_ZMM_AVX512_SPLIT_LAST = REG_PIN_ZMM15,
+    REG_PIN_ZMM_AVX512_HI16_FIRST,
+    REG_PIN_ZMM16 = REG_PIN_ZMM_AVX512_HI16_FIRST,
+    REG_PIN_ZMM17,
+    REG_PIN_ZMM18,
+    REG_PIN_ZMM19,
+    REG_PIN_ZMM20,
+    REG_PIN_ZMM21,
+    REG_PIN_ZMM22,
+    REG_PIN_ZMM23,
+    REG_PIN_ZMM24,
+    REG_PIN_ZMM25,
+    REG_PIN_ZMM26,
+    REG_PIN_ZMM27,
+    REG_PIN_ZMM28,
+    REG_PIN_ZMM29,
+    REG_PIN_ZMM30,
+    REG_PIN_ZMM31,
+    REG_PIN_ZMM_AVX512_HI16_LAST = REG_PIN_ZMM31,
+    REG_PIN_ZMM_AVX512_LAST = REG_PIN_ZMM_AVX512_HI16_LAST,
+    REG_PIN_ZMM_LAST = REG_PIN_ZMM_AVX512_LAST,
+#  else // TARGET_IA32
+    REG_PIN_ZMM_AVX512_SPLIT_LAST = REG_PIN_ZMM7,
+    REG_PIN_ZMM_AVX512_LAST = REG_PIN_ZMM_AVX512_SPLIT_LAST,
+    REG_PIN_ZMM_LAST = REG_PIN_ZMM_AVX512_LAST,
+#  endif // TARGET_IA32
+
+    REG_PIN_K_BASE,
+    REG_PIN_K0 = REG_PIN_K_BASE,
+    REG_PIN_K1,
+    REG_PIN_K2,
+    REG_PIN_K3,
+    REG_PIN_K4,
+    REG_PIN_K5,
+    REG_PIN_K6,
+    REG_PIN_K7,
+    REG_PIN_K_LAST = REG_PIN_K7,
+
+    REG_PIN_LAST = REG_PIN_K_LAST,
+
+# endif // not TARGET_DOXYGEN
+
+    REG_LAST
+
+
+} REG;
+#endif
diff --git a/src/pin/pin_lib/uop_generator.c b/src/pin/pin_lib/uop_generator.c
index 2a745b8..a1f714d 100644
--- a/src/pin/pin_lib/uop_generator.c
+++ b/src/pin/pin_lib/uop_generator.c
@@ -44,6 +44,7 @@
 #include "../../libs/hash_lib.h"
 
 #include "uop_generator.h"
+#include "math.h"
 
 /**************************************************************************************/
 /* Macros */
@@ -67,6 +68,8 @@ struct Trace_Uop_struct {
   uns  num_agen_src_regs;  // memory address calculation read for ztrace
 
   uns   inst_size;  // instruction size
+  uint64_t inst_binary_msb;        // x86 instr are 1-15 bytes. Store first 8B and last 8B.
+  uint64_t inst_binary_lsb;
   uns64 ztrace_binary;
   Addr  addr;
 
@@ -120,9 +123,9 @@ static void convert_t_uop_to_info(uns8 proc_id, Trace_Uop* t_uop,
 static void convert_dyn_uop(uns8 proc_id, Inst_Info* info, ctype_pin_inst* pi,
                             Trace_Uop* trace_uop, uns mem_size,
                             Flag is_last_uop);
+static int64 inst_info_key_addr(ctype_pin_inst* pi, int op_idx);
 
 /**************************************************************************************/
-
 void uop_generator_init(uint32_t num_cores) {
   inst_info_hash = (Hash_Table*)malloc(num_cores * sizeof(Hash_Table));
   for(uns ii = 0; ii < num_cores; ii++) {
@@ -303,7 +306,7 @@ void uop_generator_get_uop(uns proc_id, Op* op, ctype_pin_inst* inst) {
                                                trace_uop->npc;
   op->oracle_info.va  = trace_uop->va;
   op->oracle_info.npc = trace_uop->npc;
-  if(op->proc_id)
+  //if(op->proc_id)
     ASSERT(op->proc_id, op->oracle_info.npc);
   op->oracle_info.mem_size = trace_uop->mem_size;
   // op->table_info->mem_size = trace_uop->mem_size;  // because of repeat move
@@ -694,9 +697,14 @@ static uns generate_uops(uns8 proc_id, ctype_pin_inst* pi,
 
 void convert_pinuop_to_t_uop(uns8 proc_id, ctype_pin_inst* pi,
                              Trace_Uop** trace_uop) {
+  // Due to JIT compilation, each branch must be decoded to verify which instruction the PC maps to.
+  // To decrease unnecessary malloc/free, fetch inst_info from hashmap
+  // instead of allocating. However first instruction must be decoded.
   Flag       new_entry = FALSE;
-  Addr       key_addr  = (pi->instruction_addr << 3);
+  Addr       key_addr;
   Inst_Info* info;
+
+  key_addr = inst_info_key_addr(pi, 0);
   if(pi->fake_inst) {
     info                   = (Inst_Info*)calloc(1, sizeof(Inst_Info));
     info->fake_inst        = TRUE;
@@ -707,6 +715,7 @@ void convert_pinuop_to_t_uop(uns8 proc_id, ctype_pin_inst* pi,
     info->fake_inst        = FALSE;
     info->fake_inst_reason = WPNM_NOT_IN_WPNM;
   }
+
   int ii;
   int num_uop = 0;
 
@@ -736,7 +745,7 @@ void convert_pinuop_to_t_uop(uns8 proc_id, ctype_pin_inst* pi,
           info->fake_inst        = TRUE;
           info->fake_inst_reason = pi->fake_inst_reason;
         } else {
-          key_addr = ((pi->instruction_addr << 3) + ii);
+          key_addr = inst_info_key_addr(pi, ii);
           info = (Inst_Info*)hash_table_access_create(&inst_info_hash[proc_id],
                                                       key_addr, &new_entry);
           info->fake_inst        = FALSE;
@@ -781,7 +790,7 @@ void convert_pinuop_to_t_uop(uns8 proc_id, ctype_pin_inst* pi,
     num_uop = info->trace_info.num_uop;
     for(ii = 0; ii < num_uop; ii++) {
       if(ii > 0) {
-        key_addr = ((pi->instruction_addr << 3) + ii);
+        key_addr = inst_info_key_addr(pi, ii);
         info = (Inst_Info*)hash_table_access_create(&inst_info_hash[proc_id],
                                                     key_addr, &new_entry);
       }
@@ -850,3 +859,13 @@ void convert_dyn_uop(uns8 proc_id, Inst_Info* info, ctype_pin_inst* pi,
 void uop_generator_recover(uns8 proc_id) {
   bom[proc_id] = 1;
 }
+
+// Hash instruction address and machine instruction bytes.
+// Due to JIT compilation, the address may not uniquely identify one instruction.
+// One instruction may decompose into several ops which have different instruction info.
+static int64 inst_info_key_addr(ctype_pin_inst* pi, int op_idx) {
+  int64 key_addr = 0;
+  key_addr = pi->instruction_addr << 35;  // 64-35=29 lsb of address
+  key_addr += (pi->inst_binary_lsb << 32) >> 32;  // 32b from machine instruction bytes
+  return (key_addr << 3) + op_idx;
+}
diff --git a/src/pin/pin_lib/x86_decoder.cc b/src/pin/pin_lib/x86_decoder.cc
index da0fb3c..f41f479 100644
--- a/src/pin/pin_lib/x86_decoder.cc
+++ b/src/pin/pin_lib/x86_decoder.cc
@@ -23,6 +23,9 @@
 #include <set>
 #include <string>
 #include <unordered_map>
+#ifdef MEMTRACE
+#include "pin/pin_lib/reg.h"
+#endif
 
 #include "pin/pin_lib/pin_scarab_common_lib.h"
 
@@ -37,6 +40,7 @@ typedef enum Reg_Id_struct {
 #undef REG
 
 #include "pin/pin_lib/x86_decoder.h"
+/* static_assert(sizeof(REG) == sizeof(uint8_t), "REG is not the right size"); */
 
 struct iclass_to_scarab {
   int     opcode;
@@ -77,6 +81,7 @@ static Reg_Array_Info reg_array_infos[NUM_REG_ARRAYS] = {
 
 // maps for translation from pin to scarab
 uint8_t reg_compress_map[(int)REG_LAST + 1] = {0};  // Assuming REG_INV is 0
+uint8_t reg_compress_map_pin[(int)REG_LAST + 1] = {0};  // Assuming REG_INV is 0
 // Assuming OP_INV is 0
 iclass_to_scarab iclass_to_scarab_map[XED_ICLASS_LAST] = {{0}};
 
@@ -112,6 +117,17 @@ void fill_in_basic_info(ctype_pin_inst* info, const INS& ins) {
                    category == XED_CATEGORY_RET);
   info->is_lock     = INS_LockPrefix(ins);
   info->is_repeat   = INS_HasRealRep(ins);
+
+  // ifdef MEMTRACE can be removed after rebasing on main scarab repo
+  // (Heiner introduced some build fixes/refactoring)
+  #ifdef MEMTRACE
+  for (int ii = 0; (ii < 8) && (ii < info->size); ii++) {
+    info->inst_binary_lsb = (info->inst_binary_lsb << 8) + INS_Byte(ins, ii);
+  }
+  for (int ii = 0; (ii < 16) && (ii < info->size); ii++) {
+    info->inst_binary_msb = (info->inst_binary_msb << 8) + INS_Byte(ins, ii);
+  }
+  #endif
 }
 
 uint32_t add_dependency_info_memtrace(ctype_pin_inst* info, const INS& ins) {
@@ -170,9 +186,10 @@ uint32_t add_dependency_info_memtrace(ctype_pin_inst* info, const INS& ins) {
       }
     }
   }
-    if (info->instruction_addr == 0x7fb083ade789 && INS_MemoryOperandCount(ins)>1)
-      std::cout << "mem ops " << INS_MemoryOperandCount(ins) << " lds " << info->num_ld << " st " << info->num_st << std::endl;
-
+  if(info->instruction_addr == 0x7fb083ade789 &&
+     INS_MemoryOperandCount(ins) > 1)
+    std::cout << "mem ops " << INS_MemoryOperandCount(ins) << " lds "
+              << info->num_ld << " st " << info->num_st << std::endl;
   return max_op_width;
 }
 
@@ -292,6 +309,9 @@ void fill_in_cf_info(ctype_pin_inst* info, const INS& ins) {
 
   if(INS_IsRet(ins)) {
     info->cf_type = CF_RET;
+  } else if(INS_IsSyscall(ins) || INS_IsSysret(ins) || INS_IsInterrupt(ins)) {
+      /* std::cout << "Setting info for PC " << std::hex << info->instruction_addr << std::endl; */
+    info->cf_type = CF_SYS;
   } else if(INS_IsIndirectBranchOrCall(ins)) {
     // indirect
     if(category == XED_CATEGORY_UNCOND_BR)
@@ -310,15 +330,29 @@ void fill_in_cf_info(ctype_pin_inst* info, const INS& ins) {
     else if(category == XED_CATEGORY_CALL)
       info->cf_type = CF_CALL;
     info->branch_target = INS_DirectBranchOrCallTargetAddress(ins);
-  } else if(INS_IsSyscall(ins) || INS_IsSysret(ins) || INS_IsInterrupt(ins)) {
-    info->cf_type = CF_SYS;
+#ifdef MEMTRACE
+    /* std::cout << " setting direct target for PC: " << std::hex << ins.pc << " to: " << info->branch_target << std::endl; */
+#endif
   }
-
   info->is_ifetch_barrier = is_ifetch_barrier(ins);
 }
-
 void print_err_if_invalid(ctype_pin_inst* info, const INS& ins) {
-  if(info->op_type == OP_INV) {
+    bool invalid = info->op_type == OP_INV;
+#ifdef MEMTRACE
+    bool correct = info->cf_type != NOT_CF || (info->instruction_addr + info->size + xed_operand_values_get_branch_displacement_int32(ins.ins)) == info->branch_target;
+    if (info->is_repeat) correct = true;  // Ignore rep instr since the target is the same instr.
+#else
+    bool correct = true;
+#endif
+  if(invalid || !correct) {
+      if(invalid) {
+          std::cout << "Invalid inst! ";
+      }
+#ifdef MEMTRACE
+      if(!correct) {
+          std::cout << "Not correct inst: " << +info->cf_type << ", " << std::hex << info->instruction_addr << ' ' << std::dec << +info->size << ' ' << xed_operand_values_get_branch_displacement_int32(ins.ins) << ' ' << std::hex << info->branch_target << ' ';
+      }
+#endif
     //(*glb_err_ostream)
     std::cout
       << "Unmapped instruction at "
@@ -360,13 +394,20 @@ uint8_t is_ifetch_barrier(const INS& ins) {
 }
 
 static compressed_reg_t reg_compress(REG pin_reg, ADDRINT ip) {
-  compressed_reg_t result = reg_compress_map[pin_reg];
+    const auto& map = [](){
+#ifdef MEMTRACE
+        return reg_compress_map;
+#else
+        return reg_compress_map_pin;
+#endif
+    }();
+  compressed_reg_t result = map[pin_reg];
   if(result == SCARAB_REG_INV && (int)pin_reg != 0) {
     /*(*glb_err_ostream)*/ std::cerr << "Invalid register operand "
 #ifdef MEMTRACE
 		       << std::string(xed_reg_enum_t2str(pin_reg))
 #else
-      //                       << LEVEL_BASE::REG_StringShort(pin_reg)
+                             /* << "converting reg to string: " << LEVEL_BASE::REG_StringShort(pin_reg) */
 #endif
                        << " at: " << std::hex << ip << std::endl;
   }
@@ -396,320 +437,319 @@ void init_reg_compress_map(void) {
   // Makes sure src_regs is wide enough for Scarab registers.
   assert(SCARAB_NUM_REGS < (1 << (sizeof(compressed_reg_t) * 8)));
 
-#ifndef MEMTRACE
-  reg_compress_map[0]                  = 0;
-  reg_compress_map[(int)REG_GR_BASE]   = SCARAB_REG_RDI;
-  reg_compress_map[(int)REG_EDI]       = SCARAB_REG_RDI;
-  reg_compress_map[(int)REG_GDI]       = SCARAB_REG_RDI;
-  reg_compress_map[(int)REG_ESI]       = SCARAB_REG_RSI;
-  reg_compress_map[(int)REG_GSI]       = SCARAB_REG_RSI;
-  reg_compress_map[(int)REG_EBP]       = SCARAB_REG_RBP;
-  reg_compress_map[(int)REG_GBP]       = SCARAB_REG_RBP;
-  reg_compress_map[(int)REG_ESP]       = SCARAB_REG_RSP;
-  reg_compress_map[(int)REG_STACK_PTR] = SCARAB_REG_RSP;
-  reg_compress_map[(int)REG_EBX]       = SCARAB_REG_RBX;
-  reg_compress_map[(int)REG_GBX]       = SCARAB_REG_RBX;
-  reg_compress_map[(int)REG_EDX]       = SCARAB_REG_RDX;
-  reg_compress_map[(int)REG_GDX]       = SCARAB_REG_RDX;
-  reg_compress_map[(int)REG_ECX]       = SCARAB_REG_RCX;
-  reg_compress_map[(int)REG_GCX]       = SCARAB_REG_RCX;
-  reg_compress_map[(int)REG_EAX]       = SCARAB_REG_RAX;
-  reg_compress_map[(int)REG_GAX]       = SCARAB_REG_RAX;
-  reg_compress_map[(int)REG_GR_LAST]   = SCARAB_REG_RAX;
-  reg_compress_map[(int)REG_SEG_BASE]  = SCARAB_REG_CS;
-  reg_compress_map[(int)REG_SEG_CS]    = SCARAB_REG_CS;
-  reg_compress_map[(int)REG_SEG_SS]    = SCARAB_REG_SS;
-  reg_compress_map[(int)REG_SEG_DS]    = SCARAB_REG_DS;
-  reg_compress_map[(int)REG_SEG_ES]    = SCARAB_REG_ES;
-  reg_compress_map[(int)REG_SEG_FS]    = SCARAB_REG_FS;
-  reg_compress_map[(int)REG_SEG_GS]    = SCARAB_REG_GS;
-  reg_compress_map[(int)REG_SEG_LAST]  = SCARAB_REG_GS;
-  // Treating any flag dependency as ZPS because we could not
-  // get finer grain dependicies from PIN
-  reg_compress_map[(int)REG_EFLAGS]   = SCARAB_REG_ZPS;
-  reg_compress_map[(int)REG_GFLAGS]   = SCARAB_REG_ZPS;
-  reg_compress_map[(int)REG_EIP]      = SCARAB_REG_RIP;
-  reg_compress_map[(int)REG_INST_PTR] = SCARAB_REG_RIP;
-  reg_compress_map[(int)REG_AL]       = SCARAB_REG_RAX;
-  reg_compress_map[(int)REG_AH]       = SCARAB_REG_RAX;
-  reg_compress_map[(int)REG_AX]       = SCARAB_REG_RAX;
-  reg_compress_map[(int)REG_CL]       = SCARAB_REG_RCX;
-  reg_compress_map[(int)REG_CH]       = SCARAB_REG_RCX;
-  reg_compress_map[(int)REG_CX]       = SCARAB_REG_RCX;
-  reg_compress_map[(int)REG_DL]       = SCARAB_REG_RDX;
-  reg_compress_map[(int)REG_DH]       = SCARAB_REG_RDX;
-  reg_compress_map[(int)REG_DX]       = SCARAB_REG_RDX;
-  reg_compress_map[(int)REG_BL]       = SCARAB_REG_RBX;
-  reg_compress_map[(int)REG_BH]       = SCARAB_REG_RBX;
-  reg_compress_map[(int)REG_BX]       = SCARAB_REG_RBX;
-  reg_compress_map[(int)REG_BP]       = SCARAB_REG_RBX;
-  reg_compress_map[(int)REG_SI]       = SCARAB_REG_RSI;
-  reg_compress_map[(int)REG_DI]       = SCARAB_REG_RDI;
-  reg_compress_map[(int)REG_SP]       = SCARAB_REG_RSP;
-  reg_compress_map[(int)REG_FLAGS]    = SCARAB_REG_ZPS;
-  reg_compress_map[(int)REG_IP]       = SCARAB_REG_RIP;
-  reg_compress_map[(int)REG_MM_BASE]  = SCARAB_REG_ZMM0;
-  reg_compress_map[(int)REG_MM0]      = SCARAB_REG_ZMM0;
-  reg_compress_map[(int)REG_MM1]      = SCARAB_REG_ZMM1;
-  reg_compress_map[(int)REG_MM2]      = SCARAB_REG_ZMM2;
-  reg_compress_map[(int)REG_MM3]      = SCARAB_REG_ZMM3;
-  reg_compress_map[(int)REG_MM4]      = SCARAB_REG_ZMM4;
-  reg_compress_map[(int)REG_MM5]      = SCARAB_REG_ZMM5;
-  reg_compress_map[(int)REG_MM6]      = SCARAB_REG_ZMM6;
-  reg_compress_map[(int)REG_MM7]      = SCARAB_REG_ZMM7;
-  reg_compress_map[(int)REG_MM_LAST]  = SCARAB_REG_ZMM7;
-
-  reg_compress_map[(int)REG_XMM_BASE]        = SCARAB_REG_ZMM0;
-  reg_compress_map[(int)REG_XMM0]            = SCARAB_REG_ZMM0;
-  reg_compress_map[(int)REG_XMM1]            = SCARAB_REG_ZMM1;
-  reg_compress_map[(int)REG_XMM2]            = SCARAB_REG_ZMM2;
-  reg_compress_map[(int)REG_XMM3]            = SCARAB_REG_ZMM3;
-  reg_compress_map[(int)REG_XMM4]            = SCARAB_REG_ZMM4;
-  reg_compress_map[(int)REG_XMM5]            = SCARAB_REG_ZMM5;
-  reg_compress_map[(int)REG_XMM6]            = SCARAB_REG_ZMM6;
-  reg_compress_map[(int)REG_XMM7]            = SCARAB_REG_ZMM7;
-  reg_compress_map[(int)REG_XMM_SSE_LAST]    = SCARAB_REG_ZMM7;
-  reg_compress_map[(int)REG_XMM_AVX_LAST]    = SCARAB_REG_ZMM7;
-  reg_compress_map[(int)REG_XMM_AVX512_LAST] = SCARAB_REG_ZMM7;
-  reg_compress_map[(int)REG_XMM_LAST]        = SCARAB_REG_ZMM7;
-
-  reg_compress_map[(int)REG_YMM_BASE]        = SCARAB_REG_ZMM0;
-  reg_compress_map[(int)REG_YMM0]            = SCARAB_REG_ZMM0;
-  reg_compress_map[(int)REG_YMM1]            = SCARAB_REG_ZMM1;
-  reg_compress_map[(int)REG_YMM2]            = SCARAB_REG_ZMM2;
-  reg_compress_map[(int)REG_YMM3]            = SCARAB_REG_ZMM3;
-  reg_compress_map[(int)REG_YMM4]            = SCARAB_REG_ZMM4;
-  reg_compress_map[(int)REG_YMM5]            = SCARAB_REG_ZMM5;
-  reg_compress_map[(int)REG_YMM6]            = SCARAB_REG_ZMM6;
-  reg_compress_map[(int)REG_YMM7]            = SCARAB_REG_ZMM7;
-  reg_compress_map[(int)REG_YMM_AVX_LAST]    = SCARAB_REG_ZMM7;
-  reg_compress_map[(int)REG_YMM_AVX512_LAST] = SCARAB_REG_ZMM7;
-  reg_compress_map[(int)REG_YMM_LAST]        = SCARAB_REG_ZMM7;
-
-  reg_compress_map[(int)REG_ZMM_BASE]              = SCARAB_REG_ZMM0;
-  reg_compress_map[(int)REG_ZMM0]                  = SCARAB_REG_ZMM0;
-  reg_compress_map[(int)REG_ZMM1]                  = SCARAB_REG_ZMM1;
-  reg_compress_map[(int)REG_ZMM2]                  = SCARAB_REG_ZMM2;
-  reg_compress_map[(int)REG_ZMM3]                  = SCARAB_REG_ZMM3;
-  reg_compress_map[(int)REG_ZMM4]                  = SCARAB_REG_ZMM4;
-  reg_compress_map[(int)REG_ZMM5]                  = SCARAB_REG_ZMM5;
-  reg_compress_map[(int)REG_ZMM6]                  = SCARAB_REG_ZMM6;
-  reg_compress_map[(int)REG_ZMM7]                  = SCARAB_REG_ZMM7;
-  reg_compress_map[(int)REG_ZMM_AVX512_SPLIT_LAST] = SCARAB_REG_ZMM7;
-  reg_compress_map[(int)REG_ZMM_AVX512_LAST]       = SCARAB_REG_ZMM7;
-  reg_compress_map[(int)REG_ZMM_LAST]              = SCARAB_REG_ZMM7;
-
-  reg_compress_map[(int)REG_K_BASE]             = SCARAB_REG_K0;
-  reg_compress_map[(int)REG_IMPLICIT_FULL_MASK] = SCARAB_REG_K0;
-  reg_compress_map[(int)REG_K0]                 = SCARAB_REG_K0;
-  reg_compress_map[(int)REG_K1]                 = SCARAB_REG_K1;
-  reg_compress_map[(int)REG_K2]                 = SCARAB_REG_K2;
-  reg_compress_map[(int)REG_K3]                 = SCARAB_REG_K3;
-  reg_compress_map[(int)REG_K4]                 = SCARAB_REG_K4;
-  reg_compress_map[(int)REG_K5]                 = SCARAB_REG_K5;
-  reg_compress_map[(int)REG_K6]                 = SCARAB_REG_K6;
-  reg_compress_map[(int)REG_K7]                 = SCARAB_REG_K7;
-  reg_compress_map[(int)REG_K_LAST]             = SCARAB_REG_K7;
-
-  reg_compress_map[(int)REG_MXCSR]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_MXCSRMASK] = SCARAB_REG_OTHER;
-
-  reg_compress_map[(int)REG_FPST_BASE]     = SCARAB_REG_FPCW;
-  reg_compress_map[(int)REG_FPSTATUS_BASE] = SCARAB_REG_FPCW;
-  reg_compress_map[(int)REG_FPCW]          = SCARAB_REG_FPCW;
-  reg_compress_map[(int)REG_FPSW]          = SCARAB_REG_FPST;
-  reg_compress_map[(int)REG_FPTAG]         = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_FPIP_OFF]      = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_FPIP_SEL]      = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_FPOPCODE]      = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_FPDP_OFF]      = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_FPDP_SEL]      = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_FPSTATUS_LAST] = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_FPTAG_FULL]    = SCARAB_REG_OTHER;
-
-  reg_compress_map[(int)REG_ST_BASE]   = SCARAB_REG_FP0;
-  reg_compress_map[(int)REG_ST0]       = SCARAB_REG_FP0;
-  reg_compress_map[(int)REG_ST1]       = SCARAB_REG_FP1;
-  reg_compress_map[(int)REG_ST2]       = SCARAB_REG_FP2;
-  reg_compress_map[(int)REG_ST3]       = SCARAB_REG_FP3;
-  reg_compress_map[(int)REG_ST4]       = SCARAB_REG_FP4;
-  reg_compress_map[(int)REG_ST5]       = SCARAB_REG_FP5;
-  reg_compress_map[(int)REG_ST6]       = SCARAB_REG_FP6;
-  reg_compress_map[(int)REG_ST7]       = SCARAB_REG_FP7;
-  reg_compress_map[(int)REG_ST_LAST]   = SCARAB_REG_FP7;
-  reg_compress_map[(int)REG_FPST_LAST] = SCARAB_REG_FP7;
-
-  reg_compress_map[(int)REG_DR_BASE] = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_DR0]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_DR1]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_DR2]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_DR3]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_DR4]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_DR5]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_DR6]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_DR7]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_DR_LAST] = SCARAB_REG_OTHER;
-
-  reg_compress_map[(int)REG_CR_BASE] = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_CR0]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_CR1]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_CR2]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_CR3]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_CR4]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_CR_LAST] = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_TSSR]    = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_LDTR]    = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_TR_BASE] = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_TR]      = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_TR3]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_TR4]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_TR5]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_TR6]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_TR7]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)REG_TR_LAST] = SCARAB_REG_OTHER;
+  reg_compress_map_pin[0]                  = 0;
+  reg_compress_map_pin[(int)REG_GR_BASE]   = SCARAB_REG_RDI;
+  reg_compress_map_pin[(int)REG_EDI]       = SCARAB_REG_RDI;
+  reg_compress_map_pin[(int)REG_GDI]       = SCARAB_REG_RDI;
+  reg_compress_map_pin[(int)REG_ESI]       = SCARAB_REG_RSI;
+  reg_compress_map_pin[(int)REG_GSI]       = SCARAB_REG_RSI;
+  reg_compress_map_pin[(int)REG_EBP]       = SCARAB_REG_RBP;
+  reg_compress_map_pin[(int)REG_GBP]       = SCARAB_REG_RBP;
+  reg_compress_map_pin[(int)REG_ESP]       = SCARAB_REG_RSP;
+  reg_compress_map_pin[(int)REG_STACK_PTR] = SCARAB_REG_RSP;
+  reg_compress_map_pin[(int)REG_EBX]       = SCARAB_REG_RBX;
+  reg_compress_map_pin[(int)REG_GBX]       = SCARAB_REG_RBX;
+  reg_compress_map_pin[(int)REG_EDX]       = SCARAB_REG_RDX;
+  reg_compress_map_pin[(int)REG_GDX]       = SCARAB_REG_RDX;
+  reg_compress_map_pin[(int)REG_ECX]       = SCARAB_REG_RCX;
+  reg_compress_map_pin[(int)REG_GCX]       = SCARAB_REG_RCX;
+  reg_compress_map_pin[(int)REG_EAX]       = SCARAB_REG_RAX;
+  reg_compress_map_pin[(int)REG_GAX]       = SCARAB_REG_RAX;
+  reg_compress_map_pin[(int)REG_GR_LAST]   = SCARAB_REG_RAX;
+  reg_compress_map_pin[(int)REG_SEG_BASE]  = SCARAB_REG_CS;
+  reg_compress_map_pin[(int)REG_SEG_CS]    = SCARAB_REG_CS;
+  reg_compress_map_pin[(int)REG_SEG_SS]    = SCARAB_REG_SS;
+  reg_compress_map_pin[(int)REG_SEG_DS]    = SCARAB_REG_DS;
+  reg_compress_map_pin[(int)REG_SEG_ES]    = SCARAB_REG_ES;
+  reg_compress_map_pin[(int)REG_SEG_FS]    = SCARAB_REG_FS;
+  reg_compress_map_pin[(int)REG_SEG_GS]    = SCARAB_REG_GS;
+  reg_compress_map_pin[(int)REG_SEG_LAST]  = SCARAB_REG_GS;
+  // Treating any _pinflag dependency as ZPS because we could not
+  // get finer gra_pinin dependicies from PIN
+  reg_compress_map_pin[(int)REG_EFLAGS]   = SCARAB_REG_ZPS;
+  reg_compress_map_pin[(int)REG_GFLAGS]   = SCARAB_REG_ZPS;
+  reg_compress_map_pin[(int)REG_EIP]      = SCARAB_REG_RIP;
+  reg_compress_map_pin[(int)REG_INST_PTR] = SCARAB_REG_RIP;
+  reg_compress_map_pin[(int)REG_AL]       = SCARAB_REG_RAX;
+  reg_compress_map_pin[(int)REG_AH]       = SCARAB_REG_RAX;
+  reg_compress_map_pin[(int)REG_AX]       = SCARAB_REG_RAX;
+  reg_compress_map_pin[(int)REG_CL]       = SCARAB_REG_RCX;
+  reg_compress_map_pin[(int)REG_CH]       = SCARAB_REG_RCX;
+  reg_compress_map_pin[(int)REG_CX]       = SCARAB_REG_RCX;
+  reg_compress_map_pin[(int)REG_DL]       = SCARAB_REG_RDX;
+  reg_compress_map_pin[(int)REG_DH]       = SCARAB_REG_RDX;
+  reg_compress_map_pin[(int)REG_DX]       = SCARAB_REG_RDX;
+  reg_compress_map_pin[(int)REG_BL]       = SCARAB_REG_RBX;
+  reg_compress_map_pin[(int)REG_BH]       = SCARAB_REG_RBX;
+  reg_compress_map_pin[(int)REG_BX]       = SCARAB_REG_RBX;
+  reg_compress_map_pin[(int)REG_BP]       = SCARAB_REG_RBX;
+  reg_compress_map_pin[(int)REG_SI]       = SCARAB_REG_RSI;
+  reg_compress_map_pin[(int)REG_DI]       = SCARAB_REG_RDI;
+  reg_compress_map_pin[(int)REG_SP]       = SCARAB_REG_RSP;
+  reg_compress_map_pin[(int)REG_FLAGS]    = SCARAB_REG_ZPS;
+  reg_compress_map_pin[(int)REG_IP]       = SCARAB_REG_RIP;
+  reg_compress_map_pin[(int)REG_MM_BASE]  = SCARAB_REG_ZMM0;
+  reg_compress_map_pin[(int)REG_MM0]      = SCARAB_REG_ZMM0;
+  reg_compress_map_pin[(int)REG_MM1]      = SCARAB_REG_ZMM1;
+  reg_compress_map_pin[(int)REG_MM2]      = SCARAB_REG_ZMM2;
+  reg_compress_map_pin[(int)REG_MM3]      = SCARAB_REG_ZMM3;
+  reg_compress_map_pin[(int)REG_MM4]      = SCARAB_REG_ZMM4;
+  reg_compress_map_pin[(int)REG_MM5]      = SCARAB_REG_ZMM5;
+  reg_compress_map_pin[(int)REG_MM6]      = SCARAB_REG_ZMM6;
+  reg_compress_map_pin[(int)REG_MM7]      = SCARAB_REG_ZMM7;
+  reg_compress_map_pin[(int)REG_MM_LAST]  = SCARAB_REG_ZMM7;
+
+  reg_compress_map_pin[(int)REG_XMM_BASE]        = SCARAB_REG_ZMM0;
+  reg_compress_map_pin[(int)REG_XMM0]            = SCARAB_REG_ZMM0;
+  reg_compress_map_pin[(int)REG_XMM1]            = SCARAB_REG_ZMM1;
+  reg_compress_map_pin[(int)REG_XMM2]            = SCARAB_REG_ZMM2;
+  reg_compress_map_pin[(int)REG_XMM3]            = SCARAB_REG_ZMM3;
+  reg_compress_map_pin[(int)REG_XMM4]            = SCARAB_REG_ZMM4;
+  reg_compress_map_pin[(int)REG_XMM5]            = SCARAB_REG_ZMM5;
+  reg_compress_map_pin[(int)REG_XMM6]            = SCARAB_REG_ZMM6;
+  reg_compress_map_pin[(int)REG_XMM7]            = SCARAB_REG_ZMM7;
+  reg_compress_map_pin[(int)REG_XMM_SSE_LAST]    = SCARAB_REG_ZMM7;
+  reg_compress_map_pin[(int)REG_XMM_AVX_LAST]    = SCARAB_REG_ZMM7;
+  reg_compress_map_pin[(int)REG_XMM_AVX512_LAST] = SCARAB_REG_ZMM7;
+  reg_compress_map_pin[(int)REG_XMM_LAST]        = SCARAB_REG_ZMM7;
+
+  reg_compress_map_pin[(int)REG_YMM_BASE]        = SCARAB_REG_ZMM0;
+  reg_compress_map_pin[(int)REG_YMM0]            = SCARAB_REG_ZMM0;
+  reg_compress_map_pin[(int)REG_YMM1]            = SCARAB_REG_ZMM1;
+  reg_compress_map_pin[(int)REG_YMM2]            = SCARAB_REG_ZMM2;
+  reg_compress_map_pin[(int)REG_YMM3]            = SCARAB_REG_ZMM3;
+  reg_compress_map_pin[(int)REG_YMM4]            = SCARAB_REG_ZMM4;
+  reg_compress_map_pin[(int)REG_YMM5]            = SCARAB_REG_ZMM5;
+  reg_compress_map_pin[(int)REG_YMM6]            = SCARAB_REG_ZMM6;
+  reg_compress_map_pin[(int)REG_YMM7]            = SCARAB_REG_ZMM7;
+  reg_compress_map_pin[(int)REG_YMM_AVX_LAST]    = SCARAB_REG_ZMM7;
+  reg_compress_map_pin[(int)REG_YMM_AVX512_LAST] = SCARAB_REG_ZMM7;
+  reg_compress_map_pin[(int)REG_YMM_LAST]        = SCARAB_REG_ZMM7;
+
+  reg_compress_map_pin[(int)REG_ZMM_BASE]              = SCARAB_REG_ZMM0;
+  reg_compress_map_pin[(int)REG_ZMM0]                  = SCARAB_REG_ZMM0;
+  reg_compress_map_pin[(int)REG_ZMM1]                  = SCARAB_REG_ZMM1;
+  reg_compress_map_pin[(int)REG_ZMM2]                  = SCARAB_REG_ZMM2;
+  reg_compress_map_pin[(int)REG_ZMM3]                  = SCARAB_REG_ZMM3;
+  reg_compress_map_pin[(int)REG_ZMM4]                  = SCARAB_REG_ZMM4;
+  reg_compress_map_pin[(int)REG_ZMM5]                  = SCARAB_REG_ZMM5;
+  reg_compress_map_pin[(int)REG_ZMM6]                  = SCARAB_REG_ZMM6;
+  reg_compress_map_pin[(int)REG_ZMM7]                  = SCARAB_REG_ZMM7;
+  reg_compress_map_pin[(int)REG_ZMM_AVX512_SPLIT_LAST] = SCARAB_REG_ZMM7;
+  reg_compress_map_pin[(int)REG_ZMM_AVX512_LAST]       = SCARAB_REG_ZMM7;
+  reg_compress_map_pin[(int)REG_ZMM_LAST]              = SCARAB_REG_ZMM7;
+
+  reg_compress_map_pin[(int)REG_K_BASE]             = SCARAB_REG_K0;
+  reg_compress_map_pin[(int)REG_IMPLICIT_FULL_MASK] = SCARAB_REG_K0;
+  reg_compress_map_pin[(int)REG_K0]                 = SCARAB_REG_K0;
+  reg_compress_map_pin[(int)REG_K1]                 = SCARAB_REG_K1;
+  reg_compress_map_pin[(int)REG_K2]                 = SCARAB_REG_K2;
+  reg_compress_map_pin[(int)REG_K3]                 = SCARAB_REG_K3;
+  reg_compress_map_pin[(int)REG_K4]                 = SCARAB_REG_K4;
+  reg_compress_map_pin[(int)REG_K5]                 = SCARAB_REG_K5;
+  reg_compress_map_pin[(int)REG_K6]                 = SCARAB_REG_K6;
+  reg_compress_map_pin[(int)REG_K7]                 = SCARAB_REG_K7;
+  reg_compress_map_pin[(int)REG_K_LAST]             = SCARAB_REG_K7;
+
+  reg_compress_map_pin[(int)REG_MXCSR]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_MXCSRMASK] = SCARAB_REG_OTHER;
+
+  reg_compress_map_pin[(int)REG_FPST_BASE]     = SCARAB_REG_FPCW;
+  reg_compress_map_pin[(int)REG_FPSTATUS_BASE] = SCARAB_REG_FPCW;
+  reg_compress_map_pin[(int)REG_FPCW]          = SCARAB_REG_FPCW;
+  reg_compress_map_pin[(int)REG_FPSW]          = SCARAB_REG_FPST;
+  reg_compress_map_pin[(int)REG_FPTAG]         = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_FPIP_OFF]      = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_FPIP_SEL]      = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_FPOPCODE]      = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_FPDP_OFF]      = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_FPDP_SEL]      = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_FPSTATUS_LAST] = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_FPTAG_FULL]    = SCARAB_REG_OTHER;
+
+  reg_compress_map_pin[(int)REG_ST_BASE]   = SCARAB_REG_FP0;
+  reg_compress_map_pin[(int)REG_ST0]       = SCARAB_REG_FP0;
+  reg_compress_map_pin[(int)REG_ST1]       = SCARAB_REG_FP1;
+  reg_compress_map_pin[(int)REG_ST2]       = SCARAB_REG_FP2;
+  reg_compress_map_pin[(int)REG_ST3]       = SCARAB_REG_FP3;
+  reg_compress_map_pin[(int)REG_ST4]       = SCARAB_REG_FP4;
+  reg_compress_map_pin[(int)REG_ST5]       = SCARAB_REG_FP5;
+  reg_compress_map_pin[(int)REG_ST6]       = SCARAB_REG_FP6;
+  reg_compress_map_pin[(int)REG_ST7]       = SCARAB_REG_FP7;
+  reg_compress_map_pin[(int)REG_ST_LAST]   = SCARAB_REG_FP7;
+  reg_compress_map_pin[(int)REG_FPST_LAST] = SCARAB_REG_FP7;
+
+  reg_compress_map_pin[(int)REG_DR_BASE] = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_DR0]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_DR1]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_DR2]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_DR3]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_DR4]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_DR5]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_DR6]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_DR7]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_DR_LAST] = SCARAB_REG_OTHER;
+
+  reg_compress_map_pin[(int)REG_CR_BASE] = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_CR0]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_CR1]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_CR2]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_CR3]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_CR4]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_CR_LAST] = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_TSSR]    = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_LDTR]    = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_TR_BASE] = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_TR]      = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_TR3]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_TR4]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_TR5]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_TR6]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_TR7]     = SCARAB_REG_OTHER;
+  reg_compress_map_pin[(int)REG_TR_LAST] = SCARAB_REG_OTHER;
 
 #if defined(TARGET_IA32E)
-  reg_compress_map[(int)REG_RDI]     = SCARAB_REG_RDI;
-  reg_compress_map[(int)REG_RSI]     = SCARAB_REG_RSI;
-  reg_compress_map[(int)REG_RBP]     = SCARAB_REG_RBP;
-  reg_compress_map[(int)REG_RSP]     = SCARAB_REG_RSP;
-  reg_compress_map[(int)REG_RBX]     = SCARAB_REG_RBX;
-  reg_compress_map[(int)REG_RDX]     = SCARAB_REG_RDX;
-  reg_compress_map[(int)REG_RCX]     = SCARAB_REG_RCX;
-  reg_compress_map[(int)REG_RAX]     = SCARAB_REG_RAX;
-  reg_compress_map[(int)REG_R8]      = SCARAB_REG_R8;
-  reg_compress_map[(int)REG_R9]      = SCARAB_REG_R9;
-  reg_compress_map[(int)REG_R10]     = SCARAB_REG_R10;
-  reg_compress_map[(int)REG_R11]     = SCARAB_REG_R11;
-  reg_compress_map[(int)REG_R12]     = SCARAB_REG_R12;
-  reg_compress_map[(int)REG_R13]     = SCARAB_REG_R13;
-  reg_compress_map[(int)REG_R14]     = SCARAB_REG_R14;
-  reg_compress_map[(int)REG_R15]     = SCARAB_REG_R15;
-  reg_compress_map[(int)REG_GR_LAST] = SCARAB_REG_R15;
-  reg_compress_map[(int)REG_RFLAGS]  = SCARAB_REG_ZPS;
-  reg_compress_map[(int)REG_RIP]     = SCARAB_REG_RIP;
-
-  reg_compress_map[(int)REG_DIL]  = SCARAB_REG_RDI;
-  reg_compress_map[(int)REG_SIL]  = SCARAB_REG_RSI;
-  reg_compress_map[(int)REG_BPL]  = SCARAB_REG_RBP;
-  reg_compress_map[(int)REG_SPL]  = SCARAB_REG_RSP;
-  reg_compress_map[(int)REG_R8B]  = SCARAB_REG_R8;
-  reg_compress_map[(int)REG_R8W]  = SCARAB_REG_R8;
-  reg_compress_map[(int)REG_R8D]  = SCARAB_REG_R8;
-  reg_compress_map[(int)REG_R9B]  = SCARAB_REG_R9;
-  reg_compress_map[(int)REG_R9W]  = SCARAB_REG_R9;
-  reg_compress_map[(int)REG_R9D]  = SCARAB_REG_R9;
-  reg_compress_map[(int)REG_R10B] = SCARAB_REG_R10;
-  reg_compress_map[(int)REG_R10W] = SCARAB_REG_R10;
-  reg_compress_map[(int)REG_R10D] = SCARAB_REG_R10;
-  reg_compress_map[(int)REG_R11B] = SCARAB_REG_R11;
-  reg_compress_map[(int)REG_R11W] = SCARAB_REG_R11;
-  reg_compress_map[(int)REG_R11D] = SCARAB_REG_R11;
-  reg_compress_map[(int)REG_R12B] = SCARAB_REG_R12;
-  reg_compress_map[(int)REG_R12W] = SCARAB_REG_R12;
-  reg_compress_map[(int)REG_R12D] = SCARAB_REG_R12;
-  reg_compress_map[(int)REG_R13B] = SCARAB_REG_R13;
-  reg_compress_map[(int)REG_R13W] = SCARAB_REG_R13;
-  reg_compress_map[(int)REG_R13D] = SCARAB_REG_R13;
-  reg_compress_map[(int)REG_R14B] = SCARAB_REG_R14;
-  reg_compress_map[(int)REG_R14W] = SCARAB_REG_R14;
-  reg_compress_map[(int)REG_R14D] = SCARAB_REG_R14;
-  reg_compress_map[(int)REG_R15B] = SCARAB_REG_R15;
-  reg_compress_map[(int)REG_R15W] = SCARAB_REG_R15;
-  reg_compress_map[(int)REG_R15D] = SCARAB_REG_R15;
-
-  reg_compress_map[(int)REG_XMM8]                  = SCARAB_REG_ZMM8;
-  reg_compress_map[(int)REG_XMM9]                  = SCARAB_REG_ZMM9;
-  reg_compress_map[(int)REG_XMM10]                 = SCARAB_REG_ZMM10;
-  reg_compress_map[(int)REG_XMM11]                 = SCARAB_REG_ZMM11;
-  reg_compress_map[(int)REG_XMM12]                 = SCARAB_REG_ZMM12;
-  reg_compress_map[(int)REG_XMM13]                 = SCARAB_REG_ZMM13;
-  reg_compress_map[(int)REG_XMM14]                 = SCARAB_REG_ZMM14;
-  reg_compress_map[(int)REG_XMM15]                 = SCARAB_REG_ZMM15;
-  reg_compress_map[(int)REG_XMM_SSE_LAST]          = SCARAB_REG_ZMM15;
-  reg_compress_map[(int)REG_XMM_AVX_LAST]          = SCARAB_REG_ZMM15;
-  reg_compress_map[(int)REG_XMM16]                 = SCARAB_REG_ZMM16;
-  reg_compress_map[(int)REG_XMM_AVX512_HI16_FIRST] = SCARAB_REG_ZMM16;
-  reg_compress_map[(int)REG_XMM17]                 = SCARAB_REG_ZMM17;
-  reg_compress_map[(int)REG_XMM18]                 = SCARAB_REG_ZMM18;
-  reg_compress_map[(int)REG_XMM19]                 = SCARAB_REG_ZMM19;
-  reg_compress_map[(int)REG_XMM20]                 = SCARAB_REG_ZMM20;
-  reg_compress_map[(int)REG_XMM21]                 = SCARAB_REG_ZMM21;
-  reg_compress_map[(int)REG_XMM22]                 = SCARAB_REG_ZMM22;
-  reg_compress_map[(int)REG_XMM23]                 = SCARAB_REG_ZMM23;
-  reg_compress_map[(int)REG_XMM24]                 = SCARAB_REG_ZMM24;
-  reg_compress_map[(int)REG_XMM25]                 = SCARAB_REG_ZMM25;
-  reg_compress_map[(int)REG_XMM26]                 = SCARAB_REG_ZMM26;
-  reg_compress_map[(int)REG_XMM27]                 = SCARAB_REG_ZMM27;
-  reg_compress_map[(int)REG_XMM28]                 = SCARAB_REG_ZMM28;
-  reg_compress_map[(int)REG_XMM29]                 = SCARAB_REG_ZMM29;
-  reg_compress_map[(int)REG_XMM30]                 = SCARAB_REG_ZMM30;
-  reg_compress_map[(int)REG_XMM31]                 = SCARAB_REG_ZMM31;
-  reg_compress_map[(int)REG_XMM_AVX512_HI16_LAST]  = SCARAB_REG_ZMM31;
-  reg_compress_map[(int)REG_XMM_AVX512_LAST]       = SCARAB_REG_ZMM31;
-  reg_compress_map[(int)REG_XMM_LAST]              = SCARAB_REG_ZMM31;
-
-  reg_compress_map[(int)REG_YMM8]                  = SCARAB_REG_ZMM8;
-  reg_compress_map[(int)REG_YMM9]                  = SCARAB_REG_ZMM9;
-  reg_compress_map[(int)REG_YMM10]                 = SCARAB_REG_ZMM10;
-  reg_compress_map[(int)REG_YMM11]                 = SCARAB_REG_ZMM11;
-  reg_compress_map[(int)REG_YMM12]                 = SCARAB_REG_ZMM12;
-  reg_compress_map[(int)REG_YMM13]                 = SCARAB_REG_ZMM13;
-  reg_compress_map[(int)REG_YMM14]                 = SCARAB_REG_ZMM14;
-  reg_compress_map[(int)REG_YMM15]                 = SCARAB_REG_ZMM15;
-  reg_compress_map[(int)REG_YMM_AVX_LAST]          = SCARAB_REG_ZMM15;
-  reg_compress_map[(int)REG_YMM16]                 = SCARAB_REG_ZMM16;
-  reg_compress_map[(int)REG_YMM_AVX512_HI16_FIRST] = SCARAB_REG_ZMM16;
-  reg_compress_map[(int)REG_YMM17]                 = SCARAB_REG_ZMM17;
-  reg_compress_map[(int)REG_YMM18]                 = SCARAB_REG_ZMM18;
-  reg_compress_map[(int)REG_YMM19]                 = SCARAB_REG_ZMM19;
-  reg_compress_map[(int)REG_YMM20]                 = SCARAB_REG_ZMM20;
-  reg_compress_map[(int)REG_YMM21]                 = SCARAB_REG_ZMM21;
-  reg_compress_map[(int)REG_YMM22]                 = SCARAB_REG_ZMM22;
-  reg_compress_map[(int)REG_YMM23]                 = SCARAB_REG_ZMM23;
-  reg_compress_map[(int)REG_YMM24]                 = SCARAB_REG_ZMM24;
-  reg_compress_map[(int)REG_YMM25]                 = SCARAB_REG_ZMM25;
-  reg_compress_map[(int)REG_YMM26]                 = SCARAB_REG_ZMM26;
-  reg_compress_map[(int)REG_YMM27]                 = SCARAB_REG_ZMM27;
-  reg_compress_map[(int)REG_YMM28]                 = SCARAB_REG_ZMM28;
-  reg_compress_map[(int)REG_YMM29]                 = SCARAB_REG_ZMM29;
-  reg_compress_map[(int)REG_YMM30]                 = SCARAB_REG_ZMM30;
-  reg_compress_map[(int)REG_YMM31]                 = SCARAB_REG_ZMM31;
-  reg_compress_map[(int)REG_YMM_AVX512_HI16_LAST]  = SCARAB_REG_ZMM31;
-  reg_compress_map[(int)REG_YMM_AVX512_LAST]       = SCARAB_REG_ZMM31;
-  reg_compress_map[(int)REG_YMM_LAST]              = SCARAB_REG_ZMM31;
-
-  reg_compress_map[(int)REG_ZMM8]                  = SCARAB_REG_ZMM8;
-  reg_compress_map[(int)REG_ZMM9]                  = SCARAB_REG_ZMM9;
-  reg_compress_map[(int)REG_ZMM10]                 = SCARAB_REG_ZMM10;
-  reg_compress_map[(int)REG_ZMM11]                 = SCARAB_REG_ZMM11;
-  reg_compress_map[(int)REG_ZMM12]                 = SCARAB_REG_ZMM12;
-  reg_compress_map[(int)REG_ZMM13]                 = SCARAB_REG_ZMM13;
-  reg_compress_map[(int)REG_ZMM14]                 = SCARAB_REG_ZMM14;
-  reg_compress_map[(int)REG_ZMM15]                 = SCARAB_REG_ZMM15;
-  reg_compress_map[(int)REG_ZMM_AVX512_SPLIT_LAST] = SCARAB_REG_ZMM15;
-  reg_compress_map[(int)REG_ZMM16]                 = SCARAB_REG_ZMM16;
-  reg_compress_map[(int)REG_ZMM_AVX512_HI16_FIRST] = SCARAB_REG_ZMM16;
-  reg_compress_map[(int)REG_ZMM17]                 = SCARAB_REG_ZMM17;
-  reg_compress_map[(int)REG_ZMM18]                 = SCARAB_REG_ZMM18;
-  reg_compress_map[(int)REG_ZMM19]                 = SCARAB_REG_ZMM19;
-  reg_compress_map[(int)REG_ZMM20]                 = SCARAB_REG_ZMM20;
-  reg_compress_map[(int)REG_ZMM21]                 = SCARAB_REG_ZMM21;
-  reg_compress_map[(int)REG_ZMM22]                 = SCARAB_REG_ZMM22;
-  reg_compress_map[(int)REG_ZMM23]                 = SCARAB_REG_ZMM23;
-  reg_compress_map[(int)REG_ZMM24]                 = SCARAB_REG_ZMM24;
-  reg_compress_map[(int)REG_ZMM25]                 = SCARAB_REG_ZMM25;
-  reg_compress_map[(int)REG_ZMM26]                 = SCARAB_REG_ZMM26;
-  reg_compress_map[(int)REG_ZMM27]                 = SCARAB_REG_ZMM27;
-  reg_compress_map[(int)REG_ZMM28]                 = SCARAB_REG_ZMM28;
-  reg_compress_map[(int)REG_ZMM29]                 = SCARAB_REG_ZMM29;
-  reg_compress_map[(int)REG_ZMM30]                 = SCARAB_REG_ZMM30;
-  reg_compress_map[(int)REG_ZMM31]                 = SCARAB_REG_ZMM31;
-  reg_compress_map[(int)REG_ZMM_AVX512_HI16_LAST]  = SCARAB_REG_ZMM31;
-  reg_compress_map[(int)REG_ZMM_AVX512_LAST]       = SCARAB_REG_ZMM31;
-  reg_compress_map[(int)REG_ZMM_LAST]              = SCARAB_REG_ZMM31;
+  reg_compress_map_pin[(int)REG_RDI]     = SCARAB_REG_RDI;
+  reg_compress_map_pin[(int)REG_RSI]     = SCARAB_REG_RSI;
+  reg_compress_map_pin[(int)REG_RBP]     = SCARAB_REG_RBP;
+  reg_compress_map_pin[(int)REG_RSP]     = SCARAB_REG_RSP;
+  reg_compress_map_pin[(int)REG_RBX]     = SCARAB_REG_RBX;
+  reg_compress_map_pin[(int)REG_RDX]     = SCARAB_REG_RDX;
+  reg_compress_map_pin[(int)REG_RCX]     = SCARAB_REG_RCX;
+  reg_compress_map_pin[(int)REG_RAX]     = SCARAB_REG_RAX;
+  reg_compress_map_pin[(int)REG_R8]      = SCARAB_REG_R8;
+  reg_compress_map_pin[(int)REG_R9]      = SCARAB_REG_R9;
+  reg_compress_map_pin[(int)REG_R10]     = SCARAB_REG_R10;
+  reg_compress_map_pin[(int)REG_R11]     = SCARAB_REG_R11;
+  reg_compress_map_pin[(int)REG_R12]     = SCARAB_REG_R12;
+  reg_compress_map_pin[(int)REG_R13]     = SCARAB_REG_R13;
+  reg_compress_map_pin[(int)REG_R14]     = SCARAB_REG_R14;
+  reg_compress_map_pin[(int)REG_R15]     = SCARAB_REG_R15;
+  reg_compress_map_pin[(int)REG_GR_LAST] = SCARAB_REG_R15;
+  reg_compress_map_pin[(int)REG_RFLAGS]  = SCARAB_REG_ZPS;
+  reg_compress_map_pin[(int)REG_RIP]     = SCARAB_REG_RIP;
+
+  reg_compress_map_pin[(int)REG_DIL]  = SCARAB_REG_RDI;
+  reg_compress_map_pin[(int)REG_SIL]  = SCARAB_REG_RSI;
+  reg_compress_map_pin[(int)REG_BPL]  = SCARAB_REG_RBP;
+  reg_compress_map_pin[(int)REG_SPL]  = SCARAB_REG_RSP;
+  reg_compress_map_pin[(int)REG_R8B]  = SCARAB_REG_R8;
+  reg_compress_map_pin[(int)REG_R8W]  = SCARAB_REG_R8;
+  reg_compress_map_pin[(int)REG_R8D]  = SCARAB_REG_R8;
+  reg_compress_map_pin[(int)REG_R9B]  = SCARAB_REG_R9;
+  reg_compress_map_pin[(int)REG_R9W]  = SCARAB_REG_R9;
+  reg_compress_map_pin[(int)REG_R9D]  = SCARAB_REG_R9;
+  reg_compress_map_pin[(int)REG_R10B] = SCARAB_REG_R10;
+  reg_compress_map_pin[(int)REG_R10W] = SCARAB_REG_R10;
+  reg_compress_map_pin[(int)REG_R10D] = SCARAB_REG_R10;
+  reg_compress_map_pin[(int)REG_R11B] = SCARAB_REG_R11;
+  reg_compress_map_pin[(int)REG_R11W] = SCARAB_REG_R11;
+  reg_compress_map_pin[(int)REG_R11D] = SCARAB_REG_R11;
+  reg_compress_map_pin[(int)REG_R12B] = SCARAB_REG_R12;
+  reg_compress_map_pin[(int)REG_R12W] = SCARAB_REG_R12;
+  reg_compress_map_pin[(int)REG_R12D] = SCARAB_REG_R12;
+  reg_compress_map_pin[(int)REG_R13B] = SCARAB_REG_R13;
+  reg_compress_map_pin[(int)REG_R13W] = SCARAB_REG_R13;
+  reg_compress_map_pin[(int)REG_R13D] = SCARAB_REG_R13;
+  reg_compress_map_pin[(int)REG_R14B] = SCARAB_REG_R14;
+  reg_compress_map_pin[(int)REG_R14W] = SCARAB_REG_R14;
+  reg_compress_map_pin[(int)REG_R14D] = SCARAB_REG_R14;
+  reg_compress_map_pin[(int)REG_R15B] = SCARAB_REG_R15;
+  reg_compress_map_pin[(int)REG_R15W] = SCARAB_REG_R15;
+  reg_compress_map_pin[(int)REG_R15D] = SCARAB_REG_R15;
+
+  reg_compress_map_pin[(int)REG_XMM8]                  = SCARAB_REG_ZMM8;
+  reg_compress_map_pin[(int)REG_XMM9]                  = SCARAB_REG_ZMM9;
+  reg_compress_map_pin[(int)REG_XMM10]                 = SCARAB_REG_ZMM10;
+  reg_compress_map_pin[(int)REG_XMM11]                 = SCARAB_REG_ZMM11;
+  reg_compress_map_pin[(int)REG_XMM12]                 = SCARAB_REG_ZMM12;
+  reg_compress_map_pin[(int)REG_XMM13]                 = SCARAB_REG_ZMM13;
+  reg_compress_map_pin[(int)REG_XMM14]                 = SCARAB_REG_ZMM14;
+  reg_compress_map_pin[(int)REG_XMM15]                 = SCARAB_REG_ZMM15;
+  reg_compress_map_pin[(int)REG_XMM_SSE_LAST]          = SCARAB_REG_ZMM15;
+  reg_compress_map_pin[(int)REG_XMM_AVX_LAST]          = SCARAB_REG_ZMM15;
+  reg_compress_map_pin[(int)REG_XMM16]                 = SCARAB_REG_ZMM16;
+  reg_compress_map_pin[(int)REG_XMM_AVX512_HI16_FIRST] = SCARAB_REG_ZMM16;
+  reg_compress_map_pin[(int)REG_XMM17]                 = SCARAB_REG_ZMM17;
+  reg_compress_map_pin[(int)REG_XMM18]                 = SCARAB_REG_ZMM18;
+  reg_compress_map_pin[(int)REG_XMM19]                 = SCARAB_REG_ZMM19;
+  reg_compress_map_pin[(int)REG_XMM20]                 = SCARAB_REG_ZMM20;
+  reg_compress_map_pin[(int)REG_XMM21]                 = SCARAB_REG_ZMM21;
+  reg_compress_map_pin[(int)REG_XMM22]                 = SCARAB_REG_ZMM22;
+  reg_compress_map_pin[(int)REG_XMM23]                 = SCARAB_REG_ZMM23;
+  reg_compress_map_pin[(int)REG_XMM24]                 = SCARAB_REG_ZMM24;
+  reg_compress_map_pin[(int)REG_XMM25]                 = SCARAB_REG_ZMM25;
+  reg_compress_map_pin[(int)REG_XMM26]                 = SCARAB_REG_ZMM26;
+  reg_compress_map_pin[(int)REG_XMM27]                 = SCARAB_REG_ZMM27;
+  reg_compress_map_pin[(int)REG_XMM28]                 = SCARAB_REG_ZMM28;
+  reg_compress_map_pin[(int)REG_XMM29]                 = SCARAB_REG_ZMM29;
+  reg_compress_map_pin[(int)REG_XMM30]                 = SCARAB_REG_ZMM30;
+  reg_compress_map_pin[(int)REG_XMM31]                 = SCARAB_REG_ZMM31;
+  reg_compress_map_pin[(int)REG_XMM_AVX512_HI16_LAST]  = SCARAB_REG_ZMM31;
+  reg_compress_map_pin[(int)REG_XMM_AVX512_LAST]       = SCARAB_REG_ZMM31;
+  reg_compress_map_pin[(int)REG_XMM_LAST]              = SCARAB_REG_ZMM31;
+
+  reg_compress_map_pin[(int)REG_YMM8]                  = SCARAB_REG_ZMM8;
+  reg_compress_map_pin[(int)REG_YMM9]                  = SCARAB_REG_ZMM9;
+  reg_compress_map_pin[(int)REG_YMM10]                 = SCARAB_REG_ZMM10;
+  reg_compress_map_pin[(int)REG_YMM11]                 = SCARAB_REG_ZMM11;
+  reg_compress_map_pin[(int)REG_YMM12]                 = SCARAB_REG_ZMM12;
+  reg_compress_map_pin[(int)REG_YMM13]                 = SCARAB_REG_ZMM13;
+  reg_compress_map_pin[(int)REG_YMM14]                 = SCARAB_REG_ZMM14;
+  reg_compress_map_pin[(int)REG_YMM15]                 = SCARAB_REG_ZMM15;
+  reg_compress_map_pin[(int)REG_YMM_AVX_LAST]          = SCARAB_REG_ZMM15;
+  reg_compress_map_pin[(int)REG_YMM16]                 = SCARAB_REG_ZMM16;
+  reg_compress_map_pin[(int)REG_YMM_AVX512_HI16_FIRST] = SCARAB_REG_ZMM16;
+  reg_compress_map_pin[(int)REG_YMM17]                 = SCARAB_REG_ZMM17;
+  reg_compress_map_pin[(int)REG_YMM18]                 = SCARAB_REG_ZMM18;
+  reg_compress_map_pin[(int)REG_YMM19]                 = SCARAB_REG_ZMM19;
+  reg_compress_map_pin[(int)REG_YMM20]                 = SCARAB_REG_ZMM20;
+  reg_compress_map_pin[(int)REG_YMM21]                 = SCARAB_REG_ZMM21;
+  reg_compress_map_pin[(int)REG_YMM22]                 = SCARAB_REG_ZMM22;
+  reg_compress_map_pin[(int)REG_YMM23]                 = SCARAB_REG_ZMM23;
+  reg_compress_map_pin[(int)REG_YMM24]                 = SCARAB_REG_ZMM24;
+  reg_compress_map_pin[(int)REG_YMM25]                 = SCARAB_REG_ZMM25;
+  reg_compress_map_pin[(int)REG_YMM26]                 = SCARAB_REG_ZMM26;
+  reg_compress_map_pin[(int)REG_YMM27]                 = SCARAB_REG_ZMM27;
+  reg_compress_map_pin[(int)REG_YMM28]                 = SCARAB_REG_ZMM28;
+  reg_compress_map_pin[(int)REG_YMM29]                 = SCARAB_REG_ZMM29;
+  reg_compress_map_pin[(int)REG_YMM30]                 = SCARAB_REG_ZMM30;
+  reg_compress_map_pin[(int)REG_YMM31]                 = SCARAB_REG_ZMM31;
+  reg_compress_map_pin[(int)REG_YMM_AVX512_HI16_LAST]  = SCARAB_REG_ZMM31;
+  reg_compress_map_pin[(int)REG_YMM_AVX512_LAST]       = SCARAB_REG_ZMM31;
+  reg_compress_map_pin[(int)REG_YMM_LAST]              = SCARAB_REG_ZMM31;
+
+  reg_compress_map_pin[(int)REG_ZMM8]                  = SCARAB_REG_ZMM8;
+  reg_compress_map_pin[(int)REG_ZMM9]                  = SCARAB_REG_ZMM9;
+  reg_compress_map_pin[(int)REG_ZMM10]                 = SCARAB_REG_ZMM10;
+  reg_compress_map_pin[(int)REG_ZMM11]                 = SCARAB_REG_ZMM11;
+  reg_compress_map_pin[(int)REG_ZMM12]                 = SCARAB_REG_ZMM12;
+  reg_compress_map_pin[(int)REG_ZMM13]                 = SCARAB_REG_ZMM13;
+  reg_compress_map_pin[(int)REG_ZMM14]                 = SCARAB_REG_ZMM14;
+  reg_compress_map_pin[(int)REG_ZMM15]                 = SCARAB_REG_ZMM15;
+  reg_compress_map_pin[(int)REG_ZMM_AVX512_SPLIT_LAST] = SCARAB_REG_ZMM15;
+  reg_compress_map_pin[(int)REG_ZMM16]                 = SCARAB_REG_ZMM16;
+  reg_compress_map_pin[(int)REG_ZMM_AVX512_HI16_FIRST] = SCARAB_REG_ZMM16;
+  reg_compress_map_pin[(int)REG_ZMM17]                 = SCARAB_REG_ZMM17;
+  reg_compress_map_pin[(int)REG_ZMM18]                 = SCARAB_REG_ZMM18;
+  reg_compress_map_pin[(int)REG_ZMM19]                 = SCARAB_REG_ZMM19;
+  reg_compress_map_pin[(int)REG_ZMM20]                 = SCARAB_REG_ZMM20;
+  reg_compress_map_pin[(int)REG_ZMM21]                 = SCARAB_REG_ZMM21;
+  reg_compress_map_pin[(int)REG_ZMM22]                 = SCARAB_REG_ZMM22;
+  reg_compress_map_pin[(int)REG_ZMM23]                 = SCARAB_REG_ZMM23;
+  reg_compress_map_pin[(int)REG_ZMM24]                 = SCARAB_REG_ZMM24;
+  reg_compress_map_pin[(int)REG_ZMM25]                 = SCARAB_REG_ZMM25;
+  reg_compress_map_pin[(int)REG_ZMM26]                 = SCARAB_REG_ZMM26;
+  reg_compress_map_pin[(int)REG_ZMM27]                 = SCARAB_REG_ZMM27;
+  reg_compress_map_pin[(int)REG_ZMM28]                 = SCARAB_REG_ZMM28;
+  reg_compress_map_pin[(int)REG_ZMM29]                 = SCARAB_REG_ZMM29;
+  reg_compress_map_pin[(int)REG_ZMM30]                 = SCARAB_REG_ZMM30;
+  reg_compress_map_pin[(int)REG_ZMM31]                 = SCARAB_REG_ZMM31;
+  reg_compress_map_pin[(int)REG_ZMM_AVX512_HI16_LAST]  = SCARAB_REG_ZMM31;
+  reg_compress_map_pin[(int)REG_ZMM_AVX512_LAST]       = SCARAB_REG_ZMM31;
+  reg_compress_map_pin[(int)REG_ZMM_LAST]              = SCARAB_REG_ZMM31;
 #endif
-#else
+#ifdef MEMTRACE
   reg_compress_map[0]                      = 0;
   reg_compress_map[(int)XED_REG_RDI]       = SCARAB_REG_RDI;
   reg_compress_map[(int)XED_REG_EDI]       = SCARAB_REG_RDI;
@@ -878,8 +918,8 @@ void init_reg_compress_map(void) {
 
   //reg_compress_map[(int)XED_REG_CR_BASE] = SCARAB_REG_OTHER;
   reg_compress_map[(int)XED_REG_TSC]     = SCARAB_REG_OTHER;
-  reg_compress_map[(int)XED_REG_TSCAUX]  = SCARAB_REG_OTHER;
-  reg_compress_map[(int)XED_REG_XCR0]    = SCARAB_REG_OTHER;
+  reg_compress_map[(int)XED_REG_TSCAUX]   = SCARAB_REG_OTHER;
+  reg_compress_map[(int)XED_REG_XCR0]     = SCARAB_REG_OTHER;
   reg_compress_map[(int)XED_REG_CR0]     = SCARAB_REG_OTHER;
   reg_compress_map[(int)XED_REG_CR1]     = SCARAB_REG_OTHER;
   reg_compress_map[(int)XED_REG_CR2]     = SCARAB_REG_OTHER;
@@ -1066,6 +1106,8 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_ADDSUBPD] = {OP_FADD, 8, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_ADDSUBPS] = {OP_FADD, 4, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_ADD_LOCK] = {OP_IADD, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_AESENC]   = {OP_PIPELINED_MEDIUM, -1, -1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_AESENCLAST]   = {OP_PIPELINED_MEDIUM, -1, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_AND]      = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_ANDN]     = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_ANDNPD]   = {OP_LOGIC, 8, -1, NONE};
@@ -1091,8 +1133,10 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_CBW]       = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_CDQ]       = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_CDQE]      = {OP_LOGIC, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_CLAC]   = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_CLC]       = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_CLD]       = {OP_LOGIC, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_CLI]       = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_CMC]       = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_CMOVB]     = {OP_CMOV, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_CMOVBE]    = {OP_CMOV, -1, 1, NONE};
@@ -1129,6 +1173,7 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_CMPXCHG_LOCK]    = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_COMISD]          = {OP_ICMP, 8, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_COMISS]          = {OP_ICMP, 4, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_CRC32]           = {OP_PIPELINED_MEDIUM, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_CVTDQ2PD]        = {OP_FCVT, 8, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_CVTDQ2PS]        = {OP_FCVT, 4, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_CVTPD2DQ]        = {OP_FCVT, 8, -1, NONE};
@@ -1163,6 +1208,7 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_DIVPS]    = {OP_FDIV, 4, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_DIVSD]    = {OP_FDIV, 8, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_DIVSS]    = {OP_FDIV, 4, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_ENTER]    = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_FABS]     = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_FADD]     = {OP_FADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_FADDP]    = {OP_FADD, -1, 1, NONE};
@@ -1209,6 +1255,7 @@ void init_pin_opcode_convert(void) {
                                            NONE};  // Potential FMOV
   iclass_to_scarab_map[XED_ICLASS_FLDCW]   = {OP_NOTPIPELINED_MEDIUM, -1, 1,
                                             NONE};
+  iclass_to_scarab_map[XED_ICLASS_FLDENV]   = {OP_NOTPIPELINED_MEDIUM, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_FLDL2E]  = {OP_MOV, -1, 1,
                                              NONE};  // Potential FMOV
   iclass_to_scarab_map[XED_ICLASS_FLDL2T]  = {OP_MOV, -1, 1,
@@ -1225,6 +1272,7 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_FNCLEX] = {OP_NOTPIPELINED_SLOW, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_FNSTCW] = {OP_NOTPIPELINED_MEDIUM, -1, 1,
                                              NONE};
+  iclass_to_scarab_map[XED_ICLASS_FNSTENV] = {OP_NOTPIPELINED_SLOW, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_FNSTSW] = {OP_NOTPIPELINED_MEDIUM, -1, 1,
                                              NONE};
   iclass_to_scarab_map[XED_ICLASS_FNOP]   = {OP_NOP, -1, 1, NONE};
@@ -1258,13 +1306,17 @@ void init_pin_opcode_convert(void) {
                                               NONE};
   iclass_to_scarab_map[XED_ICLASS_IDIV]    = {OP_IDIV, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_IMUL]    = {OP_IMUL, -1, 1, NONE};
-  iclass_to_scarab_map[XED_ICLASS_INC]     = {OP_IADD, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_IN]     = {OP_IADD, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_INC]     = {OP_MOV, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_INT]     = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_INT3]    = {OP_IADD, -1, 1, NONE};
   // TODO: find out what opcodes these iclasses correspond to.
   // iclass_to_scarab_map[XED_ICLASS_INCPPSD] = {OP_IADD, -1, 1, NONE};
   // iclass_to_scarab_map[XED_ICLASS_INCPPSQ] = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_INC_LOCK] = {OP_IADD, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_INVLPG] = {OP_LOGIC, -1, 1, NONE};
+
+  iclass_to_scarab_map[XED_ICLASS_IRETQ]    = {OP_PIPELINED_MEDIUM, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_JB]       = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_JBE]      = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_JCXZ]     = {OP_IADD, -1, 1, NONE};
@@ -1306,6 +1358,7 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_LODSD]  = {OP_MOV, 4, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_LODSQ]  = {OP_MOV, 8, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_LODSW]  = {OP_MOV, 2, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_LOOP]   = {OP_CF,  -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_LSL]    = {OP_LDA, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_LZCNT]  = {OP_PIPELINED_FAST, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_MAXPD]  = {OP_FCMP, 8, -1, NONE};
@@ -1319,9 +1372,12 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_MINPS]     = {OP_FCMP, 4, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_MINSD]     = {OP_FCMP, 8, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_MINSS]     = {OP_FCMP, 4, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_MONITOR]   = {OP_LDA, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_MOV]       = {OP_MOV, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_MOVAPD]    = {OP_MOV, 8, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_MOVAPS]    = {OP_MOV, 4, -1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_MOV_CR]    = {OP_MOV, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_MOV_DR]    = {OP_MOV, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_MOVBE]     = {OP_MOV, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_MOVD]      = {OP_PIPELINED_FAST, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_MOVDDUP]   = {OP_MOV, 8, -1, NONE};
@@ -1370,6 +1426,7 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_MULSD]     = {OP_FMUL, 8, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_MULSS]     = {OP_FMUL, 4, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_MULX]      = {OP_IMUL, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_MWAIT]      = {OP_NOTPIPELINED_MEDIUM, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_NEG]       = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_NEG_LOCK]  = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_NOP]       = {OP_NOP, -1, 1, NONE};
@@ -1387,8 +1444,11 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_ORPD]      = {OP_LOGIC, 8, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_ORPS]      = {OP_LOGIC, 4, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_OR_LOCK]   = {OP_LOGIC, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_OUT]   = {OP_MOV, 2, 1, NONE};
+
   iclass_to_scarab_map[XED_ICLASS_PABSB]     = {OP_LOGIC, 1, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_PABSD]     = {OP_LOGIC, 4, -1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_PACKUSWB]  = {OP_MOV, -1, -1, NONE}; // I think output lane bytes will be different than input lane bytes, but not sure.
   iclass_to_scarab_map[XED_ICLASS_PABSW]     = {OP_LOGIC, 2, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_PADDB]     = {OP_IADD, 1, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_PADDD]     = {OP_IADD, 4, -1, NONE};
@@ -1407,6 +1467,7 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_PAVGW]     = {OP_LOGIC, 2, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_PBLENDVB]  = {OP_CMOV, 1, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_PBLENDW]   = {OP_CMOV, 2, -1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_PCLMULQDQ] = {OP_IMUL, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_PCMPEQB]   = {OP_ICMP, 1, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_PCMPEQD]   = {OP_ICMP, 4, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_PCMPEQQ]   = {OP_ICMP, 8, -1, NONE};
@@ -1531,9 +1592,13 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_PUSHFD] = {OP_NOTPIPELINED_SLOW, 4, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_PUSHFQ] = {OP_NOTPIPELINED_SLOW, 8, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_PXOR]   = {OP_LOGIC, 1, -1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_RCPPS]  = {OP_FMUL, 4, -1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_RCL]      = {OP_SHIFT, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_RCR]      = {OP_SHIFT, -1, 1, NONE};
+
   iclass_to_scarab_map[XED_ICLASS_RDTSC]  = {OP_NOTPIPELINED_SLOW, 4, 1, NONE};
-  iclass_to_scarab_map[XED_ICLASS_RDTSCP]  = {OP_NOTPIPELINED_VERY_SLOW, -1, 1,
-					      NONE};
+  iclass_to_scarab_map[XED_ICLASS_RDTSCP]  = {OP_NOTPIPELINED_SLOW, 8, 1, NONE}; // 4 bytes for time-stamp (somehow), 4 for processor id
+  iclass_to_scarab_map[XED_ICLASS_RDPKRU] = {OP_MOV, 1, 2, NONE};
   // INS_Opcode() never returns REPEAT variants of the iclasses
   iclass_to_scarab_map[XED_ICLASS_REPE_CMPSB] = {OP_ICMP, 1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_REPE_CMPSD] = {OP_ICMP, 4, 1, NONE};
@@ -1606,6 +1671,10 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_SQRTPS]  = {OP_FDIV, 4, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_SQRTSD]  = {OP_FDIV, 8, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_SQRTSS]  = {OP_FDIV, 4, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_STAC]   = {OP_LOGIC, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_STC]   = {OP_LOGIC, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_STD]   = {OP_LOGIC, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_STI]   = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_STMXCSR] = {OP_PIPELINED_MEDIUM, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_STOSB]   = {OP_MOV, 1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_STOSD]   = {OP_MOV, 4, 1, NONE};
@@ -1619,6 +1688,9 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_SUB_LOCK] = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_SYSCALL]  = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_SYSENTER] = {OP_IADD, -1, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_SYSRET] = {OP_IADD, -1, 1, NONE};
+
+  iclass_to_scarab_map[XED_ICLASS_SWAPGS]  = {OP_MOV,-1,1,NONE};
   iclass_to_scarab_map[XED_ICLASS_TEST]     = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_TZCNT]    = {OP_NOTPIPELINED_MEDIUM, -1, 1,
                                             NONE};
@@ -1713,6 +1785,8 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_VDIVPS]    = {OP_FDIV, 4, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_VDIVSD]    = {OP_FDIV, 8, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_VDIVSS]    = {OP_FDIV, 4, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_VERR]  = {OP_LOGIC, 2, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_VERW]  = {OP_LOGIC, 2, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_VEXTRACTF128]  = {OP_MOV, 16, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_VEXTRACTF32X4] = {OP_MOV, 16, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_VEXTRACTF32X8] = {OP_MOV, 32, 1, NONE};
@@ -2197,6 +2271,7 @@ void init_pin_opcode_convert(void) {
   iclass_to_scarab_map[XED_ICLASS_VXORPS]     = {OP_LOGIC, 4, -1, NONE};
   iclass_to_scarab_map[XED_ICLASS_VZEROALL]   = {OP_LOGIC, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_VZEROUPPER] = {OP_LOGIC, 16, 1, NONE};
+  iclass_to_scarab_map[XED_ICLASS_WRMSR]      = {OP_MOV, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_XADD]       = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_XADD_LOCK]  = {OP_IADD, -1, 1, NONE};
   iclass_to_scarab_map[XED_ICLASS_XCHG]       = {OP_MOV, -1, 1, NONE};
@@ -2209,6 +2284,8 @@ void init_pin_opcode_convert(void) {
                                              NONE};
   iclass_to_scarab_map[XED_ICLASS_XSAVE]    = {OP_NOTPIPELINED_VERY_SLOW, -1, 1,
                                             NONE};
+  iclass_to_scarab_map[XED_ICLASS_XSAVE64]    = {OP_NOTPIPELINED_VERY_SLOW, -1, 1,
+                                            NONE};
   iclass_to_scarab_map[XED_ICLASS_XSAVEC]   = {OP_NOTPIPELINED_VERY_SLOW, -1, 1,
                                              NONE};
 }
diff --git a/src/prefetcher/fdip.cc b/src/prefetcher/fdip.cc
index 1d93e5b..73f03b5 100644
--- a/src/prefetcher/fdip.cc
+++ b/src/prefetcher/fdip.cc
@@ -21,40 +21,93 @@ extern "C" {
 #include "prefetcher/pref.param.h"
 #include "memory/memory.param.h"
 #include "memory/memory.h"
+#include "uop_cache.h"
 }
 
 #define DUMMY_CFN ~0
-
 struct ftq_req {
-    Addr target;
-    Counter prefetch_cycle;
-    Op op;
-    bool prefetched; 
-    bool taken;
-
-    ftq_req(Addr _target, Counter _prefetch_cycle, Op _op, bool _prefetched,
-	    bool _taken)
-	: target(_target),
-	  prefetch_cycle(_prefetch_cycle), op(_op), prefetched(_prefetched),
-	  taken(_taken) {};
+  Addr target;
+  Counter prefetch_cycle;
+  Op op;
+  bool prefetched;
+  bool taken;
+
+  ftq_req(Addr _target, Counter _prefetch_cycle, Op _op, bool _prefetched,
+    bool _taken)
+  : target(_target),
+    prefetch_cycle(_prefetch_cycle), op(_op), prefetched(_prefetched),
+    taken(_taken) {};
 };
 
 std::unordered_map<Addr, Op> pc_to_op;
-std::queue<std::pair<Addr, ftq_req>> ftq;
+std::deque<std::pair<Addr, ftq_req>> ftq;
 Addr runahead_pc;
+Addr last_cl_prefetched = 0;
 Icache_Stage *ic_stage;
 Bp_Data *bp_data;
 uns cf_num    = 0;
-bool runahead_disable = true;
+Flag runahead_disable = TRUE;
 Op *recovery_checkpoint;
-bool recovery_checkpoint_valid = false;
-bool on_wrong_path = false;
+Flag recovery_checkpoint_valid = FALSE;
+Flag fdip_on_path_bp = TRUE;
+Flag fdip_on_path_pref = TRUE;
+uns64 last_runahead_uid = 0; // uid of the branch that FDIP predicted most recently
+uns64 max_runahead_uid = 0;
+Counter last_runahead_op = 0;
+Counter max_runahead_op = 0;
+extern List op_buf;
+Hash_Table top_mispred_br;
+uns64 recovery_count = 0;
+uns64 off_count = 0;
+//uns64 outstanding_prefs = 0;
+// TODO: one of 1-counter mode and 2-counter mode will be deprecated after verifying with unlimited TAGE buffer. 2-counter mode is commented out in current version.
+uns64 outstanding_prefs_on_path = 0;
+uns64 outstanding_prefs_off_path = 0;
+Flag mem_req_failed = FALSE;
+Counter max_op_num = 0;
+Counter last_recover_cycle = 0;
+
+/**************************************************************************************/
+/* init_topk_mispred: list of branches that provide a given coverage of resteers */
+void init_topk_mispred() {
+    init_hash_table(&top_mispred_br, "top mispredicting branches", 100000, sizeof(int));
+    FILE* fp = fopen(TOP_MISPRED_BR_FILEPATH, "r");
+    const int line_len = 256;
+    char line[line_len];
+    char* field;
+    float coverage;
+    Addr br;
+    fgets(line, line_len, fp); // skip first line
+    while (fgets(line, line_len, fp)) {
+      field = strtok(line, ",");
+      field = strtok(NULL, ",");
+      coverage = atof(field);
+      field = strtok(NULL, ",");
+      br = strtoull(field, NULL, 16);
+      // add to map
+      Flag new_entry;
+      hash_table_access_create(&top_mispred_br, br, &new_entry);
+      ASSERT(0, new_entry);
+      STAT_EVENT(0, TOP_MISPRED_BR);
+      if (coverage >= TOP_MISPRED_BR_RESTEER_COVERAGE)
+        break;
+    }
+}
 
 void fdip_init(Bp_Data* _bp_data,  Icache_Stage *_ic) {
-    ASSERT(ic_stage->proc_id, !(FDIP_ENABLE && USE_LATE_BP));
-    bp_data = _bp_data;
-    ic_stage = _ic;
-    ASSERT(ic_stage->proc_id, ic_stage);
+  ASSERT(ic_stage->proc_id, !(FDIP_ENABLE && USE_LATE_BP));
+  if (!FETCH_BREAK_ON_TAKEN) { // icache_stage does not break on taken branches
+    ASSERT(ic_stage->proc_id, FDIP_MAX_TAKEN_BRANCHES >= ISSUE_WIDTH);
+    ASSERT(ic_stage->proc_id, FDIP_MAX_OUTSTANDING_PREFETCHES >= ISSUE_WIDTH);
+  }
+  bp_data = _bp_data;
+  ic_stage = _ic;
+  ASSERT(ic_stage->proc_id, ic_stage);
+  if ((FDIP_DUAL_PATH_PREF_IC_ENABLE || FDIP_DUAL_PATH_PREF_UOC_ENABLE)
+        && TOP_MISPRED_BR_RESTEER_COVERAGE)
+    init_topk_mispred();
+  // Only one of these prefetching options should be set.
+  ASSERT(ic_stage->proc_id, UOC_ORACLE_PREF + UOC_PREF + FDIP_DUAL_PATH_PREF_UOC_ENABLE <= 1);
 }
 
 /* Called when a branch completes in the functional unit */
@@ -65,11 +118,11 @@ void fdip_resolve(Op *op) {
  * recovery checkpoint, the checkpoint is no longer valid. At this point we
  * do not have a valid recovery checkpoint. */
 void fdip_retire(Op *op) {
-    if (recovery_checkpoint_valid &&
-	op->recovery_info.branch_id ==
-	recovery_checkpoint->recovery_info.branch_id) {
-	recovery_checkpoint_valid = false;
-    }
+  if (recovery_checkpoint_valid &&
+    op->recovery_info.branch_id ==
+    recovery_checkpoint->recovery_info.branch_id) {
+    recovery_checkpoint_valid = FALSE;
+  }
 }
 
 /* When the branch is initially predicted by FDIP, a placeholder op struct is
@@ -77,53 +130,14 @@ void fdip_retire(Op *op) {
  * data into the current op
  */
 void patch_oracle_info(Op *op, Op *req, Addr bp_pc) {
-    //Copy placeholder recovery info into current op
-    op->recovery_info = req->recovery_info;
-    //Update dynamic fields of the current op that were unavailable for FDIP
-    op->recovery_info.new_dir = op->oracle_info.dir;
-    op->recovery_info.op_num = op->op_num;
-    op->recovery_info.PC = op->inst_info->addr;
-    op->recovery_info.oracle_dir = op->oracle_info.dir;
-    op->recovery_info.branchTarget = op->oracle_info.target;
-
-    Op_Info *op_info = &op->oracle_info;
-    Op_Info *req_info = &req->oracle_info;
-
-    op_info->pred_addr = req_info->pred_addr;
-    op_info->btb_miss_resolved = req_info->btb_miss_resolved;
-    op_info->pred_npc = req_info->pred_npc;
-    op_info->pred = req_info->pred;
-    op_info->btb_miss = req_info->btb_miss;
-    op_info->no_target = req_info->no_target;
-
-// FIXME late prefetcher
-    //uns8 late_pred;  // predicted direction of branch, set by the multi-cycle
-    // branch predictor
-    //Addr late_pred_npc;  // predicted next pc field by the multi-cycle branch
-    // predictor
-    //Flag late_misfetch;  // true if target address is the ONLY thing that was
-    // wrong after the multi-cycle branch prediction kicks in
-    //Flag  late_mispred;  // true if the multi-cycle branch predictor mispredicted
-    //Flag  recovery_sch;  // true if this op_info has scheduled a recovery
-    op_info->pred_global_hist = req_info->pred_global_hist;
-
-//TODO: Verify perceptron correctness with FDIP
-    op_info->pred_perceptron_global_hist =
-	req_info->pred_perceptron_global_hist;
-    op_info->pred_conf_perceptron_global_hist =
-	req_info->pred_conf_perceptron_global_hist;
-    op_info->pred_conf_perceptron_global_misp_hist =
-	req_info->pred_conf_perceptron_global_misp_hist;
-    op_info->pred_gpht_entry = req_info->pred_gpht_entry;
-    op_info->pred_ppht_entry = req_info->pred_ppht_entry;
-    op_info->pred_spht_entry = req_info->pred_spht_entry;
-    
-    op_info->pred_local_hist = req_info->pred_local_hist;
-    op_info->pred_targ_hist = req_info->pred_targ_hist;
-    op_info->hybridgp_gpred = req_info->hybridgp_gpred;
-    op_info->hybridgp_ppred = req_info->hybridgp_ppred;
-    op_info->pred_tc_selector_entry = req_info->pred_tc_selector_entry;
-    op_info->ibp_miss = req_info->ibp_miss;
+  //Copy placeholder recovery info into current op
+  op->recovery_info = req->recovery_info;
+  //Update dynamic fields of the current op that were unavailable for FDIP
+  op->recovery_info.new_dir = op->oracle_info.dir;
+  op->recovery_info.op_num = op->op_num;
+  op->recovery_info.PC = op->inst_info->addr;
+  op->recovery_info.oracle_dir = op->oracle_info.dir;
+  op->recovery_info.branchTarget = op->oracle_info.target;
 }
 
 /* Called when the frontend consumes the next prediction. There exist three
@@ -133,125 +147,61 @@ void patch_oracle_info(Op *op, Op *req, Addr bp_pc) {
  * predicted a branch that is never actually processed by the frontend
  */ 
 Addr fdip_pred(Addr bp_pc, Op *op) {
-    fdip_new_branch(op->fetch_addr, op);
-    if (ftq.empty()) {
-	/* If the FTQ is empty we have not been able to predict ahead. Should
-	 * be a corner case e.g. directly after a recovery.
-	 */
-    	runahead_disable = false;
-	Addr target = bp_predict_op(g_bp_data, op, cf_num++, bp_pc);
-	bp_predict_op_evaluate(g_bp_data, op, target);
-	recovery_checkpoint = op;
-	recovery_checkpoint_valid = true;
-	runahead_pc = target;
-	on_wrong_path = false;
-	STAT_EVENT(ic_stage->proc_id, FDIP_PRED_FTQ_EMPTY);
-	return target;
-    }
-    else if (ftq.front().first == op->fetch_addr){
-	/* FDIP predicted a branch on the right path. Consume the target and
-	 * patch the current instruction.
-	 */
-	STAT_EVENT(ic_stage->proc_id, FDIP_PRED_CORRECT_PATH);
-        auto req = &ftq.front().second;
-	auto target = req->target;
-	patch_oracle_info(op, &req->op, bp_pc);
-	//Re-evaluate FDIP direction prediction based on the current oracle info
-	bp_predict_op_evaluate(g_bp_data, op, req->target);
-	op->cf_within_fetch = cf_num++;
-	if (!on_wrong_path) {
-	    //We may have mispredicted once but the branch PCs seen by the
-	    //frontend and FDIP still match
-	    recovery_checkpoint = op;
-	    recovery_checkpoint_valid = true;
-	}
-	/* A branch processed by FDIP has been mispredicted. We do not recover
-	 * at this point as we want FDIP to continue prefetching on the wrong
-	 * path. Recover only if this branch is resolved by the backend.
-	 */
-	ASSERT(ic_stage->proc_id, req->taken == req->op.oracle_info.pred);
-	if (op->oracle_info.mispred || op->oracle_info.misfetch) {
-	    on_wrong_path = true;
-	    ASSERT(ic_stage->proc_id, op->oracle_info.mispred ||
-		   op->oracle_info.misfetch);
-	}
-	else {
-	    ASSERT(ic_stage->proc_id, !op->oracle_info.mispred &&
-		   !op->oracle_info.misfetch);
-	}
-	if (req->prefetched) {
-	    STAT_EVENT(ic_stage->proc_id, FDIP_PREF_CORRECT_PATH);
-	    INC_STAT_EVENT(ic_stage->proc_id, FDIP_SAVED_PREF_CYC,
-			   cycle_count - req->prefetch_cycle);
-	}
-	ftq.pop();
-	return target;
-    }
-    else {
-	/* If the executed branch does not match the oldest entry in the FTQ,
-	 * we are on the wrong path. Recover, clear FTQ and retry branch pred.
-	 * At this point we may not have realized that we are on the wrong path
-	 * as the branch has not been resolved yet. This should only happen with
-	 * trace frontend which do not actually see branches on the wrong path.
-	 */
-	INC_STAT_EVENT(ic_stage->proc_id, FDIP_PRED_WRONG_PATH, ftq.size());
-	if (recovery_checkpoint_valid) {
-	    /* The frontend has fetched a branch and that branch is still
-	     * executing. Hence we have a valid recovery point to which we will
-	     * recover (squash all later branches predicted by FDIP).
-	     */
-	    bp_recover_op(g_bp_data, recovery_checkpoint->table_info->cf_type,
-			  &recovery_checkpoint->recovery_info);
-	} else {
-	    /* MAYBE A HACK: The backend currently is not executing a branch. As
-	     * a result we have no valid recovery point. FDIP predicted a branch
-	     * that is never actually executed.
-	     * First recover it so that all later branches are squashed then
-	     * retire it like a regular instruction to keep the BP state in sync
-	     * The BP history now contains an executed, mispredicted, retired
-	     * branch that has never actually gone through the backend, this
-	     * impact TAGE's accuracy (hopefully it doesn't happen often).
-	     */
-	    //Copy req onto the stack, as bp_recover_op will clear the ftq 
-	    ftq_req req = ftq.front().second;
-	    bp_recover_op(g_bp_data, req.op.table_info->cf_type,
-			  &req.op.recovery_info);
-	    bp_retire_op(g_bp_data, &req.op);
-	    STAT_EVENT(ic_stage->proc_id, FDIP_SQUASH_FAKE_BRANCH);
-	}
-	fdip_clear_ftq(bp_pc);
-	auto target =  bp_predict_op(g_bp_data, op, cf_num++, bp_pc);
-	target = bp_predict_op_evaluate(g_bp_data, op, target);
-	recovery_checkpoint = op;
-	recovery_checkpoint_valid = true;
-	runahead_disable = false;
-	runahead_pc = target;
-	on_wrong_path = false;
-
-	return target;
-    }
+  ASSERT(ic_stage->proc_id, !ftq.empty());
+  ASSERT(ic_stage->proc_id, ftq.front().first == op->fetch_addr);
+  /* FDIP predicted a branch on the right path. Consume the target and
+   * patch the current instruction.
+   */
+  STAT_EVENT(ic_stage->proc_id, FDIP_PRED_CORRECT_PATH);
+  auto req = &ftq.front().second;
+  auto target = req->target;
+  patch_oracle_info(op, &req->op, bp_pc);
+  //Re-evaluate FDIP direction prediction based on the current oracle info
+  op->cf_within_fetch = cf_num++;
+  if (!fdip_on_path_bp) {
+    //We may have mispredicted once but the branch PCs seen by the
+    //frontend and FDIP still match
+    recovery_checkpoint = op;
+    recovery_checkpoint->recovery_info.npc = op->oracle_info.npc;
+    recovery_checkpoint_valid = TRUE;
+  }
+  /* A branch processed by FDIP has been mispredicted. We do not recover
+   * at this point as we want FDIP to continue prefetching on the wrong
+   * path. Recover only if this branch is resolved by the backend.
+   */
+  ASSERT(ic_stage->proc_id, req->taken == req->op.oracle_info.pred);
+  if (op->oracle_info.mispred || op->oracle_info.misfetch) {
+    ASSERT(ic_stage->proc_id, op->oracle_info.mispred ||
+        op->oracle_info.misfetch);
+  }
+  else {
+    ASSERT(ic_stage->proc_id, !op->oracle_info.mispred &&
+        !op->oracle_info.misfetch);
+  }
+  if (req->prefetched) {
+    STAT_EVENT(ic_stage->proc_id, FDIP_PREF_CORRECT_PATH);
+    INC_STAT_EVENT(ic_stage->proc_id, FDIP_SAVED_PREF_CYC,
+        cycle_count - req->prefetch_cycle);
+  }
+  ftq.pop_front();
+  return target;
 }
 
-/* To enable runahead prefetching, maintain a data structure of all branches
- * that have been seen so far. This allows us to predict branches without 
+/* To enable off-path runahead prefetching, maintain a data structure of all branches
+ * that have been seen so far. This allows us to predict branches without
  * receiving them from the simulator frontend.
  */
 void fdip_new_branch(Addr bp_pc, Op *op) {
-    if(pc_to_op.find(bp_pc) == pc_to_op.end()) {
-	pc_to_op.insert(std::pair<Addr, Op>(bp_pc, *op));
-    }
+  if(pc_to_op.find(bp_pc) == pc_to_op.end()) {
+    pc_to_op.insert(std::pair<Addr, Op>(bp_pc, *op));
+  }
 }
 
 /* Clear the FTQ, branch predictor state needs to be recovered elsewhere
  */
-void fdip_clear_ftq(Addr recover_pc) {
-    runahead_pc = recover_pc;
-    while (!ftq.empty()) {
-	if (ftq.front().second.prefetched) {
-	    STAT_EVENT(ic_stage->proc_id, FDIP_PREF_WRONG_PATH);
-	}
-	ftq.pop();
-    }
+void fdip_clear_ftq() {
+  ASSERT(ic_stage->proc_id, ftq.empty());
+  last_cl_prefetched = 0;
 }
 
 /* When a mispredicted branch is resolved, the frontend recovers the branch
@@ -259,88 +209,408 @@ void fdip_clear_ftq(Addr recover_pc) {
  * clear the FTQ.
  */
 void fdip_recover(Recovery_Info *info) {
-    recovery_checkpoint = NULL;
-    recovery_checkpoint_valid = false;
-    fdip_clear_ftq(info->npc);
+  recovery_count++;
+  ASSERT(ic_stage->proc_id, off_count == recovery_count);
+  ASSERT(ic_stage->proc_id, !PERFECT_NT_BTB || ftq.empty());
+  fdip_clear_ftq();
+  if (last_runahead_uid != max_runahead_uid)
+    last_runahead_uid = 0;
+  if (last_runahead_op != max_runahead_op)
+    last_runahead_op = 0;
+  runahead_pc = info->npc;
+  runahead_disable = FALSE;
+  fdip_on_path_bp = TRUE;
+  fdip_on_path_pref = TRUE;
+  if (UOC_ORACLE_PREF) {
+    uop_cache_issue_prefetch(info->npc, fdip_on_path_pref);
+  }
+  last_recover_cycle = cycle_count;
+  //outstanding_prefs_off_path = 0;
+  STAT_EVENT(ic_stage->proc_id, FDIP_RECOVER);
+  INC_STAT_EVENT(ic_stage->proc_id, FDIP_ON_PATH_OUTSTANDING_PREFS_ON_RECOVER, outstanding_prefs_on_path);
+  INC_STAT_EVENT(ic_stage->proc_id, FDIP_OFF_PATH_OUTSTANDING_PREFS_ON_RECOVER, outstanding_prefs_off_path);
+  fdip_update();
+}
+
+/* When a misfetch (btb miss) is resolved, the frontend informs FDIP to clear FDIP branch predictor states including the FTQ.
+ */
+void fdip_redirect(Addr recover_pc) {
+  bp_recover_op(g_bp_data, recovery_checkpoint->table_info->cf_type, &recovery_checkpoint->recovery_info);
 }
 
 // Returns true if prefetch was emitted
-bool fdip_prefetch(Addr target, Op *op) {
-    Addr line_addr;
-
-    auto line = (Inst_Info**)cache_access(&ic_stage->icache, target,
-					  &line_addr, TRUE);
-    if (!line) {
-	if(new_mem_req(MRT_IFETCH, ic_stage->proc_id, line_addr,
-		       ICACHE_LINE_SIZE, 0, NULL, icache_fill_line,
-		       unique_count,
-		       0)) {
-	    STAT_EVENT(ic_stage->proc_id, FDIP_PREFETCHES);
-	    return true;
-	}
+Flag fdip_prefetch(Addr target, Op *op) {
+  static Addr last_line_addr_prefetched = 0;
+  Addr line_addr;
+  Flag success = FAILED;
+  void* line = NULL;
+
+  if(!FDIP_ALWAYS_PREFETCH)
+    line = (Inst_Info**)cache_access(&ic_stage->icache, target,
+                              &line_addr, TRUE);
+  if (fdip_on_path_pref) {
+    STAT_EVENT(ic_stage->proc_id, FDIP_ATTEMPTED_PREF_ON_PATH);
+  } else {
+    STAT_EVENT(ic_stage->proc_id, FDIP_ATTEMPTED_PREF_OFF_PATH);
+  }
+
+  if(FDIP_ALWAYS_PREFETCH || (!line && (last_line_addr_prefetched != line_addr))) {
+    if(FDIP_PREF_NO_LATENCY) {
+      Mem_Req req;
+      req.off_path             = op ? op->off_path : FALSE;
+      req.off_path_confirmed   = FALSE;
+      req.type                 = MRT_FDIPPRF;
+      req.proc_id              = ic_stage->proc_id;
+      req.addr                 = line_addr;
+      req.oldest_op_unique_num = (Counter)0;
+      req.oldest_op_op_num     = (Counter)0;
+      req.oldest_op_addr       = (Addr)0;
+      req.dirty_l0             = op && op->table_info->mem_type == MEM_ST && !op->off_path;
+      if(icache_fill_line(&req)) {
+        STAT_EVENT(ic_stage->proc_id, FDIP_PREFETCHES);
+        success = SUCCESS_NEW;
+      }
+    } else {
+      success = new_mem_req(MRT_FDIPPRF, ic_stage->proc_id, line_addr,
+                    ICACHE_LINE_SIZE, 0, NULL, instr_fill_line, unique_count, 0);
+      if (success) {
+        STAT_EVENT(ic_stage->proc_id, FDIP_PREFETCHES);
+      } else {
+        //mem_req_failed = TRUE;
+        ASSERT(ic_stage->proc_id, false);
+      }
     }
-    else {
-	STAT_EVENT(ic_stage->proc_id, FDIP_PREF_ICACHE_HIT);
+  }
+  else {
+    STAT_EVENT(ic_stage->proc_id, FDIP_PREF_ICACHE_HIT);
+  }
+  last_line_addr_prefetched = line_addr;
+  return success;
+}
+
+int fdip_dual_path_prefetch(Addr target, Op* op) {
+  int icache_pref = 0;
+  int uoc_pref = 0;
+  ASSERT(ic->proc_id, FDIP_DUAL_PATH_PREF_IC_ENABLE 
+                  || FDIP_DUAL_PATH_PREF_UOC_ENABLE);
+
+  if (FDIP_DUAL_PATH_PREF_IC_ENABLE) {
+    // is prefetching one CL for the alt path enough?
+    // pred_target is the target stored by the BTB used if predicted taken
+    Flag success = fdip_prefetch(op->pred_target, op);
+    if (success) {
+      icache_pref += 1;
+      ftq.back().second.prefetched = true;
     }
-    return false;
+    success = fdip_prefetch(op->inst_info->addr + ICACHE_LINE_SIZE, op);
+    if (success)
+      icache_pref += 1;
+    STAT_EVENT(ic_stage->proc_id, FDIP_ALT_PATH_PREFETCHES_IC_TRIGGERED);
+    INC_STAT_EVENT(ic_stage->proc_id, FDIP_ALT_PATH_PREFETCHES_IC_EMITTED, 
+                    icache_pref);
+  }
+  if (FDIP_DUAL_PATH_PREF_UOC_ENABLE) {
+    uoc_pref += uop_cache_issue_prefetch(op->pred_target, fdip_on_path_pref && op->oracle_info.pred);
+    uoc_pref += uop_cache_issue_prefetch(op->pc_plus_offset, fdip_on_path_pref && !op->oracle_info.pred);
+    STAT_EVENT(ic_stage->proc_id, FDIP_ALT_PATH_PREFETCHES_UOC_TRIGGERED);
+    INC_STAT_EVENT(ic_stage->proc_id, FDIP_ALT_PATH_PREFETCHES_UOC_EMITTED,
+                    uoc_pref);
+  }
+  return icache_pref;
 }
 
 // Called each cycle to trigger runahead prefetches
 void fdip_update() {
-    Addr fdip_break_addr_top = ~0UL;
-    Addr fdip_break_addr_bottom = 0UL;
-    Addr CLMASK = 0x3F;
-    uint32_t predicts = 0;
-    uint32_t prefetches = 0;
-
-    if (ftq.size() > FDIP_MAX_RUNAHEAD || runahead_disable) {
-	return;
+  uns64 orig_last_runahead_uid = last_runahead_uid;
+  uint32_t taken_branches = 0;
+  Flag break_on_tage_limit = FALSE;
+
+  //if (ftq.size() > FDIP_MAX_RUNAHEAD || runahead_disable) {
+  if (runahead_disable) {
+    return;
+  }
+
+  // Predict branches across cache lines. In many implementations, the BTB
+  // is looked up once per cycle, returning all branches in the cache line
+  mem_req_failed = FALSE;
+
+  // Find the next branch after the runahead PC. As the BTB/BP is addressed
+  // with a PC and not cache line address we need to byte-wise increment the
+  // runahead PC
+  /*
+   * (1) FDIP_PERFECT_RUNAHEAD : FDIP runs ahead enough (ISSUE_WIDTH) regardless of the number of taken branches.
+   * (2) FDIP_MAX_TAKEN_BRANCHES : 2 by default
+   * (3) last_runahead_op != max_runahead_op : necessary to prevent FDIP from trying to run ahead when there are not available decoded ops.
+   *     set by find_op() when the next branch is not found
+   * (4) outstanding_prefs < FDIP_MAX_OUTSTANDING_PREFETCHES : outstanding_prefs is the number of outstanding prefetches not per cycle.
+   *     It is incremented by 1 whenever FDIP emits a cache line prefetch and decremented by 1 whenever the backend consumes a branch from FTQ.
+   *     It is reset by 0 whenever FTQ is cleared and the backend hits an FTQ-empty case.
+   * (5) bp_is_predictable() : necessary to ensure that the TAGE buffer doesn't become full.
+   * (6) !mem_req_failed : necessary for FDIP to break out from the while loop when L1 queue is full.
+   *     Even with outstanding_pref = 1, the queue becomes full because the backend consumes a branch from FTQ and decreases the count (outstanding_prefs), but it is possible that the requested cache line has not yet loaded due to the latency.
+   *     In this case, FDIP runs ahead again and continuously increases the number of memory requests although all the previous requests have not yet been handled.
+   */
+  while ((FDIP_PERFECT_RUNAHEAD && !mem_req_failed &&
+                //((!fdip_on_path_bp && (outstanding_prefs < FDIP_MAX_OUTSTANDING_PREFETCHES)) ||
+                ((!fdip_on_path_bp && (outstanding_prefs_on_path + outstanding_prefs_off_path < FDIP_MAX_OUTSTANDING_PREFETCHES)) ||
+                 (fdip_on_path_bp && (last_runahead_uid < orig_last_runahead_uid + ISSUE_WIDTH) && (last_runahead_uid != max_runahead_uid)))) ||
+      ((!FDIP_PERFECT_RUNAHEAD &&
+                (taken_branches < FDIP_MAX_TAKEN_BRANCHES) &&
+                (last_runahead_op != max_runahead_op) &&
+                //(outstanding_prefs < FDIP_MAX_OUTSTANDING_PREFETCHES) &&
+                (outstanding_prefs_on_path + outstanding_prefs_off_path < FDIP_MAX_OUTSTANDING_PREFETCHES) &&
+                //(count_fdip_mem_l1_reqs() < FDIP_MAX_OUTSTANDING_PREFETCHES) && // the actual FTQ size
+                !mem_req_failed))) {
+
+    bool btb_ras_miss = false;
+    bool ftq_pushed = false;
+
+    Op* op = NULL;
+    bool is_branch = false;
+    Addr target = 0;
+
+    if (!mem_can_allocate_req_buffer(ic_stage->proc_id, MRT_FDIPPRF)) {
+      mem_req_failed = TRUE;
+      break;
     }
+    // prediction
+    if (fdip_on_path_bp) {
+      op = find_op(runahead_pc);
+      is_branch = (op && op->table_info->cf_type)? true : false;
+      // need to break without updating runahead_pc
+      if (!is_branch && last_runahead_op == max_runahead_op)
+      {
+        break;
+      }
+      if (is_branch) {
+        if (BP_MECH != MTAGE_BP && !bp_is_predictable(g_bp_data, op)) {
+          break_on_tage_limit = TRUE;
+          move_to_prev_op();
+          break;
+        }
+        fdip_new_branch(runahead_pc, op);
+        ASSERT(ic_stage->proc_id, op->fetch_addr == runahead_pc);
+        target = bp_predict_op(g_bp_data, op, DUMMY_CFN, runahead_pc);
+        ftq.push_back(std::pair<Addr, ftq_req>(runahead_pc, ftq_req(
+                                    target, cycle_count, *op,
+                                    0, /*prefetched*/
+                                    op->oracle_info.pred == TAKEN
+                                    )));
+        ftq_pushed = true;
+        // No matter how branch is predicted, prefetch the correct next PW.
+        // Only predict branches that are found when on the on-path.
+        if (UOC_ORACLE_PREF) {
+          uop_cache_issue_prefetch(op->oracle_info.npc, true); // Does this hit every time after recovery? If so then the prefetch at fdip_recover is redundant
+        }
+        STAT_EVENT(ic_stage->proc_id, FDIP_PRED_ON_PATH);
+        Flag bf = op->table_info->bar_type & BAR_FETCH ? TRUE : FALSE;
+        btb_ras_miss = (op->oracle_info.btb_miss && op->oracle_info.pred) || (op->oracle_info.btb_miss && !op->oracle_info.pred && !bf) || target == 0;
+        if (op->oracle_info.pred)
+          taken_branches++;
+        if (op->oracle_info.mispred || op->oracle_info.misfetch) {
+          off_count++;
+          fdip_on_path_bp = FALSE;
+          if ((FDIP_STOP_ON_MISPRED && op->oracle_info.mispred) || (FDIP_STOP_ON_MISFETCH && op->oracle_info.misfetch)) {
+            runahead_disable = TRUE;
+          }
+        } else if (btb_ras_miss) {
+          ASSERT(ic_stage->proc_id, !op->oracle_info.mispred && !op->oracle_info.misfetch);
+          off_count++;
+          fdip_on_path_bp = FALSE;
+          if (FDIP_STOP_ON_BTB_MISS) {
+            runahead_disable = TRUE;
+          }
+        }
+      }
+    } else {
+      auto op_iter = pc_to_op.find(runahead_pc);
+      is_branch = op_iter != pc_to_op.end();
+      if (is_branch) {
+        op = &op_iter->second;
+        if (BP_MECH != MTAGE_BP && !bp_is_predictable(g_bp_data, op)) {
+          break_on_tage_limit = TRUE;
+          break;
+        }
 
-    // Predict branches across cache lines. In many implementations, the BTB
-    // is looked up once per cycle, returning all branches in the cache line
-    if (FDIP_BREAK_ICACHE) {
-	fdip_break_addr_top = runahead_pc | CLMASK;
-	fdip_break_addr_bottom = runahead_pc & ~CLMASK;
+        ASSERT(ic_stage->proc_id, op->fetch_addr == runahead_pc);
+        Addr* btb_target = g_bp_data->bp_btb->pred_func(g_bp_data, op);
+        if (btb_target) {
+          target = bp_predict_op(g_bp_data, op, DUMMY_CFN, runahead_pc);
+          STAT_EVENT(ic_stage->proc_id, FDIP_PRED_OFF_PATH);
+          if (op->oracle_info.pred)
+            taken_branches++;
+        }
+      }
     }
 
-    // Find the next branch after the runahead PC. As the BTB/BP is addressed
-    // with a PC and not cache line address we need to byte-wise increment the
-    // runahead PC
-    while (predicts < FDIP_BP_PER_CYC && prefetches < FDIP_PREF_PER_CYC &&
-	   ftq.size() <= FDIP_MAX_RUNAHEAD &&
-	   runahead_pc <= fdip_break_addr_top &&
-	   runahead_pc >= fdip_break_addr_bottom) {
-	auto op_iter = pc_to_op.find(runahead_pc);
-	if (op_iter != pc_to_op.end()) {
-	    Op *op = &op_iter->second;
-	    ASSERT(ic_stage->proc_id, op->fetch_addr == runahead_pc);
-	    auto target = bp_predict_op(g_bp_data, op, DUMMY_CFN, runahead_pc);
-	    ftq.push(std::pair<Addr, ftq_req>(runahead_pc, ftq_req(
-						  target, cycle_count, *op,
-						  0, /*prefetched*/
-						  op->oracle_info.pred == TAKEN
-						  )));
-	    predicts++;
-	    if (op->oracle_info.btb_miss || target == 0) {
-		//On a BTB/RAS miss we cannot continue to FDIP
-		runahead_disable = true;
-		STAT_EVENT(ic_stage->proc_id, FDIP_BTB_RAS_MISS);
-		return;
-	    }
-	    if (FDIP_NLP || op->oracle_info.pred == TAKEN) {
-		bool success = fdip_prefetch(target, op);
-		prefetches += success;
-		ftq.back().second.prefetched = success;
-		if (op->oracle_info.pred == TAKEN) {
-		    runahead_pc = target;
-		}
-		if (FDIP_BREAK_ICACHE) {
-		    fdip_break_addr_top = runahead_pc | CLMASK;
-		    fdip_break_addr_bottom = runahead_pc & ~CLMASK;
-		}
-	    }
-	}
-	runahead_pc++;
+    bool do_prefetch = !last_cl_prefetched || (get_cache_line_addr(&ic->icache, runahead_pc) != last_cl_prefetched)? TRUE : FALSE;
+    if (do_prefetch) {
+      if ((FDIP_DUAL_PATH_PREF_IC_ENABLE || FDIP_DUAL_PATH_PREF_UOC_ENABLE)
+          && hash_table_access(&top_mispred_br, runahead_pc)) {
+        // TODO: change dual path prefetch to prefetch current op instead of the target
+        //fdip_dual_path_prefetch(runahead_pc, op);
+        //last_cl_prefetched = get_cache_line_addr(&ic->icache, runahead_pc);
+      } else {
+        if (FDIP_PREF_USEFUL_LINE && !will_be_accessed(runahead_pc)) {
+          last_cl_prefetched = get_cache_line_addr(&ic->icache, runahead_pc);
+        } else {
+          Flag success = fdip_prefetch(runahead_pc, NULL);
+          if (UOC_PREF)
+            uop_cache_issue_prefetch(runahead_pc, fdip_on_path_pref);
+          last_cl_prefetched = get_cache_line_addr(&ic->icache, runahead_pc);
+          if (success) {
+            if (ftq_pushed)
+              ftq.back().second.prefetched = true;
+            fdip_inc_outstanding_prefs(success);
+            if (target)
+              STAT_EVENT(ic_stage->proc_id, op->oracle_info.pred ? 
+                  FDIP_BRANCH_TAKEN_PREF : FDIP_NL_PREF);
+            if (fdip_on_path_pref) {
+              STAT_EVENT(ic_stage->proc_id, FDIP_PREF_ON_PATH);
+            }
+            else {
+              STAT_EVENT(ic_stage->proc_id, FDIP_PREF_OFF_PATH);
+            }
+          }
+        }
+      }
     }
+
+    if (fdip_on_path_pref)
+      set_max_op_num(is_branch, last_cl_prefetched);
+    if (!fdip_on_path_bp && fdip_on_path_pref)
+      fdip_on_path_pref = FALSE;
+
+    // In an actual implemenation, FDIP cannot differentiate between a btb
+    // miss and the op not being a branch (since the BTB is used to runahead
+    // and find the next branch). Thus FDIP would continue as if it was not
+    // branch, incrementing runahead_pc. This may cause cache pollution.
+    // Boomerang CAN distinguish these cases by storing the end of the bbl
+    if (btb_ras_miss || !target)
+      runahead_pc++;
+    else {
+      ASSERT(ic_stage->proc_id, target);
+      runahead_pc = target;
+    }
+    if (PERFECT_FDIP && !fdip_on_path_pref) {
+      runahead_disable = TRUE;
+    }
+    if (runahead_disable) {
+      STAT_EVENT(ic_stage->proc_id, FDIP_BREAK_ON_OFF_PATH);
+      break;
+    }
+  }
+
+  if (taken_branches >= FDIP_MAX_TAKEN_BRANCHES) {
+    STAT_EVENT(ic_stage->proc_id, FDIP_BREAK_ON_MAX_TAKEN_BRANCHES);
+  }
+  else if (last_runahead_op == max_runahead_op) {
+    STAT_EVENT(ic_stage->proc_id, FDIP_BREAK_ON_LOOKAHEAD_BUFFER);
+  }
+  else if (outstanding_prefs_on_path + outstanding_prefs_off_path >= FDIP_MAX_OUTSTANDING_PREFETCHES)
+  // TODO: comment out the line above and uncomment the line below to use 1-outstanding prefetch counter mode
+  //else if (outstanding_prefs >= FDIP_MAX_OUTSTANDING_PREFETCHES)
+  {
+    STAT_EVENT(ic_stage->proc_id, FDIP_BREAK_ON_MAX_OUTSTANDING_PREFETCHES);
+  }
+  else if (break_on_tage_limit) {
+    STAT_EVENT(ic_stage->proc_id, FDIP_BREAK_ON_TAGE_BUFFER_LIMIT);
+  }
+  else if (mem_req_failed)
+  {
+    STAT_EVENT(ic_stage->proc_id, FDIP_BREAK_ON_FULL_L1_MEM_REQ_QUEUE);
+  }
+
+  if (FDIP_PREF_NO_LATENCY) {
+    //outstanding_prefs = 0;
+    // TODO: comment out the two following lines and uncomment the line above to use 1-outstanding prefetch counter mode
+    outstanding_prefs_on_path = 0;
+    outstanding_prefs_off_path = 0;
+  }
+  STAT_EVENT(ic_stage->proc_id, FDIP_CYCLE_COUNT);
+  INC_STAT_EVENT(ic_stage->proc_id, FDIP_ON_PATH_OUTSTANDING_PREFS, outstanding_prefs_on_path);
+  INC_STAT_EVENT(ic_stage->proc_id, FDIP_OFF_PATH_OUTSTANDING_PREFS, outstanding_prefs_off_path);
+}
+
+Flag fdip_pref_off_path(void) {
+  return !fdip_on_path_pref;
 }
+
+Flag fdip_is_max_op(Op* op) {
+  if (!FDIP_ENABLE)
+    return FALSE;
+  if (op->op_num == max_op_num)
+    return TRUE;
+  return FALSE;
+}
+
+// 1-outstanding prefetch counter mode
+//void fdip_dec_outstanding_prefs(Addr cl_addr) {
+  //ASSERT(ic_stage->proc_id, outstanding_prefs);
+  //outstanding_prefs--;
+  //return;
+//}
+
+// 2-outstanding prefetch counters mode
+void fdip_dec_outstanding_prefs(Addr cl_addr, Flag off_path, Counter emitted_cycle) {
+  if (off_path) {
+    // TODO: the following two lines should be uncommented if outstanding_prefs_off_path is reset in fdip_recover()
+    //if (emitted_cycle < last_recover_cycle)
+      //return;
+    ASSERT(ic_stage->proc_id, outstanding_prefs_off_path);
+    outstanding_prefs_off_path--;
+  } else {
+    ASSERT(ic_stage->proc_id, outstanding_prefs_on_path);
+    outstanding_prefs_on_path--;
+  }
+  return;
+}
+
+// 1-outstanding prefetch counter mode
+/*
+void fdip_inc_outstanding_prefs(Flag success) {
+  switch(success) {
+    case SUCCESS_NEW:
+    case SUCCESS_DIFF_TYPE_ADDED:
+      outstanding_prefs++;
+      break;
+    case SUCCESS_SAME_TYPE_INVALID_OFF_PATH_CHANGED:
+    case SUCCESS_SAME_TYPE_VALID_OFF_PATH_CHANGED:
+      ASSERT(ic_stage->proc_id, fdip_on_path_pref);
+    case SUCCESS_SAME_TYPE:
+    case SUCCESS_DIFF_TYPE:
+    case FAILED:
+    default:
+      break;
+  }
+}
+*/
+
+// 2-outstanding prefetch counter mode
+void fdip_inc_outstanding_prefs(Flag success) {
+  switch(success) {
+    case SUCCESS_NEW:
+    case SUCCESS_DIFF_TYPE_ADDED:
+      if (fdip_on_path_pref) {
+        outstanding_prefs_on_path++;
+      }
+      else {
+        outstanding_prefs_off_path++;
+      }
+      break;
+    case SUCCESS_SAME_TYPE_INVALID_OFF_PATH_CHANGED:
+      ASSERT(ic_stage->proc_id, fdip_on_path_pref);
+      outstanding_prefs_on_path++;
+      break;
+    case SUCCESS_SAME_TYPE_VALID_OFF_PATH_CHANGED:
+      ASSERT(ic_stage->proc_id, fdip_on_path_pref);
+      outstanding_prefs_on_path++;
+      outstanding_prefs_off_path--; // TODO: if resetting this counter on a recovery, this should be fixed. (new_mem_req should return if the replaced off-path request was emitted before or after the recovery.
+      break;
+    case SUCCESS_SAME_TYPE:
+    case SUCCESS_DIFF_TYPE:
+    case FAILED:
+    default:
+      break;
+  }
+}
\ No newline at end of file
diff --git a/src/prefetcher/fdip.h b/src/prefetcher/fdip.h
index 0f406b3..36f493e 100644
--- a/src/prefetcher/fdip.h
+++ b/src/prefetcher/fdip.h
@@ -15,12 +15,20 @@ extern "C" {
   Addr fdip_pred(Addr bp_pc, Op *op);
   void fdip_retire(Op *op);
   void fdip_resolve(Op *op);
-  void fdip_new_branch(Addr bp_pc, Op *op);
   void fdip_recover(Recovery_Info *info);
+  void fdip_redirect(Addr recover_pc);
   void fdip_update();
+  Flag fdip_pref_off_path(void);
+  Flag fdip_is_max_op(Op *op);
+  void fdip_inc_outstanding_prefs(Flag success);
+  // 1-counter mode
+  //void fdip_dec_outstanding_prefs(Addr cl_addr);
+  // 2-counters mode
+  void fdip_dec_outstanding_prefs(Addr cl_addr, Flag off_path, Counter emitted_cycle);
 
   /* Private*/
-  void fdip_clear_ftq(Addr recover_pc);
+  void fdip_new_branch(Addr bp_pc, Op *op);
+  void fdip_clear_ftq();
 
 #ifdef __cplusplus
 }
diff --git a/src/prefetcher/pref.param.def b/src/prefetcher/pref.param.def
index f464ec9..78ba59a 100644
--- a/src/prefetcher/pref.param.def
+++ b/src/prefetcher/pref.param.def
@@ -134,4 +134,24 @@ DEF_PARAM(fdip_break_icache, FDIP_BREAK_ICACHE, uns, uns, 1, )
 DEF_PARAM(fdip_max_runahead, FDIP_MAX_RUNAHEAD, uns, uns, 32, )
 DEF_PARAM(fdip_pref_per_cyc, FDIP_PREF_PER_CYC, uns, uns, 1, )
 DEF_PARAM(fdip_bp_per_cyc, FDIP_BP_PER_CYC, uns, uns, 4, )
-DEF_PARAM(fdip_nlp, FDIP_NLP, Flag, Flag, TRUE, )
+DEF_PARAM(fdip_stop_on_btb_miss, FDIP_STOP_ON_BTB_MISS, Flag, Flag, FALSE, )
+DEF_PARAM(fdip_stop_on_mispred, FDIP_STOP_ON_MISPRED, Flag, Flag, FALSE, )
+DEF_PARAM(fdip_stop_on_misfetch, FDIP_STOP_ON_MISFETCH, Flag, Flag, FALSE, )
+DEF_PARAM(fdip_pref_no_latency, FDIP_PREF_NO_LATENCY, Flag, Flag, FALSE, )
+DEF_PARAM(fdip_always_prefetch, FDIP_ALWAYS_PREFETCH, Flag, Flag, FALSE, )
+DEF_PARAM(fdip_pref_useful_line, FDIP_PREF_USEFUL_LINE, Flag, Flag, FALSE, )
+DEF_PARAM(fdip_max_outstanding_prefetches, FDIP_MAX_OUTSTANDING_PREFETCHES, uns, uns, 7, )
+DEF_PARAM(fdip_perfect_runahead, FDIP_PERFECT_RUNAHEAD, Flag, Flag, FALSE, )
+DEF_PARAM(fdip_max_runahead_uid_count, FDIP_MAX_RUNAHEAD_UID_COUNT, uns, uns, 6, )
+DEF_PARAM(fdip_max_taken_branches, FDIP_MAX_TAKEN_BRANCHES, uns, uns, 2, )
+DEF_PARAM(perfect_fdip, PERFECT_FDIP, Flag, Flag, FALSE, )
+
+DEF_PARAM(fdip_dual_path_pref_ic_enable , FDIP_DUAL_PATH_PREF_IC_ENABLE    , Flag    , Flag    , FALSE, )
+DEF_PARAM(fdip_dual_path_pref_uoc_enable , FDIP_DUAL_PATH_PREF_UOC_ENABLE  , Flag    , Flag    , FALSE, ) //uop cache
+DEF_PARAM(top_mispred_br_filepath       , TOP_MISPRED_BR_FILEPATH          , char*   , string  , "br_resteer_coverage.csv", )
+DEF_PARAM(top_mispred_br_resteer_coverage, TOP_MISPRED_BR_RESTEER_COVERAGE , float   , float   , 0, )
+// FDIP issues uop cache prefetch concurrently with icache prefetch
+DEF_PARAM(uoc_pref                      , UOC_PREF                         , Flag    , Flag    , FALSE, )
+// Only prefetch the correct path into the uop cache.
+DEF_PARAM(uoc_oracle_pref               , UOC_ORACLE_PREF                  , Flag    , Flag    , FALSE, )
+DEF_PARAM(uoc_zero_latency_pref         , UOC_ZERO_LATENCY_PREF            , Flag    , Flag    , FALSE, )
\ No newline at end of file
diff --git a/src/prefetcher/pref.stat.def b/src/prefetcher/pref.stat.def
index c1ceae5..55947c1 100644
--- a/src/prefetcher/pref.stat.def
+++ b/src/prefetcher/pref.stat.def
@@ -240,11 +240,35 @@ DEF_STAT( PREF_ACC4_LT_LP                  , DIST     ,     NO_RATIO)
 
 DEF_STAT(FDIP_PREFETCHES, COUNT, NO_RATIO)
 DEF_STAT(FDIP_PREF_CORRECT_PATH, COUNT, NO_RATIO)
-DEF_STAT(FDIP_PREF_WRONG_PATH, COUNT, NO_RATIO)
 DEF_STAT(FDIP_SAVED_PREF_CYC, COUNT, NO_RATIO)
+DEF_STAT(FDIP_RECOVER, COUNT, NO_RATIO)
+DEF_STAT(FDIP_ON_PATH_OUTSTANDING_PREFS_ON_RECOVER, COUNT, NO_RATIO)
+DEF_STAT(FDIP_OFF_PATH_OUTSTANDING_PREFS_ON_RECOVER, COUNT, NO_RATIO)
 DEF_STAT(FDIP_PREF_ICACHE_HIT, COUNT, NO_RATIO)
 DEF_STAT(FDIP_PRED_CORRECT_PATH, COUNT, NO_RATIO)
 DEF_STAT(FDIP_PRED_WRONG_PATH, COUNT, NO_RATIO)
 DEF_STAT(FDIP_PRED_FTQ_EMPTY, COUNT, NO_RATIO)
 DEF_STAT(FDIP_BTB_RAS_MISS, COUNT, NO_RATIO)
-DEF_STAT(FDIP_SQUASH_FAKE_BRANCH, COUNT, NO_RATIO)
\ No newline at end of file
+DEF_STAT(FDIP_SQUASH_FAKE_BRANCH, COUNT, NO_RATIO)
+DEF_STAT(FDIP_NL_PREF, COUNT, NO_RATIO)
+DEF_STAT(FDIP_BRANCH_TAKEN_PREF, COUNT, NO_RATIO)
+DEF_STAT(FDIP_PRED_ON_PATH, COUNT, NO_RATIO)
+DEF_STAT(FDIP_PRED_OFF_PATH, COUNT, NO_RATIO)
+DEF_STAT(FDIP_PREF_ON_PATH, DIST, NO_RATIO)
+DEF_STAT(FDIP_PREF_OFF_PATH, DIST, NO_RATIO)
+DEF_STAT(FDIP_ATTEMPTED_PREF_ON_PATH, COUNT, NO_RATIO)
+DEF_STAT(FDIP_ATTEMPTED_PREF_OFF_PATH, COUNT, NO_RATIO)
+DEF_STAT(FDIP_ALT_PATH_PREFETCHES_IC_EMITTED, COUNT, NO_RATIO)
+DEF_STAT(FDIP_ALT_PATH_PREFETCHES_IC_TRIGGERED, COUNT, NO_RATIO)
+DEF_STAT(FDIP_ALT_PATH_PREFETCHES_UOC_EMITTED, COUNT, NO_RATIO)
+DEF_STAT(FDIP_ALT_PATH_PREFETCHES_UOC_TRIGGERED, COUNT, NO_RATIO)
+DEF_STAT(TOP_MISPRED_BR, COUNT, NO_RATIO)
+DEF_STAT(FDIP_BREAK_ON_MAX_TAKEN_BRANCHES, DIST, NO_RATIO)
+DEF_STAT(FDIP_BREAK_ON_LOOKAHEAD_BUFFER, COUNT, NO_RATIO)
+DEF_STAT(FDIP_BREAK_ON_MAX_OUTSTANDING_PREFETCHES, COUNT, NO_RATIO)
+DEF_STAT(FDIP_BREAK_ON_TAGE_BUFFER_LIMIT, COUNT, NO_RATIO)
+DEF_STAT(FDIP_BREAK_ON_FULL_L1_MEM_REQ_QUEUE, COUNT, NO_RATIO)
+DEF_STAT(FDIP_BREAK_ON_OFF_PATH, DIST, NO_RATIO)
+DEF_STAT(FDIP_CYCLE_COUNT, COUNT, NO_RATIO)
+DEF_STAT(FDIP_ON_PATH_OUTSTANDING_PREFS, COUNT, NO_RATIO)
+DEF_STAT(FDIP_OFF_PATH_OUTSTANDING_PREFS, COUNT, NO_RATIO)
diff --git a/src/ramulator.cc b/src/ramulator.cc
index 6662377..1ca06ac 100644
--- a/src/ramulator.cc
+++ b/src/ramulator.cc
@@ -263,7 +263,7 @@ void to_ramulator_req(const Mem_Req* scarab_req, Request* ramulator_req) {
   if(scarab_req->type == MRT_WB || scarab_req->type == MRT_DSTORE)
     ramulator_req->type = Request::Type::WRITE;
   else if(scarab_req->type == MRT_DFETCH || scarab_req->type == MRT_IFETCH ||
-          scarab_req->type == MRT_IPRF || scarab_req->type == MRT_DPRF)
+          scarab_req->type == MRT_IPRF || scarab_req->type == MRT_DPRF || scarab_req->type == MRT_UOCPRF || scarab_req->type == MRT_FDIPPRF)
     ramulator_req->type = Request::Type::READ;
   else
     ASSERTM(scarab_req->proc_id, false,
diff --git a/src/sim.c b/src/sim.c
index 07b40e2..0a2d245 100644
--- a/src/sim.c
+++ b/src/sim.c
@@ -96,6 +96,8 @@ Counter* sim_done_last_cycle_count;
 uns*     sim_count;
 uns      operating_mode = SIMULATION_MODE;
 
+Hash_Table per_branch_stat;
+
 time_t sim_start_time; /* the time that the simulator was started */
 
 FILE* mystdout; /* default output (can be redirected via --stdout) */
@@ -232,6 +234,19 @@ static inline void check_heartbeat(uns8 proc_id, Flag final) {
                   "KIPS)\n",
                   proc_id, unsstr64(inst_count[proc_id]), unsstr64(cycle_count),
                   unsstr64(sim_time), cum_ipc, cum_ipc, cum_khz);
+          FILE* fp = fopen("per_branch_stats.csv", "w");
+          Per_Branch_Stat** entries = (Per_Branch_Stat**) hash_table_flatten(&per_branch_stat, NULL);
+          fprintf(fp, "cf_type,addr,target,bpu_hit_uc_hit,bpu_hit_uc_miss,mispred_uc_hit,"
+                  "mispred_uc_miss,misfetch_uc_hit,misfetch_uc_miss,btb_miss_uc_hit,"
+                  "btb_miss_uc_miss,recover_redirect_extra_fetch_latency\n");
+          for (int i=0; i<per_branch_stat.count; i++) {
+            Per_Branch_Stat* entry = entries[i];
+            fprintf(fp, "%i,%llx,%llx,%i,%i,%i,%i,%i,%i,%i,%i,%i\n", entry->cf_type, entry->addr, 
+              entry->target, entry->bpu_hit_uc_hit, entry->bpu_hit_uc_miss, entry->mispred_uc_hit, 
+              entry->mispred_uc_miss, entry->misfetch_uc_hit, entry->misfetch_uc_miss, 
+              entry->btb_miss_uc_hit, entry->btb_miss_uc_miss, entry->recover_redirect_extra_fetch_latency);
+          }
+          free(entries);
           break;
 
         default:
@@ -645,6 +660,7 @@ void full_sim() {
 
   init_op_pool();
   unique_count = 1;
+  init_icache_trace(); //must happen after init_op_pool
 
   sim_limit   = trigger_create("SIM_LIMIT", SIM_LIMIT, TRIGGER_ONCE);
   clear_stats = trigger_create("CLEAR_STATS", CLEAR_STATS, TRIGGER_ONCE);
diff --git a/src/uop_cache.c b/src/uop_cache.c
new file mode 100644
index 0000000..6e09d6a
--- /dev/null
+++ b/src/uop_cache.c
@@ -0,0 +1,336 @@
+/***************************************************************************************
+ * File         : uop_cache.h
+ * Author       : Peter Braun
+ * Date         : 10.28.2020
+ * Description  : Interface for interacting with uop cache object.
+ *                  Following Kotra et. al.'s MICRO 2020 description of uop cache baseline
+ ***************************************************************************************/
+
+#include "debug/debug_macros.h"
+#include "debug/debug_print.h"
+#include "globals/assert.h"
+#include "globals/global_defs.h"
+#include "globals/global_types.h"
+#include "globals/global_vars.h"
+#include "globals/utils.h"
+#include "isa/isa_macros.h"
+
+#include "bp/bp.h"
+#include "op_pool.h"
+
+#include "core.param.h"
+#include "debug/debug.param.h"
+#include "general.param.h"
+#include "statistics.h"
+
+#include "libs/cache_lib.h"
+#include "memory/memory.h"
+#include "memory/memory.param.h"
+#include "prefetcher/pref.param.h"
+#include "uop_cache.h"
+
+/**************************************************************************************/
+/* Macros */
+
+#define DEBUG(proc_id, args...) _DEBUG(proc_id, DEBUG_UOP_CACHE, ##args)
+
+// Uop cache is byte-addressable, so tag/set index are generated from full address (no offset)
+#define UOP_CACHE_LINE_SIZE       1
+#define UOP_QUEUE_SIZE            100 // at least UOP_CACHE_ASSOC * UOP_CACHE_MAX_UOPS_LINE
+#define UOP_CACHE_LINE_DATA_SIZE  sizeof(Uop_Cache_Data)
+
+/**************************************************************************************/
+/* Local Prototypes */
+
+static inline Flag insert_uop_cache(void);
+static inline Flag in_uop_cache_search(Addr search_addr, Flag update_repl);
+static Flag pw_insert(Uop_Cache_Data pw);
+
+/**************************************************************************************/
+/* Global Variables */
+
+Cache uop_cache;
+
+// uop trace/bbl accumulation
+static Uop_Cache_Data accumulating_pw = {0};
+static Op* uop_q[UOP_QUEUE_SIZE];
+
+// k: instr addr
+Hash_Table inf_size_uop_cache;
+// indexed by start addr of PW
+Hash_Table pc_to_pw;
+
+/**************************************************************************************/
+/* init_uop_cache */
+
+void init_uop_cache() {
+  if (INF_SIZE_UOP_CACHE || INF_SIZE_UOP_CACHE_PW_SIZE_LIM) {
+    init_hash_table(&inf_size_uop_cache, "infinite sized uop cache", 15000000, sizeof(int));
+  }
+  if (UOP_CACHE_SIZE == 0) {
+    return;
+  }
+  // used for prefetching
+  init_hash_table(&pc_to_pw, "Log of all PWs decoded", 15000000, 
+                    sizeof(Uop_Cache_Data));
+  // The cache library computes the number of entries from line_size and cache_size,
+  // but UOP_CACHE_LINE_SIZE must be 1 to enable indexing with the full byte-granularity address.
+
+  init_cache(&uop_cache, "UOP_CACHE", UOP_CACHE_SIZE * UOP_CACHE_LINE_SIZE, UOP_CACHE_ASSOC,
+             UOP_CACHE_LINE_SIZE,UOP_CACHE_LINE_DATA_SIZE, REPL_TRUE_LRU);
+}
+
+Flag pw_insert(Uop_Cache_Data pw) {
+  Uop_Cache_Data* cur_line_data = NULL;
+  Addr line_addr;  
+  Addr repl_line_addr;
+  pw.used = 0;
+
+  int lines_needed = pw.n_uops / UOP_CACHE_MAX_UOPS_LINE;
+  if (pw.n_uops % UOP_CACHE_MAX_UOPS_LINE) lines_needed++;
+  ASSERT(0, lines_needed > 0);
+
+  // Is the PW too big?
+  if (lines_needed > UOP_CACHE_ASSOC) {
+    STAT_EVENT(ic->proc_id, UOP_CACHE_PW_INSERT_FAILED_TOO_LONG + pw.prefetch);
+    return FALSE;
+  } else if (cache_access(&uop_cache, pw.first, &line_addr, FALSE)) {
+    STAT_EVENT(ic->proc_id, UOP_CACHE_PW_INSERT_FAILED_CACHE_HIT + pw.prefetch);
+    return FALSE;
+  } else {
+    // Insert it, taking appropriate number of lines
+    for (int jj = 0; jj < lines_needed; jj++) {
+      cur_line_data = (Uop_Cache_Data*) cache_insert(&uop_cache, 0,
+                      pw.first, &line_addr, &repl_line_addr);
+      // TODO: invalidate all lines with same pw start addr 
+      // (instead all with that have their PW start in this line)
+      if (repl_line_addr) {
+        cache_invalidate(&uop_cache, repl_line_addr, &line_addr);
+      }
+      memset(cur_line_data, 0, UOP_CACHE_LINE_DATA_SIZE);
+      *cur_line_data = pw;
+    }
+    STAT_EVENT(0, UOP_CACHE_PWS_INSERTED);
+    INC_STAT_EVENT(0, UOP_CACHE_LINES_INSERTED, lines_needed);
+  }
+  return TRUE;
+}
+
+/**************************************************************************************/
+/* insert_uop_cache: private method, only called by accumulate_op
+ *                   Drain buffer and insert. Return whether inserted.
+ */
+Flag insert_uop_cache() {
+  // PW may span multiple cache entries. 1 entry per line. Additional terminating conditions per line:
+  // 1. max uops per line
+  // 2. max imm/disp per line
+  // 3. max micro-coded instr per line (not simulated)
+
+  // Invalidate after hitting maximum allowed number of lines, as in gem5
+  // Each entry/pw indexed by physical addr of 1st instr, so insert each line using same addr
+  // Invalidation: Let LRU handle it by placing PW in one line at a time.   
+  
+  ASSERT(0, accumulating_pw.n_uops);
+  Flag success = FALSE;
+
+  Flag new_entry;
+  Uop_Cache_Data* saved_pw = (Uop_Cache_Data*) hash_table_access_create(
+                              &pc_to_pw, accumulating_pw.first, &new_entry);
+  *saved_pw = accumulating_pw;
+  int lines_needed = accumulating_pw.n_uops / UOP_CACHE_MAX_UOPS_LINE;
+  if (accumulating_pw.n_uops % UOP_CACHE_MAX_UOPS_LINE) lines_needed++;
+
+  if (INF_SIZE_UOP_CACHE || (INF_SIZE_UOP_CACHE_PW_SIZE_LIM 
+      && accumulating_pw.n_uops <= INF_SIZE_UOP_CACHE_PW_SIZE_LIM)) {
+    for (int ii = 0; ii < accumulating_pw.n_uops; ii++) {
+      Op* op = uop_q[ii];
+      Flag new_entry;
+      hash_table_access_create(&inf_size_uop_cache, op->inst_info->addr, 
+                                &new_entry);
+    }
+    success = TRUE;
+  } else if (INF_SIZE_UOP_CACHE_PW_SIZE_LIM 
+            && accumulating_pw.n_uops > INF_SIZE_UOP_CACHE_PW_SIZE_LIM) {
+    success = FALSE;
+  } else {
+    success = pw_insert(accumulating_pw);
+  }
+
+  return success;
+}
+
+static inline Flag in_uop_cache_search(Addr search_addr, Flag update_repl) {
+  static Uop_Cache_Data cur_pw = {0};
+  Addr line_addr;
+  Uop_Cache_Data* uoc_data = NULL;
+  Flag found = FALSE;
+
+  // First check if current pw has this search addr
+  if (cur_pw.first && search_addr >= cur_pw.first
+                   && search_addr <= cur_pw.last) {
+    found = TRUE;
+  } else {
+    // Next try to access a new PW starting at this addr
+    if (cache_access_all(&uop_cache, search_addr, &line_addr, update_repl, 
+                          (void**) &uoc_data)) {
+      if (uoc_data->prefetch && !uoc_data->used) {
+        STAT_EVENT(0, UOP_CACHE_PREFETCH_USED);
+      }
+      if (!uoc_data->used) {
+        STAT_EVENT(0, UOP_CACHE_LINES_USED);
+      }
+      uoc_data->used += 1;
+      cur_pw = *uoc_data;
+      found = TRUE;
+    } else {
+      memset(&cur_pw, 0, sizeof(cur_pw));
+      found = FALSE;
+    }
+  }
+
+  return found;
+}
+
+/**************************************************************************************/
+/* in_uop_cache: Iterate over all possible ops and check if contained.
+ *                    Other option: use cache to simulate capacity and maintain a map 
+ *                      data structure for pcs
+ */
+Flag in_uop_cache(Addr pc, const Counter* op_num, Flag update_repl) {
+  // A PW can span multiple cache entries. The next line used is either physically 
+  // next in the set (use flag) or anywhere in the set (use pointer), depending on impl.
+  // Here, don't care about order, just search all lines for pc in question
+  STAT_EVENT(0, IN_UOP_CACHE_CALLED);
+
+  if (ORACLE_PERFECT_UOP_CACHE) {
+    if (update_repl) {
+      STAT_EVENT(0, UOP_CACHE_HIT);
+    }
+    return TRUE;
+  } else if (INF_SIZE_UOP_CACHE || INF_SIZE_UOP_CACHE_PW_SIZE_LIM) {
+    return hash_table_access(&inf_size_uop_cache, pc) != NULL;
+  }
+  if (UOP_CACHE_SIZE == 0) {
+    return FALSE;
+  }
+
+  static Counter next_op_num = 1;
+
+  Flag found = in_uop_cache_search(pc, update_repl);
+
+  if (update_repl) {
+    STAT_EVENT(0, UOP_CACHE_MISS + found);
+    if (op_num) {
+      ASSERT(0, *op_num == next_op_num);
+      next_op_num++;
+    }
+  }
+  
+  return found;
+}
+
+void end_accumulate(void) {
+  if (UOP_CACHE_SIZE == 0 && !INF_SIZE_UOP_CACHE && !INF_SIZE_UOP_CACHE_PW_SIZE_LIM) {
+    return;
+  }
+
+  if (accumulating_pw.n_uops > 0) {
+    insert_uop_cache();
+    memset(&accumulating_pw, 0, sizeof(accumulating_pw));
+  }
+}
+
+/**************************************************************************************/
+/* accumulate_op: accumulate into buffer. Insert into cache at end of PW. */
+void accumulate_op(Op* op) {
+  // Prediction Window termination conditions:
+  // 1. end of icache line
+  // 2. branch predicted taken
+  // 3. predetermined number of branch NT (no limit in implementation)
+  // 4. uop queue full
+  // 5. too many uops to fit in entire set, even after evicting all entries
+
+  // it is possible for an instr to be partially in 2 lines. 
+  // For pw termination purposes, assume it is in first line.
+  static Counter cons_op_num = 0;
+
+  if ((UOP_CACHE_SIZE == 0 && !INF_SIZE_UOP_CACHE && !INF_SIZE_UOP_CACHE_PW_SIZE_LIM) 
+      || ORACLE_PERFECT_UOP_CACHE) {
+    return;
+  }
+
+  Addr cur_icache_line_addr = get_cache_line_addr(&ic->icache,
+                                                  accumulating_pw.first);
+  Addr icache_line_addr = get_cache_line_addr(&ic->icache, op->inst_info->addr);
+
+  if (!cur_icache_line_addr) {
+    accumulating_pw.first = op->inst_info->addr;
+    cur_icache_line_addr = icache_line_addr;
+    cons_op_num = op->op_num + 1;
+  } else {
+    ASSERT(0, op->op_num == cons_op_num);
+    cons_op_num++;
+  }
+  
+  Flag end_of_icache_line = icache_line_addr != cur_icache_line_addr;
+  Flag branch_pt = op->oracle_info.pred == TAKEN;
+  Flag uop_q_full = (accumulating_pw.n_uops + 1 > UOP_QUEUE_SIZE);
+
+  if (end_of_icache_line || branch_pt || uop_q_full) {
+    end_accumulate();
+  }
+
+  if (accumulating_pw.n_uops == 0) {
+    // occurs when THIS fxn call drains the uop queue (insertion into uop cache)
+    accumulating_pw.first = op->inst_info->addr;
+  }
+  uop_q[accumulating_pw.n_uops] = op;
+  accumulating_pw.last = op->inst_info->addr;
+  accumulating_pw.n_uops++;
+};
+
+Flag uop_cache_fill_prefetch(Addr pw_start_addr, Flag fdip_on_path) {
+  Uop_Cache_Data pw;
+  if (UOP_CACHE_SIZE == 0) {
+    return FALSE;
+  }
+
+  // on-path / off-path is not working, even for correct-path prefetching.
+  fdip_on_path = FALSE; // just use legacy method.
+
+  if (fdip_on_path) {
+    pw = get_pw_lookahead_buffer(pw_start_addr);
+  } else {
+    Uop_Cache_Data* pw_p = (Uop_Cache_Data*) hash_table_access(&pc_to_pw, pw_start_addr);
+    // if PW has not been decoded before, do nothing. Hopefully this is uncommon.
+    if (pw_p == NULL) {
+      STAT_EVENT(0, UOP_CACHE_PREFETCH_FAILED_PW_NEVER_SEEN);
+      return FALSE;
+    }
+    pw = *pw_p;
+  }
+  ASSERT(0, pw.first == pw_start_addr);
+  pw.prefetch = TRUE;
+  Flag prefetched = pw_insert(pw);
+  INC_STAT_EVENT(0, UOP_CACHE_PREFETCH, prefetched);
+  return prefetched;
+}
+
+Flag uop_cache_issue_prefetch(Addr pw_start_addr, Flag on_path) {
+  int prefetch_success = FALSE;
+
+  if (UOC_ZERO_LATENCY_PREF) {
+    prefetch_success = uop_cache_fill_prefetch(pw_start_addr, on_path);
+  } else {
+    // If no op is provided, on_path is assumed.
+    prefetch_success = new_mem_req(MRT_UOCPRF, 0, pw_start_addr,
+              ICACHE_LINE_SIZE, DECODE_CYCLES, NULL, instr_fill_line,
+              unique_count,
+              0);
+    if(!prefetch_success) {
+      STAT_EVENT(0, UOP_CACHE_PREFETCH_FAILED_MEMREQ_FAILED);
+    }
+  }
+
+  return prefetch_success;
+}
\ No newline at end of file
diff --git a/src/uop_cache.h b/src/uop_cache.h
new file mode 100644
index 0000000..826dc7f
--- /dev/null
+++ b/src/uop_cache.h
@@ -0,0 +1,39 @@
+/***************************************************************************************
+ * File         : uop_cache.h
+ * Author       : Peter Braun
+ * Date         : 10.28.2020
+ * Description  : Interface for interacting with uop cache object.
+ *                  Following Kotra et. al.'s MICRO 2020 description of uop cache baseline
+ *                  Instr comes from icache. Theoretically
+ *                  we have higher BW fetching direct with uop cache
+ ***************************************************************************************/
+
+#ifndef __UOP_CACHE_H__
+#define __UOP_CACHE_H__
+
+#include "op.h"
+#include "icache_stage.h" //needed for get_pw_lookahead_buffer
+
+/**************************************************************************************/
+/* Types */
+
+
+/**************************************************************************************/
+/* Prototypes */
+
+// only one instance of uop cache
+
+void init_uop_cache(void);
+
+/* return whether the instr pc is cached (this does not consider that the whole PW 
+    could already have been fetched, potentially introducing 1 incorrect cycle of latency)*/
+Flag in_uop_cache(Addr pc, const Counter* op_num, Flag update_repl); 
+
+void end_accumulate(void);
+/* accumulate uop into buffer. If terminating condition reached, call insert_uop_cache */
+void accumulate_op(Op* op);
+
+Flag uop_cache_fill_prefetch(Addr pw_start_addr, Flag on_path);
+Flag uop_cache_issue_prefetch(Addr pw_start_addr, Flag on_path);
+
+#endif /* #ifndef __UOP_CACHE_H__ */
diff --git a/test/test.sh b/test/test.sh
new file mode 100644
index 0000000..7147664
--- /dev/null
+++ b/test/test.sh
@@ -0,0 +1,6 @@
+rm -rf drmemtrace*
+$DRIO_ROOT/build/bin64/drrun -t drcachesim -offline -- /bin/ls
+a=$(realpath drmemtrace*)
+$DRIO_ROOT/build/bin64/drrun -t drcachesim -indir $a
+cp ../src/PARAMS.kaby_lake PARAMS.in
+../src/scarab --frontend memtrace --cbp_trace_r0=$a/trace --memtrace_modules_log=$a/raw --fetch_off_path_ops false --fdip_enable 0 --inst_limit 10000 # 1000000 causes an error possibly because of an unsupported instruction
